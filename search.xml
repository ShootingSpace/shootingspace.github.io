<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Learn AI Step by Step]]></title>
    <url>%2Fai%2F</url>
    <content type="text"><![CDATA[记录学习AI的学习笔记，内容包含基础知识的总结以及编程实现的整理。 English template 英文模板 机器学习 Coursera Machine Learning， 吴恩达的简化版机器学习 Machine Learning, 吴恩达的机器学习课程 这个比较深入 Deep Learning, 吴恩达的深度学习课程 Neural Networks for Machine Learning, Hinton的神经网络课程 深度学习 Deep learning, Coursera Machine Learning Practical: DNN, CNN, RNN 每个lab的答案在下一个lab 分支，即lab1的答案可以在lab2 branch里面看到。这个代码全部用Python class，比coursera的难度高点。 自然语言处理 自然语言处理, 斯坦福 加速自然语言处理, 爱丁堡大学 深度学习处理自然语言，斯坦福 计算机视觉 图像识别：卷积神经网络，李飞飞，斯坦福 机器人 机器人入门，斯坦福]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
        <tag>计算机视觉</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computer Science Step by Step]]></title>
    <url>%2Fcs%2F</url>
    <content type="text"><![CDATA[我学习CS历程，也包含基础知识的总结以及编程实现的整理。 每一阶段里面, 都有很多个性化选项, 仅供参考 入门 CS入门 学习编写(至少)一种面向对象编程语言(C ++，Java®，Python®) 测试你的代码 夯实基础你可以跳过这些而直接进入下面的进阶实践环节，并根据自己需要查漏补缺。但是这些都是非常重要的基础，任何时候，只要有时间，就可以去学习了解。 逻辑推理和离散数学 熟悉计算机操作系统 学习计算机网络 掌握数据库 了解编译器和优化 了解计算理论 学习其他编程语言 进阶与实践 深入了解算法和数据结构 分布式，并行和大数据 安卓开发 iOS开发 网页开发 加密与区块链 参与项目 English template 英文模板 CS入门现在的入门课基本都是用Python语言。 计算机科学导论，优达学城 CS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML CMU 15213: Introduction to Computer Systems (ICS) 面向对象编程语言一般而言，建议先学Java 或 Python，再学C++。 这三种语言都基本掌握后，再根据自身的职业需求，选择其中一个语言（或者其他语言）进一步深入练习。 面向初学者程序员的在线资源： 编程方法学，斯坦福CS106A，Java 伯克利大学CS 61A计算机程序的结构与解读，Python Java编程简介，MIT Google Python Class 面向有经验的程序员的在线资源： 数据结构，伯克利大学 CS 61B，Java 计算机程序设计，Udacity，Python 抽象编程，斯坦福 CS106B，C ++, 最新作业 http://web.stanford.edu/class/cs106b/ 《数据结构与算法分析:C++描述》, Mark A. Weiss 测试你的代码了解如何捕获错误，创建测试和破解软件. 软件测试，Udacity 软件调试，Udacity 操作系统 CMU 15213: Introduction to Computer Systems (ICS) CS162 UC Berkeley: Operating Systems and Systems Programming 逻辑推理和离散数学 数学计算机科学，麻省理工学院 数学思考导论，斯坦福大学，Coursera 概率图形模型，斯坦福大学，Coursera 博弈论，Coursera 计算理论 Introduction to the theory of computation, Michael Sipser 计算机网络 Computer Networking A Top-Down Approach, James F. Kurose, Keith W. Ross 数据库 SQL A First Course in Database Systems, Jeffrey D. Ullman, Jennifer Widom 编译器算法和数据结构了解基本数据类型(堆栈，队列和袋子)，排序算法(快速排序，合并，堆栈)，数据结构(二叉搜索树，红黑树，哈希表)和Big O. 算法简介，麻省理工学院，2011秋季 算法，普林斯顿大学，Part1 算法，普林斯顿大学，Part2 算法：设计和分析，斯坦福大学 算法，第4版，by Robert Sedgewick and Kevin Wayne 分布式，并行和大数据 Hadoop 和 MapReduce 入门，优达学城 并行计算入门：MPI, openMP, and CUDA, 斯坦福 MapReduce 极限计算，爱丁堡大学 加密与区块链 Cryptography, Stanford, Coursera Applied Cryptography, Udacity 安卓开发 Google Developer Training for Android, on Udacity iOS开发网页开发参与项目 尝试课堂以外的项目：创建和维护网站，构建自己的服务器或构建机器人。 Capstone project: Analyzing (Social) Network Data - scroll down to bottom of page,UCSD, Coursera Capstone project: Java Programming: A DIY Version of Netflix and Amazon Recommendation Engines, Duke University, Coursera Project Directory, Apache Google Summer of Code Project Archive 参与大型系统（代码库）的一小部分，阅读并理解现有代码，跟踪文档和调试 Version control with Git GitHub®：关注github热门开源项目的issue 与其他程序员一起工作 参与开源项目： 尝试提出一个issue 针对一个issue，尝试给出解决方案，并提交 pull request 参与公司实习 其他编程语言根据实际需要自行选择一种或多种学习, 一些在线资源： CS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML Codecademy JavaScript Bento JavaScript Learning Track(Bento) Egghead.io 学习如何编程：JavaScript - Epicodus Inc. 学习：查询 CSS ＆ HTML Bento CSS Learning Track(Bento) Bento HTML Learning Track(Bento) 用破折号建立个人网站 使用Webflow构建响应式网站 使用骨架构建SaaS着陆页 建立动态网站 在1小时内编写个人启动页面：实用HTML和CSS简介 学习如何编程：CSS - Epicodus Inc. 从头开始学习HTML5编程 Ruby 学习如何编程：Ruby - Epicodus Inc. RubyMonk - 交互式Ruby教程 Haskell C9：功能编程基础知识 - Erik Meijer CIS 194：Haskell简介 - Brent Yorgey CS240h：Haskell的功能系统 - Bryan O’Sullivan edX：功能编程简介 - Erik Meijer 亚琛大学：功能编程 - JürgenGiesl Lua Lua Interactive Crash Course Lua Tutorial PHP 学习如何编程：PHP - Epicodus Inc. GO Go Tutorial 参考资料Guide to technical development from Google educationOS Free Programming Books]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>计算机科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Efficient Programming in Java - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-15-efficient-programming%2F</url>
    <content type="text"><![CDATA[效率来源两个方面: 编程成本: 开发程序需要多长时间？代码是否容易阅读，修改和维护（大部分成本来自维护和可扩展性）？ 运行成本: 程序需要多长时间运行 (Time complexity)？ 需要多少内存 (Space complexity)？ Encapsulation Module: A set of methods that work together as a whole to perform some task or set of related tasks.Encapsulated: A module is said to be encapsulated if its implementation is completely hidden, and it can be accessed only through a documented interface. API’s An API(Application Programming Interface) of an ADT is the list of constructors and methods and a short description of each. API 包括语法规范和语义规范 编译器确认语法符合要求 测试帮助确认语义描述是否正确 一般都是用英文写的(可能包含使用例子) ADT’sAbstract Data Structures 是由其行为属性定义的抽象类型, 跟如何实现无关. 三种方式实现Stack的push(Item x): 使用 extension 以借用LinkedList&lt;Item&gt;的方法: 12345public class ExtensionStack&lt;Item&gt; extends LinkedList&lt;Item&gt; &#123; public void push(Item x) &#123; add(x); &#125;&#125; 委托模式Delegation， 生成Linked List并调用其方法来达到目的 123456public class DelegationStack&lt;Item&gt; &#123; private LinkedList&lt;Item&gt; L = new LinkedList&lt;Item&gt;(); public void push(Item x) &#123; L.add(x); &#125;&#125; 类似方法2, 只是这里可以利用任何实现了List接口的类, 如Linked List, ArrayList, 等等 12345678910public class StackAdapter&lt;Item&gt; &#123; private List L; public StackAdapter(List&lt;Item&gt; worker) &#123; L = worker; &#125; public void push(Item x) &#123; L.add(x); &#125;&#125; Delegation vs Extension: Extension 一般是基于对父类有比较清楚的了解认知下才会使用。此外，扩展基本上等于在说明正在扩展的类与被扩展类是相似的。如果两个类无法看做是同属的, 那么就用委托模式。 Views: 通过视图进行的更改会影响底层对象。1234567/** Create an ArrayList. */List&lt;String&gt; L = new ArrayList&lt;&gt;();/** Add some items. */L.add(“at”); L.add(“ax”); …List&lt;String&gt; SL = l.subList(1, 4);/** Mutate that thing. */SL.set(0, “jug”); Asymptotic AnalysisCare about what happens for very large N (asymptotic behavior). We want to consider what types of algorithms would best handle scalability - Algorithms that scale well have better asymptotic runtime behavior. Simplification Summary Only consider the worst case. Pick a representative operation (aka: cost model) Ignore lower order terms Ignore multiplicative constants. Simplified Analysis Process Choose cost model (representative operation) Figure out the order of growth for the count of representative operation by either: Making an exact count, and discarding unnecessary pieces Only consider the worst case. Ignore lower order terms Ignore constants. Or, using intuition/inspection to determine orders of growth. Big ThetaFormalizing Order of Growth: Suppose a function $R(N)$ with order of growth $f(N)$, this is represented as $R(N) \in \Theta(f(N))$ in notation. Means that there exists positive constants $k_1, k_2$ such that: $$k_1⋅f(N)≤R(N)≤k_​2⋅f(N),$$ for all values of $N$ greater than some $N_0$(a very large N). Procedure: Given a piece of code, express its runtime as a function $R(N)$ $N$ is some property of the input of the function. Oftentimes, $N$ represents the size of the input Rather than finding $R(N)$ exactly, instead care about the order of growth of $R(N)$. One approach (not universal): Choose a representative operation Let $C(N)$ = count of how many times that operation occurs, as a function of $N$. Determine order of growth $C(N) \in \Theta(f(N))$ Often (but not always) consider the worst case count. If operation takes constant time, then $R(N) \in \Theta(f(N))$ 在 Big Theta 的范畴内，对于涉及 logarithm 的情况，底数并不重要，任何底数都是等价的：Binary search: $\Theta(\log N)$ 直接忽略底数符号。Selection sort: $\Theta(N^2)$Merge two sorted array (Merge Sort): $\Theta(N)$ 用 merge sort 加速 selection sort - 把 selection sort 递归地平分, 总共能分解出$\log_2N$个 merge sorts, 伪代码:123456If the list is size 1: returnelse: Mergesort the left half Mergesort the right half Merge the results Total runtime is $≈Nk$, where $k = \log_2(N)$ is the number of levels, overall runtime is $\Theta(N \log N)$.$N^2$ vs. $N \log N$ is an enormous difference. Going from $N\log N$ to $N$ is nice, but not a radical change. Useful math:$1 + 2 + 3 + … + N = N * (N + 1) / 2 = \Theta(N^2)$$1 + 2 + 4 + … + N = 2N - 1 = \Theta(N)$ Big OBig Theta expresses the exact order of as a function of the input size. However, if the runtime depends on more than just the size of the input, then we must qualify our statements into different cases before using Big Theta. Big O: $R(N) \in O(f(N))$, means that there exists positive constants $k_2$, such that: $R(N) \leq k_2 \cdot f(N)$ for all values of $N$ greater than some $N_0$(a very large $N$). This is a looser condition than Big Theta since Big O does not care about the lower bound, thus it is less informative than Big Theta. To summarize the usefulness of Big O: It allows us to make simple statements without case qualifications, in cases where the runtime is different for different inputs. Sometimes, for particularly tricky problems, we (the computer science community) don’t know the exact runtime, so we may only state an upper bound. It’s a lot easier to write proofs for Big O than Big Theta, like we saw in finding the runtime of mergesort in the previous chapter. This is beyond the scope of this course. 类似的也可以定义一个下限概念 - Big Omega ($\Omega$)， 一般用于表明一个问题的难度有多大。 Summary Big O is an upper bound (“less than or equals”) Big Omega is a lower bound (“greater than or equals”) Big Theta is both an upper and lower bound (“equals”) Big O does NOT mean “worst case”. We can still describe worst cases using Big Theta Big Omega does NOT mean “best case”. We can still describe best cases using Big Theta Big O is sometimes colloquially used in cases where Big Theta would provide a more precise statement– from: https://joshhug.gitbooks.io/ Amortized Analysis平摊分析 假如有两种交税方式： 每天付 3 金币 每次付的金币呈指数级增长，但通知付款频率呈指数级下降 第1天：付 1 第2天：付 2 (累计 3) 第4天：付 4 (累积 7) 第8天：付 8 (累积 15) 哪种付的钱比较少？第二种比较划算，本质上等同于每天付 2，就是amortized constant。 A more rigorous examination of amortized analysis is done here, in three steps: Pick a cost model (like in regular runtime analysis) Compute the average cost of the i’th operation Show that this average (amortized) cost is bounded by a constant. 类似的应用在Array list 扩容中提到的 geometric resizing 方法(实际也是Python list 使用的方法)有体现, 所以使用一个因数来扩容数组, 可以让 ArrayList 的 add操作变为 amortized constant time. 总结 Amortized analysis provides a way to prove the average cost of operations. If we chose $a_i$ such that $\Phi_i$ is never negative and $a_i$ is constant for all $i$, then the amortized cost is an upper bound on the true cost.– from: https://joshhug.gitbooks.io/]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 语法和特性 - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-14-other-syntaxes%2F</url>
    <content type="text"><![CDATA[Java 高级语法。 Extends, Casting, Higher Order Functions定义接口之间的层次关系可以使用implement, SLList&lt;xxx&gt; implements List61B&lt;xxx&gt;. 但假如我们想要定义class之间的层次关系呢？ 假设要构建一个RotatingSLList，它具有与SLList相同的功能，如addFirst, size等，但是需要额外的rotateRight操作将最后一项放到列表的前面，因为继承允许子类重用已经定义的类中的代码。所以让RotatingSLList类从SLList继承部分代码:1public class RotatingSLList&lt;Item&gt; extends SLList&lt;Item&gt; &#123;&#125; RotatingSLList“是一种”SLList, extends可以让我们继承SLList的原始功能，并能修改或添加其他功能。123456789/** The rotateRight method takes in an existing list, and rotates every element one spot to the right, moving the last item to the front of the list. For example, input [5, 9, 15, 22] should return [22, 5, 9, 15].*/public void rotateRight() &#123; Item x = removeLast(); addFirst(x);&#125; 通过extends, 子类继承父类的所有成员，成员包括： 所有实例和静态变量 所有方法 所有嵌套类 但注意，构造函数不继承，并且私有成员不能被子类直接访问。 上面的例子使用父类的removeLast()把最后的元素直接丢弃了，但假如有一个子类VengefulSLList想保留被丢弃的元素呢? 考虑到子类可以直接override父类的成员, 可以override父类的removeLast, 通过添加一个实例变量来追踪所有已删除的元素.12345678910111213141516171819public class VengefulSLList&lt;Item&gt; extends SLList&lt;Item&gt; &#123; SLList&lt;Item&gt; deletedItems; public VengefulSLList() &#123; deleteItems = new SLList&lt;Item&gt;(); &#125; @Override public Item removeLast() &#123; Item x = super.removeLast(); deletedItems.addLast(x); return x; &#125; /** Prints deleted items. */ public void printLostItems() &#123; deletedItems.print(); &#125;&#125; Constructors Are Not Inherited Java要求所有子类的构造函数必须先调用其某一超类的构造函数。– https://docs.oracle.com/javase/tutorial/java/IandI/super.html 因为逻辑上，如果作为基础的超类没有构建，那么子类的构建的无从谈起。完整的子类构造函数应该是：1234public VengefulSLList() &#123; super(); // 第一行 deletedItems = new SLList&lt;Item&gt;();&#125; 之前的例子没有super();也可以通过编译，是因为Java会自动为我们调用超类的无参数构造函数。 具体分情况考虑： 编译器会自动为任何没有构造函数的类提供一个无参数的默认构造函数：这个默认构造函数将调用其超类的（accessible）无参构造函数。 如果子类构造函数没有指定要调用哪个超类构造函数：则编译器将自动调用超类的可访问的无参数构造函数 12public class Base &#123; &#125;public class Derived extends Base &#123; &#125; 如果其超类有有参数构造函数，但没有无参数构造函数，那么编译出错： 1public class Base &#123; public Base(String s) &#123; &#125; &#125; 此时要在子类构造函数第一行添加super(s) 如果超类的无参数构造函数是不可访问的，那么编译出错： 1public class Base &#123; private Base() &#123; &#125; &#125; 如果没有显式的超类，那么就调用隐式的超类Object的无参构造函数。 Constructor Chaining：当子类构造函数调用其父类的构造函数时（无论是显式还是隐式调用），可以认为有一链式的连续调用构造函数，一直到Object的构造函数 The Object ClassJava中的每个类都是 Object class的后代，或者扩展了Object类。即使在类中没有显式的extends仍然隐式地继承了Object。也就是所有 classes 都继承了 Object class 提供的方法:123456789101112String toString()boolean equals(Object obj)Class&lt;?&gt; getClass()int hashCode()protected Object clone()protected void finalize()void notify()void notifyAll()void wait()void wait(long timeout)void wait(long timeout, int nanos)-- https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html ==检查两个变量是否引用同一个对象（检查内存地址位）; .equals()默认是等同于==, 但不同的类可能会通过 override 重写它的功能(如Array.equals()就是重写为检查数组内容是否相同). 当override .equals()时，注意： 必须体现 equivalence relation reflexive: x.equals(x) is true symmetric: x.equals(y) IFF y.equals(x) transitive: x.equals(y) and y.equals(z) implies x.equals(z) 要 override 原本的.equals()，必须接收一个 Object 参数 必须 consistent：假如x.equals(y), 那么只要x和y保持不变, 那么x继续等于y. null永远非真: x.equals(null) 一定是false Interfaces don’t extend Object.– http://docs.oracle.com/javase/specs/jls/se7/html/jls-9.html#jls-9.2 Encapsulation封装是面向对象编程的基本原则之一，也是程序员处理复杂性一个方法。管理复杂性是编写大型程序时必须面对的主要挑战之一。 对抗复杂性的一些工具包括: Hierarchical abstraction: 创建一个个具有明确的 abstraction barriers 的抽象层 Abstraction Barriers：使用private, 保证对象内部不能被查看, 确保底层的复杂性不会暴露给外部世界。 “Design for change” (D. Parnas) Organize program around objects. Let objects decide how things are done. Hide information others don’t need. 大概的想法都是 - 程序应该被构建成模块化，可互换的片段，可以在不破坏系统的情况下进行交换。 封装就是构建在这种对外部隐藏信息的概念上。以细胞为类比：细胞内部可能非常复杂，由染色体，线粒体，核糖体等组成，但它完全被封装在一个单一模块中 - 抽象了内部的复杂性。 In computer science terms, a module can be defined as a set of methods that work together as a whole to perform a task or set of related tasks. Now, if the implementation details of a module are kept internally hidden and the only way to interact with it is through a documented interface, then that module is said to be encapsulated. Type Checking123456789public static void main(String[] args) &#123; VengefulSLList&lt;Integer&gt; vsl = new VengefulSLList&lt;Integer&gt;(9); SLList&lt;Integer&gt; sl = vsl; // 超类包含子类 //sl dynamic type is VengefulSLList sl.addLast(50); sl.removeLast(); // 根据 dynamic type 选择 VengefulSLList 的 removeLast sl.printLostItems(); //编译不过, 因为编译时检查的是 static type VengefulSLList&lt;Integer&gt; vsl2 = sl; // 编译不过, 子类无法包含超类&#125; Expressions 是 compile-time types (static), 使用new的表达式具有指定的 compile-time types: SLList&lt;Integer&gt; sl = new VengefulSLList&lt;Integer&gt;();, 表达式右边 compile-time types 是VengefulSLList。编译器检查并保证VengefulSLList一定也是SLList，因此允许此赋值. VengefulSLList&lt;Integer&gt; vsl = new SLList&lt;Integer&gt;();, 表达式右边 compile-time types 是SLList。编译器检查, 因为SLList并不一定是VengefulSLList，故编译报错. Method calls have compile-time types equal to their declared type.123456public static Dog maxDog(Dog d1, Dog d2) &#123; ... &#125;Poodle frank = new Poodle("Frank", 5);Poodle frankJr = new Poodle("Frank Jr.", 15);Dog largerDog = maxDog(frank, frankJr);Poodle largerPoodle = maxDog(frank, frankJr); // 编译不过! RHS compile-time type is Dog 编译器报错, 因为普遍意义的 Dog object 并不总是贵宾犬, 虽然此时我们都知道这里的”狗”肯定是指贵宾犬. 有没有办法让编译器理解这种特殊情况呢? Casting通过 casting, 可以告诉编译器一个表达式有某个特定的 compile-time types.1Poodle largerPoodle = (Poodle) maxDog(frank, frankJr); 编译通过, 右边 compile-time type 转换为 Poodle. Caution: Casting is a powerful but dangerous tool. Essentially, casting is telling the compiler not to do its type-checking duties - telling it to trust you and act the way you want it to. Inheritance CheatsheetVengefulSLList extends SLList means VengefulSLList “is-an” SLList, and inherits all of SLList’s members:总结 Inheritance 的一些要点: 当子类VengefulSLList extends 超类SLList时, 意味着VengefulSLList也”是”SLList, 并继承SLList的所有成员: Variables, methods, nested classes 除了 constructors: Subclass constructors 必须先调用 superclass constructor; 通过 super 调用 overridden superclass methods 和 constructors. 调用 overridden methods 遵循两个规则: 编译器只允许与 static type 符合的行为. 对于 overridden methods, 调用是基于 dynamic type 可以使用 casting 来规避 compiler type checking. Higher Order Functions A higher order function is a function that treats other functions as data. 在 Java 7 及之前的版本, memory boxes (variables) 不能包含指向 functions 的 pointers, 也就是无法给 functions 指定 types. 所以不能像Python一样直接把 function 作为参数传递到另一个 function 中。只能借用 interface：123public interface IntUnaryFunction &#123; int apply(int x);&#125; 123456789101112131415// 定义一个方法public class TenX implements IntUnaryFunction &#123; /* Returns ten times the argument. */ public int apply(int x) &#123; return 10 * x; &#125;&#125;// 高阶方法public static int do_twice(IntUnaryFunction f, int x) &#123; return f.apply(f.apply(x));&#125;// 调用高阶方法System.out.println(do_twice(new TenX(), 2)); Java 8 引入java.util.Function&lt;T, R&gt;接口, 可以接受存储一个函数，&lt;T, R&gt;对应该函数的参数和返回对象 Subtype Polymorphism 多态（polymorphism），是指指相同的消息给予不同的对象会引发不同的动作。 动态多态（dynamic polymorphism）：通过类继承机制和虚函数机制生效于运行期。可以优雅地处理异质对象集合，只要其共同的基类定义了虚函数的接口。 在面向对象程序设计中，多态一般是指子类型多态（Subtype polymorphism）或包含多态（inclusion polymorphism）。一般是通过某种可代换性（ substitutability）与另一个数据类型（超类型，supertype）相关的数据类型，这意味着为在这个超类型的元素上运算而写计算机程序也可以在子类型的元素上运算。 静态多态（static polymorphism）：模板也允许将不同的特殊行为和单个泛化记号相关联，由于这种关联处理于编译期而非运行期，因此被称为“静态”。可以用来实现类型安全、运行高效的同质对象集合操作。C++ STL不采用动态多态来实现就是个例子。 非参数化多态或译作特设多态（Ad-hoc polymorphism）： 函数重载（Function Overloading） 运算符重载（Operator Overloading） 带变量的宏多态（macro polymorphism） 参数化多态（Parametric polymorphism）：把类型作为参数的多态。在面向对象程序设计中，这被称作泛型编程。 子类型反映了类型（即面向对象的接口）之间的关系；而继承反映了一类对象可以从另一类对象创造出来，是语言特性的实现。因此，子类型也称接口继承；继承称作实现继承。 ComparableJava的对象不能直接使用&gt;, &lt;, =进行比较. 在Python或C++中，当应用于不同对象类型时，比较运算符可以重新定义，但Java不支持。但可以借用接口继承，Java提供了一个Comparable接口，以保证任何实现该接口的类包含一个比较方法：1234567/** Return negative if this &lt; o. Return 0 if this equals o. Return positive if this &gt; o.*/public interface Comparable&lt;T&gt; &#123; public int compareTo(T obj);&#125; 当有class需要比较时, 就实现这个接口:123456public class Dog implements Comparable&lt;Dog&gt; &#123; ... public int compareTo(Dog uddaDog) &#123; return this.size - uddaDog.size; &#125;&#125; Comparator除了自然顺序（Natural order，指代Comparable接口定义的compareTo的判断标准，在这里是size），如果我们想用其他方式对狗进行比较排序呢？在Python可以使用HOF，编写新的比较函数，然后直接以参数形式传递该函数。但Java的方案是使用Comparator接口：123public interface Comparator&lt;T&gt; &#123; int compare(T o1, T o2);&#125; 例如，当需要按照狗的名字字母顺序（alphabetical order）排序时，就在 Dog class 内部编写一个实现Comparator接口的嵌套类NameComparator：12345678910111213141516171819import java.util.Comparator;public class Dog implements Comparable&lt;Dog&gt; &#123; ... public int compareTo(Dog uddaDog) &#123; return this.size - uddaDog.size; &#125; // 直接利用 String 已经定义好的 compareTo private static class NameComparator implements Comparator&lt;Dog&gt; &#123; public int compare(Dog a, Dog b) &#123; return a.name.compareTo(b.name); &#125; &#125; public static Comparator&lt;Dog&gt; getNameComparator() &#123; return new NameComparator(); &#125;&#125; 在main函数中调用时12345678Dog d1 = new Dog("Alpha", 3);Dog d2 = new Dog("Beta", 15);java.util.Comparator&lt;Dog&gt; nc = Dog.getNameComparator();if (nc.compare(d1, d2) &gt; 0) &#123; //这里就会按照 alphabet 比较 d1.bark();&#125; else &#123; d2.bark();&#125; 同理若需要增加其他判断标准，就创建新的实现Comparator的 class. Comparable与Comparator接口提供了回调(call back)的能力: 有时一个函数需要调用另一个尚未写好的 helper function, 这时这个 helper function 就是 call back。比如“排序函数”需要“比较函数”的帮助。 不同语言对于回调有不同的处理方式 Python 用函数传递 - Higher Order Functions Java 选择把函数包含在一个接口中 Comparable 是对象自身主动与另一个对象进行比较。它嵌入在待比较的对象内，定义了一种类型的 natural ordering。 Comparator 更像是将两个对象进行比较的第三方对象。由于只有一个compareTo的空间，如果想要支持多种方式进行比较，则必须使用Comparator。 Abstract Data Types (ADTS)ArrayDeque和LinkedListDeque都是实现deque这个接口，deque只是罗列了一些 methods，也即是一种合约，保证会实现的行为。而这些方法的具体实现则是由ArrayDeque和LinkedListDeque完成。从概念上讲，deque就是一种抽象的数据类型，只说会有什么行为，但不体现这些行为的具体实现方式，所以是抽象的。 Java LibrariesJava有一些内置的抽象数据类型，打包在Java库中。 三个最重要的ADTs来自java.util库： List 列表：一个有序的元素集合，如ArrayList Set 集合：元素严格唯一（不重复）的(无序)集合，如HashSet Map 映射：A collection of Key - value 映射, key是唯一的。通过key访问value，如HashMap。 12345678910111213141516171819202122232425262728293031323334/** takes in a String inputFileNameand puts every word from the input file into a list*/public static List&lt;String&gt; getWords(String inputFileName) &#123; List&lt;String&gt; lst = new ArrayList&lt;String&gt;(); In in = new In(); while (!in.isEmpty()) &#123; lst.add(in.readString()); &#125; return lst;&#125;/** takes in a List&lt;String&gt; and counts how many unique words there are in the file.*/public static int countUniqueWords(List&lt;String&gt; words) &#123; Set&lt;String&gt; ss = new HashSet&lt;&gt;(); for (String s : words) &#123; ss.add(s); &#125; return ss.size();&#125;/** takes in a List&lt;String&gt; targets and a List&lt;String&gt; words,and finds the number of times each target word appears in the word list.*/public static Map&lt;String, Integer&gt; collectWordCount(List&lt;String&gt; words) &#123; Map&lt;String, Integer&gt; counts = new HashMap&lt;String, Integer&gt;(); for (String t: target) &#123; counts.put(s, 0); &#125; for (String s: words) &#123; if (counts.containsKey(s)) &#123; counts.put(word, counts.get(s)+1); &#125; &#125; return counts;&#125; 通过设置环境变量（如CLASSPATH =）让Java编译器/解释器知道去哪里找 libraries。 CLASSPATH：Linux or MacOS, paths are separated by :. In Windows, paths are separated by ;. /home/--/--/javalib/*, 在.class和.jar文件内查找依赖包，用于指定绝对路径。有同名时，会根据环境变量的先后顺序去排序靠前的。 ./指当前目录，../指上一层目录，用于指定相对路径。 也可以指定classpath, 这样系统的CLASSPATH会被忽略: javac -cp ./:/home/stuff/:../ Foo.java, 当有重名时, 选择顺序就是指明的路径顺序（当前目录-stuff目录-上一层目录） IntelliJ会忽略CLASSPATH，它会自动调用-cp, 变量是基于当前项目指定的 libraries.12345678910111213/** 查看 IntelliJ 使用的 classpath*/import java.net.URL;import java.net.URLClassLoader;public static void main(String[] args) &#123; ClassLoader cl = ClassLoader.getSystemClassLoader(); URL[] urls = ((URLClassLoader)cl).getURLs(); for(URL url: urls)&#123; System.out.println(url.getFile()); &#125;&#125; 其他ADT堆栈 Stacks 和队列 Queues 是两种类似的线性集合。堆栈是后进先出的ADT：元素总是从数据结构的一端添加或删除。队列是先进先出的ADT. 二者都支持以下操作:push(): 加入peek(): 返回下一个poll(): 返回下一个并删除 Java的Deque(double ended queue, “deck”) 接口融合了堆栈和队列, 支持两端的元素插入和移除. 优先级队列 priority queue 的每个元素都有一个与之关联的优先级，以决定从队列中元素操作的顺序。 Abstract classesInterface: 除非指定access modifier, 否则所有的方法默认都是public （Java 9 支持 private） 可以提供变量, 但都是public static final, 也即没有实例变量 无法实例化 除非指定为default，否则所有方法均为抽象的 一个类可以实现多个接口 根据协议承诺, 实现类必须实现其继承的接口的所有抽象方法; 否则要声明为抽象类. Abstract classes 介于接口和 classes 之间。 方法可以是public或private, 也支持protected和package private. 支持任何类型的变量 无法实例化 除非指定为abstract，否则方法默认是具体的实现 每个类只能实现一个 Abstract classes 抽象类不需要实现其继承的接口所有抽象方法 基本上，接口能做的抽象类都可以做。实际应用中，抽象类通常用于部分地实现接口，在接口和实际的类中间扮演一个中间概念。1234567891011121314151617public interface Car &#123; void move(Speed x); void stop();&#125;public abstract class DeluxeModel implements Car &#123; public double x; ... public void move(Speed x) &#123; ... &#125; public abstract void autoPilot();&#125;// 实现时, 要 override 所有抽象方法public class TeslaX extends DeluxeModel &#123; public void stop() &#123; ... &#125; public void autoPilot() &#123; ... &#125;&#125; 若不确定用哪种, 就优先考虑接口，以降低复杂性。– https://docs.oracle.com/javase/tutorial/java/IandI/abstract.html Packages and JAR同样功能的类可能有多种版本, 或者不同类刚好命名相同。通过 packages 来为每个 classes 提供唯一的标识名称，如java.util. A package is a namespace that organizes classes and interfaces. 在IntelliJ的操作： 创建 package：1, File → New Package2, 选择 package name (i.e. “ug.joshh.animal”) 给 Package 添加(新) Java 文件：1, 右键 package name2, New → Java Class3, 命名 class, 然后 IntelliJ 会自动把文件放进正确的路径, 并添加 package declaration. 移动其他.java文件到 Package1, 在文件顶部声明 package [packagename]2, 将文件存储在（移动到）与 package name 对应的文件夹中：如ug.joshh.animal 对应ug/joshh/animal文件路径. 注意, 不存在sub-package这种概念, 即ug.joshh.Animal和ug.joshh.Plant是完全不同的. Default packages没有在文件顶部明确指明 package name 的Java类默认属于 default package 的一部分。 一般而言, Java文件应该以明确的 package 声明开头以避免将文件留在 default package 中（除非它是一个非常小的示例程序）。因为来自 default package 的代码无法 import，并且可能会意外地在 default package 下创建相同名称的类。 JAR Files一般情况下，程序会包含多个.class文件。如果想共享此程序，可以把压缩成一个.jar文件，此.jar文件将包含程序所有.class文件以及其他附加信息。JAR文件就像zip文件一样, 可以将文件解压缩回.java文件。JAR文件并不会加密保护代码. Creating a JAR File (IntelliJ) Go to File → Project Structure → Artifacts → JAR → “From modules with dependencies” Click OK a couple of times Click Build → Build Artifacts (this will create a JAR file in a folder called “Artifacts”) Distribute this JAR file to other Java programmers, who can now import it into IntelliJ (or otherwise) Access Controlcs61b Josh Hug:Private Only code from the given class can access private members. Package Private The default access given to Java members if there is no explicit modifier written. Classes that belong in the same package can access, but not subclasses! Protected Classes within the same package and subclasses can access these members, but the rest of the world (e.g. classes external to the package or non-subclasses) cannot! Subtypes might need it, but subtype clients will not. Public Open and promised to the world, once deployed, the public members’ signatures should not change.就像承诺和合同，尽量不要更改，以便用户始终可以（用已有的代码）访问。如果开发者要舍弃某一个Public，一般标识为deprecated. 细节: Access is Based Only on Static Types 接口的方法默认是public的 Build Systems可以简单地将文件放入适当的位置，然后通过 Maven, Ant 和 Gradle 等工具使用 Build Systems 来自动设置项目, 省去了手动加载一长串 libraries. 丰富扩展generics的语法或特性Autoboxing在Java中调用包含 Generics 的class时，需要提供确切的类型参数。对于每一种 primitive type (byte, short, int, long, float, double, boolean, char)，必须要用其对应的 reference type (Byte, Short, Integer, Long, Float, Double, Boolean, Character) - 也即是 wrapper classes 作为泛型的实际类型参数。虽然声明函数和变量时必须要用 wraper classes，但在实际的数值传递中，对于 primitives 类型的数据，并不需要显示地转换为 reference types。 因为 Java 有 Autoboxing，可以隐式地在 wrapper/primitives 类型间转换. Java会自动 “box” 和 “unbox” primitive type 和其对应的 reference type 之间的值。也就是说，如果Java期望的是 wrapper classes （如Integer），假如即使接收到的是 int 这样的基本类型，Java也会“autoboxing”这种整数。12345public static void blah(Integer x) &#123; System.out.println(x);&#125;int x = 20;blah(x); // 实际上会转换为 blah(new Integer(20)) 反过来就是unboxing。 Autoboxing/Unboxing 注意事项: 不适用于 array 数组 有性能负担 Wrapper types 比 primitive types 占用更多内存: 在大多数现代的系统里，对象的引用地址占用64位，还需要额外的64位开销用于存储动态类型等信息。 更多信息参考 Memory usage of Java objects: general guide 或 Memory Usage Estimation in Java. WideningJava会根据需要在 primitive types 之间自动扩展.12345public static void blahDouble(double x) &#123; System.out.println(“double: “ + x);&#125;int x = 20;blahDouble(x); //等同于 blahDouble((double) x) 但如果想从一个 wider type 转换为 narrower type，则必须手动 cast.有关 widening 的更多详细信息，包括哪些类型比其他类型更 wider ，参阅官方的Java文档。 Generic Methods泛型方法的语法用map数据类型举例. 假如有一个现成的映射数据结构Map61B, 我们需要为她写两个helper methods放在MapHelper里面. get(Map61B, key)：返回映射中给定key对应的值（如果存在），否则返回null。假如这样写public static V get(Map61B&lt;K, V&gt; map, String key) { ... }会报错. 要将方法声明为泛型，必须在返回类型前面指定正式的类型参数123456789public static &lt;K,V&gt; V get(Map61B&lt;K,V&gt; map, K key) &#123; if map.containsKey(key) &#123; return map.get(key); &#125; return null;&#125;ArrayMap&lt;Integer, String&gt; isMap = new ArrayMap&lt;Integer, String&gt;();System.out.println(mapHelper.get(isMap, 5)); //Java可以自行推断`isMap`的数据类型. maxKey: 返回给定ArrayMap中所有keys的最大值（仅在key可以比较的情况下）。实现时要注意, 不能直接用&lt;,&gt;,==来比较, 因为key不一定是primitives. 需要用Comparable接口的compareTo,12345678910public static &lt;K extends Comparable&lt;K&gt;, V&gt; K maxKey(Map61B&lt;K, V&gt; map) &#123; List&lt;K&gt; keylist = map.keys(); K largest = map.get(0); for (K k: keylist) &#123; if (k.compareTo(largest)) &#123; largest = k; &#125; &#125; return largest;&#125; K extends Comparable&lt;K&gt; 保证了keys必须实现Comparable接口（也是一个generic接口）, 并可以与其他K进行比较。 这里没有使用implement, 而是用extends, 这里跟前面的polymorphism不同. K extends Comparable&lt;K&gt;是type upper bounding, 意味着k必须是一种Comparable, 但不需要具备Comparable的所有方法行为. 在inheritance的含义中，extends指为子类提供超类的能力. 在generic范畴内, extends只是陈述一个事实：该类是其扩展的类的一个子类, 是加了约束, 而不是赋予能力. Immutability An immutable data type is a data type whose instances cannot change in any observable way after instantiation. 比如String是immutable, Array是mutable. 防止变量在第一次赋值后被更改 可以使用final: 在 class constructor 里面, 或者变量初始化时, 给变量赋值一次, 之后就无法再被赋值了. 要保证immutable不一定要使用final, 有时候也可以用private. Immutable data types 因为属性不能改变, 缺点是需要创建一个新对象才能更改属性，优点是： 可以防止bugs, 并使debugging更容易 可以信赖对象具有某种行为/特质 注意： 将一个引用声明为final并不会保证引用指向的对象是immutable. public final ArrayDeque&lt;String&gt;() deque = new ArrayDeque&lt;String&gt;();变量deque是final的, 仅意味着不能重新被赋值, 但其指向的数组队列对象自身还是可变的. 使用Reflection API，甚至可能对private变量进行更改 Throwing Exceptions当程序出现错误时，假如继续运行下去已经没有意义（或者根本不可能继续），那么我们就想要中断正常的控制流程 - throws an exception。 比如当想从某ArrayMap中提取某个不存在的键值时, java自动抛出一个implicit exception1234$ java ExceptionDemoException in thread "main" java.lang.ArrayIndexOutOfBoundsException: -1at ArrayMap.get(ArrayMap.java:38)at ExceptionDemo.main(ExceptionDemo.java:6) 如果想让自己的程序抛出更详细的信息, 可以在程序中加入explicit exception1234567public V get(K key) &#123; intlocation = findKey(key); if(location &lt; 0) &#123; throw newIllegalArgumentException("Key " + key + " does not exist in map."\); &#125; return values[findKey(key)];&#125; 1234$java ExceptionDemoException in thread "main" java.lang.IllegalArgumentException: Key yolp does not exist in map.at ArrayMap.get(ArrayMap.java:40)at ExceptionDemo.main(ExceptionDemo.java:6) 单纯 throw exception 会导致代码崩溃。但是通过 try - catch “捕捉”异常(RuntimeException 是 Java object), 可以防止程序崩溃。 比如通过捕捉异常, 来引入修正措施: 下面这个代码通过1234567891011121314/** 当狗在生气时, 如果尝试拍拍它，会抛出一个 RuntimeException，捕捉到 exception 后, 用香蕉来抚慰它. */Dog d = new Dog("Lucy", "Retriever", 80);d.becomeAngry();try &#123; d.receivePat();&#125; catch (Exception e) &#123; System.out.println( "Tried to pat: " + e); d.eatTreat("banana");&#125;d.receivePat();System.out.println(d); 123$ java ExceptionDemoTried to pat: java.lang.RuntimeException: grrr... snarl snarlLucy is a displeased Retriever weighing 80.0 standard lb units. 1234567$ java ExceptionDemoTried to pat: java.lang.RuntimeException: grrr... snarl snarlLucy munches the bananaLucy enjoys the pat.Lucy is a happy Retriever weighing 80.0 standard lb units. 使用if else来管理异常会让代码变得很乱而难以阅读. 而使用try catch可以为每种类型的 exception 提供不同的应对。使代码像清晰的记述文般铺展开来: 首先，尝试执行所需的操作。然后，捕捉任何错误。1234567try &#123; ...&#125; catch (...) &#123; doSomething;&#125; catch (...) &#123; doSomething;&#125; ... 好的代码像一个故事, 构建上有一定的美感。这种清晰度使代码的长期维护变得更容易。 Uncaught Exceptions Stack Trace如果 exception 到达堆栈底部后仍未被捕获，程序崩溃，Java 打印出堆栈的跟踪:1234java.lang.RuntimeException in thread “main”:at ArrayRingBuffer.peek:63at GuitarString.sample:48at GuitarHeroLite.java:110 程序猿可以据此追踪错误路径。 Checked vs Unchecked Exceptions有时候，某些抛出的 exception 无法通过编译，可以理解为这些异常在编译器看来是非常恶心的存在，需要程序猿必须给这些 exception 提供明确的应对处理方案 - 这种叫 checked exception （”must be checked”）。123456public class Eagle &#123; public static void gulgate() &#123; if (today == “Thursday”) &#123; throw new IOException("hi"); &#125; &#125;&#125; 1234$ javac EagleEagle.java:4: error: unreported exception IOException; must be caught or declared to be thrownthrow new IOException("hi"); &#125;^ 很明显，Java对此IOException并不满意, 因为IOExceptions是 checked exception, 而这里没有提供应对处理方案。但假如换做RuntimeException就可以编译通过 (虽然在 runtime 时会崩溃).Errors 和 Runtime Exceptions, 以及它们的子类都是unchecked. 这种异常都是直到运行时才能发现的错误, 它们一般无法从中恢复到正常状态 - 比如代码尝试从数组中获取-1元素，好像就没什么解决办法。 其余的都是 checked exception, 它们其中大多数都有修正的可能性。例如遇到FileNotFound，可以考虑要求用户重新指定他们想要的文件 (可能是因为错误输入导致的)。 Java在尽最大努力确保每个程序运行时不会崩溃，所以它不会允许程序留下任何明明可以应对修正却没有被明确地修正的错误。 两种方法来处理 checked error: Catch 123456789public static void gulgate() &#123; try &#123; if (today == “Thursday”) &#123; throw new IOException("hi"); &#125; &#125; catch (Exception e) &#123; System.out.println("psych!"); &#125;&#125; 假如能够应对，尽量用 catch 锁定异常防止其逃逸。 Specify: 如果实在不想在该方法中处理这种异常，可以将责任推迟到别的地方。我们可以指定该方法是危险的 123public static void gulgate() throws IOException &#123; ... throw new IOException("hi"); ...&#125; 然后任何其他调用gulgate()的方法也变成危险的了, 它们也需要被处理(同样使用两种方法之一) 123456789101112// catchpublic static void main(String[] args) &#123; try &#123; gulgate(); &#125; catch(IOException e) &#123; System.out.println("Averted!"); &#125;&#125;// 或 specifypublic static void main(String[] args) throws IOException &#123; gulgate();&#125; 需要明确异常处理责任人。同时确保调用者知道该方法是危险的！ IterationJava提供了 foreach (enhanced for) 的循环简写语法:12345ArrayMap&lt;String, Integer&gt; am = new ArrayMap&lt;String, Integer&gt;();for (String s : am) &#123; System.out.println(s);&#125; 实现的关键原理是使用Iterable接口使一个类变成可迭代的: 该接口包含一个iterator()方法用于返回一个Iterator对象。Iterator接口定义Iterator对象和hasNext(), next()方法来进行实际的迭代操作。123456789101112131415161718192021222324252627public class ArrayMap&lt;K, V&gt; implements Map61B&lt;K, V&gt;, Iterable&lt;K&gt; &#123; private K[] keys; private V[] values; int size; public ArrayMap() &#123; keys = (K[]) new Object[100]; values = (V[]) new Object[100]; size = 0; &#125; @Override public Iterator&lt;T&gt; iterator() &#123; return new KeyIterator(); &#125; public class KeyIterator implements Iterator&lt;K&gt; &#123; private int ptr; public KeyIterator() &#123; ptr = 0; &#125; public boolean hasNext() &#123; return (ptr != size); &#125; public K next() &#123; K returnItem = keys[ptr]; ptr = ptr + 1; return returnItem; &#125; &#125;&#125; 不同的数据结构，Iterator有不同的实现方式. KeyIterator即使是private也可以编译, 因为iterator()在这里是public的:1234567891011import java.util.Iterator;public class Demo&#123; public static void main(String[] args) &#123; ArrayMap&lt;String, Integer&gt; am = new ArrayMap&lt;String, Integer&gt;(); Iterator&lt;String&gt; it = am.iterator(); for (String s : am) &#123; ... &#125; &#125;&#125; 除了用嵌套类来自定义实现Iterator, 也可以利用数据结构本身的特性. 比如ArrayMap里面刚好包含一个可迭代的数据结构List keys1234public Iterator&lt;T&gt; iterator() &#123; List&lt;K&gt; keylist = keys(); return keylist.Iterator();&#125; 总结Java的特性 Packages Good: Organizing, making things package private Bad: Specific Static type checking Good: Checks for errors early , reads more like a story Bad: Not too flexible, (casting) Inheritance. Good: Reuse of code Bad: “Is a”, the path of debugging gets annoying, can’t instantiate, implement every method of an interface]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software Architecture, Process, and Management - Informatics - University of Edinburgh 爱丁堡大学]]></title>
    <url>%2FUoE-sapm%2F</url>
    <content type="text"><![CDATA[Reference:microsoftIBMSoftware Architecture in Practice (3rd edition), Bass, Clements, and Kazman What is Software Architecture?Software architecture is often described as the organization or structure of a system, where the system represents a collection of components that accomplish a specific function or set of functions. grouping components into areas of concern (layers): For example, the UI, business processing, and data access. focus on interaction between the components and how different components work together. 在书中的定义： The software architecture of a system is the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both.– Software Architecture in Practice (3rd edition), Bass, Clements, and Kazman 架构的关注点在于系统内各个应用和模块的交互和调用。软件架构的设计，需要考虑满足什么样的需求（用户或甲方），如何解决和优化问题（不同的方向各有偏重），操作中如何做选择（在不同的方面平衡，妥协）。 Architecture and design concerns very often overlap - The selection of data structures and algorithms or the implementation details of individual components are design concerns. 没必要强硬区分二者，而应该综合起来看待。某些情况下，决策是自然而然的结构层面的；在某些情况下，决策更多是关于于设计层面，以及设计如何帮助实现架构。 软件架构的定义有如下隐含意思：1, Architecture Is a Set of Software StructuresThree frequently occurring types of structure:– Modular structure: static structure that focus on how functionality is divided up, structured, and assigned to development and implementation teams.– Component and Connector structure: runtime structures that focus on how components interact (e.g. information passing, synchronisation, interference,…)– Allocation structures: mapping to organizational, development, installation, execution environments. (e.g. Components are deployed onto hardware to execute) 2, Architecture Is an abstractionArchitecture specifcally omits certain information about elements that is not useful for reasoning about the system - in particular, it omits information that has no ramifcations outside of a single element. 3, Every Software System Has a Software architecture 4, Architecture Includes behaviorBehavior embodies how elements interact with each other. 在本课程, 一个软件项目成功与否, 基于如下三点考量:– The software is delivered on schedule– Development costs were within budget– The software meets the needs of users Contexts for Software Architecture Technical: where architecture supports technical activity like measurement, V&amp;V, compliance,… Controlling Quality Attributes Availability - ensuring there is a system to take over if a system fails. Safety - ensuring that the system only behaves as intended and has no additonal behaviour. Testability - ensuring: elements are clearly able to be isolated we know what behaviour to expect of components of the system we know how components relate to modules so we can track down faulty code We know how components are intended to integrate to give the overall behaviour Other qa: performance, usability, interoperability,.. Design - Patterns, Styles, Domain Specific Architecture (DSSA) A DSSA is collection of (pre-decided) design decisions that: Capture important aspects of particular tasks (domain), Common across a range of systems in the domain Typically they will have some predefined structures These are not general purpose because they incorporate many specific characteristics of the domain. Architectural pattern is a set of architectural design decisions that are applicable to a recurring design problem, and parameterized to account for different software development contexts in which that problem appears. Similar to DSSAs but capture less of the behaviour and attributes of the system More general because they are intended to abstract a common pattern over several domains. Three-Tiered Pattern: State(database)-Logic(Business)-Display(UI) Model-View-Controller (MVC): to separate between information, presentation and user interaction. Sense-Compute-Control: Structuring embedded control applications Project lifecycle: where architecture interacts with and supports development process Lifecycle Models: V-model, iterative models (Boehm’s spiral model), Agile Business: where architecture supports organisations, e.g. customer organisations and development organisations. Professional: where the role of architect defines requirements and constraints on architects. Quality Attributes (QA)Architecture is the right level of abstraction to resolve conflicts between Stakeholders. Quality Attributes specify, usually quantitative, requirements on particular bits of functionality or on the whole systems (e.g. that the system should be available 99% of the time). Problems With QA1, Often QA requirements are not “testable”, for example modifiable, usable, dependable or resilient.2, It can be difficult to map from a concern about the system to a QA. For example, a high failure rate in some transaction could be a performance issue or it could be an availability issue.3, Communities around a particular quality attribute have developed their own terminology (e.g. security has attacks, performance has events, etc). The solution for 1 and 2 is to use quality attribute scenarios to provide sufficient specificity to avoid some of these issues. Quality Attributes Scenarios 场景A quality attribute requirement should be unambiguous and testable. To specify quality attribute requirements, we capture them formally as six-part scenarios: Source of stimulus. This is some entity (a human, a computer system, or a system administrator) that generated the stimulus. Stimulus. A condition (event) that requires a response when it arrives at a system. e.g. a user operation to the usability community, or an attack to the security community. Environment. The stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation. For many systems, “normal” operation can refer to one of a number of modes. For these kinds of systems, the environment should specify in which mode the system is executing. Artifact. A collection of systems, the whole system, or part of the system that is stimulated e.g. the configuration checker in the system. Response. The response is the activity undertaken as the result of the arrival of the stimulus. e.g. the configuration issue is identified and then repaired. Response measure. how to measure the response so the scenario is testable. e.g. time to detect the wrong configuration and the time to repair. Each QA has a General Scenario associated with it that tries to capture the possible components involved in that particular QA. This acts as a template or guide for the architect specifying a specific QA Scenario. Specific QA Scenarios take account of specific stimuli and measures on response, they capture the specification of the QA for a particular system. Achieving QA through tacticsArchitectural tactics are design decisions to achieve the required quality attributes, more specifcally, to control responses to stimuli. The focus of a tactic is on a single quality attribute response. Within a tactic, there is no consideration of tradeoffs (differ from architectural patterns, where tradeoffs are built into the pattern). By cataloging tactics, we provide a way of making design more systematic within some limitations. An architecture can be viewed as the result of applying a collection of design decisions. A systematic categorization of these decisions: Allocation of responsibilities 责任分配: Identifying the important responsibilities, and determining how these responsibilities are allocated to static and runtime elements (namely, modules, components, and connectors). Coordination model 模型协调 - Components in the architecture interact with one another via a collection of mechanisms. What elements in the system need to coordinate with one another. What properties the coordination needs to have (e.g. timing properties, security of coordination, …) Choosing the mechanisms (ideally a small number) that realize properties like statefulness, synchrony, delivery guarantees, performance. Data model: Every system must represent artifacts of system-wide interest—data—in some internal fashion Choosing abstractions, operations, and properties. How data is created and destroyed, access methods, … Maintaining metadata that controls the interpretation of the data. Organising the data, what kind of system will be used to store it, how will it be backed up, how do we recover from data loss Management of resources: hard (CPU, memory, battery, I/O ports…) or soft resources(system locks, software buffers, thread pools…): Identifying resources to be managed What system element should manage a resource 资源共享策略和争端仲裁 Work out sharing strategies and how to arbitrate in contention situations Consider the consequences of resource starvation(e.g. Memory). Mapping among architectural elements two important types of mapping: Mapping between different types of elements in the architecture, e.g. from static development structures (modules) to execution elements e.g. threads or processes. Mappings between software elements and environment elements, e.g. from processes to specific processors and other hardware. Useful mappings include: code to runtime structure; runtime elements to environment; data model elements to data stores. Binding time decisions: introduce allowable ranges of variation. This variation can range from design time by a designer to runtime by an end user might allocate a responsibility. The decisions in the other six categories have an associated binding time decision: we might want some variability in the resources to be managed determined at run time or we might make the coordination model negotiable at runtime if we want to inter-operate with a range of systems. Choice of technology: critical to being able to realize all the other decisions in a concrete system. What technologies are available What tools are available to support technologies How much training will it take to be able to use a technology? What are the full range of consequences of the choice of a technology (e.g. it may restrict markets because it is incompatible with some other technologies). If the technology is new, how does it fit into the existing preferred technologies for the organisation. Availability Availability refers to a property of software that it is there and ready to carry out its task when you need it to be. The availability of a system is usually defined to be the probability it will be there when you ask it to work: $\frac{mtbf}{mtbf+mttr}$ $mtbf$ – mean time between failures: MTBF of a component is the sum of the lengths of the operational periods divided by the number of observed failures: $mtbf = \frac{t}{N(t)}$, $t$ is the cumulative operating time, $N(t)$ is the observed number of failures by time $t$. 假设恒定的故障率 $\lambda$，则 $mtbf = \frac{1}{\lambda}$ $mttr$ – mean time to repair Availability measures the quality of service in terms of running versus down time Reliability indicates the fraction of all attempted operations that complete successfully. The reliability of the system is: $R(t) = e^{-\lambda t}$ where the parameter $\lambda$ is called the failure rate.由于MTBF主要针对可以修复的系统，因此建议针对不可修复的系统（在故障后选择更换而不是修复系统的情况）使用平均故障时间（MTTF），在数学上二者是等价的。MTTF: Mean Time To (first) Failure, or Expected Life.$ MTTF = E(t_f) = \int_0^\infty R(t)dt = \frac{1}{\lambda}$ Faults, Errors, Failures: A fault is something in the system (e.g. failed component, wrong bit of code,…) that can cause the system to move into an error state when the fault is activated, an error may then eventually cause an externally observable deviation from the intended operation - failure. Generic Scenario Design Checklist for AvailabilityAllocation of Responsibilities■ Determine the system responsibilities that need to be highly available.■ Within those responsibilities, ensure that additional responsibilities have been allocated to detect an omission, crash, incorrect timing, or incorrect response.■ Additionally, ensure that there are responsibilities to do the following:• Log the fault• Notify appropriate entities (people or systems)• Disable the source of events causing the fault• Be temporarily unavailable• Fix or mask the fault/failure• Operate in a degraded mode Coordination ModelDetermine the system responsibilities that need to be highly available. With respect to those responsibilities, do the following:■ Ensure that coordination mechanisms can detect an omission, crash, incorrect timing, or incorrect response. For example, whether guaranteed delivery is necessary. Will the coordination work under conditions of degraded communication?■ Ensure that coordination mechanisms enable the logging of the fault, notification of appropriate entities, disabling of the source of the events causing the fault, fxing or masking the fault, or operating in a degraded mode.■ Ensure that the coordination model supports the replacement of the artifacts used (processors, communications channels, persistent storage, and processes). For example, does replacement of a server allow the system to continue to operate?■ Determine if the coordination will work under conditions of degraded communication, at startup/shutdown, in repair mode, or under overloaded operation. For example, how much lost information can the coordination model withstand and with what consequences? Data Model■ Determine which portions of the system need to be highly available.■ Within those portions, determine which data abstractions, along with their operations or their properties, could cause a fault of omission, a crash, incorrect timing behavior, or an incorrect response.■ For those data abstractions, operations, and properties, ensure that they can be disabled, be temporarily unavailable, or be fxed or masked in the event of a fault.■ For example, ensure that write requests are cached if a server is temporarily unavailable and performed when the server is returned to service. Mapping among Architectural Elements■ Determine which artifacts (processors, communication channels, persistent storage, or processes) may produce a fault.■ Ensure that the mapping (or remapping) of architectural elements is ﬂexible enough to permit the recovery from the fault. This may involve a consideration of the following:• Which processes on failed processors need to be reassigned at runtime• Which processors, data stores, or communication channels can be activated or reassigned at runtime• How data on failed processors or storage can be served by replacement units• How quickly the system can be reinstalled based on the units of delivery provided• How to (re)assign runtime elements to processors, communication channels, and data stores• When employing tactics that depend on redundancy of functionality, the mapping from modules to redundant components is important. For example, it is possible to write one module that contains code appropriate for both the active component and backup components in a protection group. Resource Management■ Determine what critical resources are necessary to continue operating in the presence of a fault.■ Ensure there are suffcient remaining resources in the event of a fault to log the fault; notify appropriate entities (people or systems); disable the source of events causing the fault; be temporarily unavailable; fx or mask the fault/failure; operate normally, in startup, shutdown, repair mode, degraded operation, and overloaded operation.■ Determine the availability time for critical resources, what critical resources must be available during specifed time intervals, time intervals during which the critical resources may be in a degraded mode, and repair time for critical resources. Ensure that the critical resources are available during these time intervals.■ For example, ensure that input queues are large enough to buffer anticipated messages if a server fails so that the messages are not permanently lost. Binding Time■ Determine how and when architectural elements are bound.■ If late binding is used to alternate between components that can themselves be sources of faults (e.g., processes, processors, communication channels), ensure the chosen availability strategy is suffcient to cover faults introduced by all sources.■ For example:• If late binding is used to switch between artifacts such as processors that will receive or be the subject of faults, will the chosen fault detection and recovery mechanisms work for all possible bindings?• If late binding is used to change the defnition or tolerance of what constitutes a fault (e.g., how long a process can go without responding before a fault is assumed), is the recovery strategy chosen suffcient to handle all cases? For example, if a fault is ﬂagged after 0.1 milliseconds, but the recovery mechanism takes 1.5 seconds to work, that might be an unacceptable mismatch.• What are the availability characteristics of the late binding mechanism itself? Can it fail? Choice of Technology■ Determine the available technologies that can (help) detect faults, recover from faults, or reintroduce failed components.■ Determine what technologies are available that help the response to a fault (e.g., event loggers).■ Determine the availability characteristics of chosen technologies themselves: What faults can they recover from? What faults might they introduce into the system? PerformanceTo ensure resource is effectively monitored and managed. Design Checklist for PerformanceAllocation of Responsibilities■ Work out areas responsibility of that require heavy resource use to ensure time-critical events take place.■ Work out processing requirements.■ Take account of:• Responsibilites arising from threads crossing boundaries of responsibility• Responsibilities for thread management• Responsibilities for scheduling shared resources Coordination Model■ What needs to coordinate.■ Is there concurrency? Ensure it is safe.■ Ensure coordination is appropriate for the style of stimulus.■ Ensure the properties of the coordination model are good for the stimuli and concurrency control? Data Model■ Determine what parts of the data model will be heavily loaded or behaves tight time constraints.■ For those data abstractions, determine:• Would keeping multiple copies help?• Would partitioning the data help?• Is it possible to reduce processing requirements for the data?• Does adding resource help deal with data bottlenecks? Mapping Among Architecture Elements■ Does colocation of some components reduce latencies?■ Ensure components with high processing needs are allocated to big processors■ Consider introducing concurrency when you map.■ Consider whether some mappings introduce bottlenecks (e.g. allocating non-interfering tasks to the same thread) Resource Management■ Work out what needs high levels of resource■ Ensure these are monitoredand managed under all operating modes.■ For example:• Time critical components• Thread management• Prioritization• Locking and scheduling strategies• Deploying additional resource to meet elevated load. Binding time■ Look at when you bind.■ Consider the cost of binding at different times■ Try to avoid performance penalties caused by late binding. Choice of Technology■ Is the technology right to let you meet hard deadlines and resource use (e.g. use a real-time OS with proper scheduling).■ Do you know its characteristics under load and its limits?■ Does your choice of technology give you the ability to set the following:• Good scheduling• Priorities• Policies for demand reduction• Allocating processing to tasks• Other performance-related parameters.■ Does your choice of technology introduce excessive overhead for heavily used operations? Security最简单的表征安全的三个特征 - confdentiality, integrity, and availability (CIA): 机密性 Confidentiality: Only those who should have access are given access. 完整性 Integrity: Data or services are not subject to unauthorised manipulation. 可用性 Availability: the system is available for legitimate use. 其他用于支撑 CIA 的特征: 认证识别 Authentication verifes the identities of the parties to a transaction and checks if they are truly who they claim to be. 不可否认性 Nonrepudiation guarantees that the sender of a message cannot later deny having sent the message, and that the recipient cannot deny having received the message. 授权 Authorization grants a user the privileges to perform a task. Security General Scenario A Design Checklist for SecurityAllocation of Responsibilities■ Ensure all actors have identities■ Authenticate identities■ Check authorizations■ Ensure authorization is required for all such actors■ Log attempts, successes and failures on all sensitive operations■ Ensure data is encrypted■ Ensure responsibilities are allocated to appropriate actors. Coordination Model■ Ensure coordination mechanisms use authentication and authorisation.■ Ensure coordination mechanisms are not vulnerable to impersonation, tampering, interception, …■ Ensure data involved in coordination is protected using encryption.■ Monitor level of demand for communication to identify excessive demands Data Model■ Ensure there is a valid data model that disallows invalid data flows.■ Ensure logging of access, modification and attempted access or modification.■ Data is protected in flight and at rest using appropriate encryption.■ Ensure appropriate backup/recovery mechanisms are in place. Mapping among Architectural Elements■ Explore how different mappings change the way users can access resources.■ Ensure for all of these mappings the models of access and authorisation are preserved.• Actors should be identified and authenticated• Use appropriate authorisation mechanisms• Ensure logging is enabled• Ensure data is protected by encryption• Recognise impact of attack on resources■ Ensure recovery from attack is possible Resource Management■ Explore the overheads resulting from monitoring, detecting, preventing and recovering from attacks.■ Analyse how a user can access and make demands on critical resources.■ Manage resource access to ensure malicious use of resource is detected and managed.■ Identify the potential for corruption/contamination and how to manage this.■ Explore the potential for resource use to be used as a covert channel to transmit data.■ Limit resources used to manage attempts at unauthorised use Binding Time■ Explore the consequences of varying binding times on the ability to trust an actor or component.■ Put in place appropriate mechanisms to ensure trust given binding time.■ Explore potential impact on resource use, capacity/throughput, response time■ Ensure appropriate encryption of all data around binding.■ Explore the potential of variation in binding time as a covert channel. Choice of Technologies■ Ensure limitations of technologies are understood and the potential for future compromise is well identified.■ Ensure your chosen technologies support the tactics you want to deploy to protect the system. ConnectorsKey part of Architectures■ Connect components and define the rules of interaction between components• Simple: shared variable access; method calls; …• Complex: database access; client-server; scheduler; load balancer■ Connectors provide: Interaction ducts; In coding often connectors are implicit, but in software architecture:■ They are identified and have an identity■ Capture system interaction (at the level of components)■ They have a specification that can be complex Relationship between Connectors and components:■ Components have application-specific functionality.■ Connectors provide interaction mechanisms that are generic across different applications.■ Interaction may involve multiple components■ Interaction may have a protocol associated to it. The specification of the connector protocols determine: the types of interface that it works with; properties of interaction; rules about ordering of interaction; measurable features of interaction. Benefits of Explicit Connectors■ Interaction is defined by the arrangement of the connectors (as far as possible)■ Component interaction is defined by the pattern of connectors in the architecture■ Interaction is “independent” of the components The main roles(services) of Connectors are: Communication Information is transmitted between components (e.g. message passing; method call; remote procedure call,…). Connectors constrain things: Direction of flow (e.g. pipes), Capacity, rates of flow, etc. May have other effects e.g. coordination (e.g. blocking I/O) Influences measurable Quality Attributes of the system Separates communication from functional aspects (components do the functional part). Coordination: Controls the timing relationship of functional aspects of the system, e.g. coordinating the arrival of data at a collection of components Conversion How to get components to interact that don’t have the right means of interaction. 如何让兼容性差的组件进行交互？ Incompatibilities might be related to: datatypes, ordering, frequency, structure of parameters etc. Examples of types of converters: Wrappers (deal with structural issues), Adaptors (deal with datatype incompatibilities) Facilitation Enable interaction among a group of components that are intended to interact. Help manage the interaction Examples: load balancer; replication management; redundancy management; scheduler Can also relate to coordination, e.g. synchronization (critical sections; monitors) Select ConnectorsTypes of Connector: • Method/Procedure call • Data access • Events • Stream • Distributor • Arbitrator • Adaptor Selection Determine a system’s interconnection and interaction needs Determine roles to be fulfilled by the system’s connectors: Communication, coordination, conversion, facilitation For each connector Determine its appropriate type(s) Determine its dimensions of interest Select appropriate values for each dimension For multi-type, i.e., composite connectors, determine the atomic connector compatibilities Architectural Patterns An architectural patterns is a package of design decisions that is found repeatedly in practice, has known properties that permit reuse, and describes a class of architectures. An architectural pattern comprises: A context that provides the frame for a problem. A problem that is a generalised description of a class of problems often with QA requirements that should be met. A solution that is suitably generalised in the same way as the problem. A solution: Describes the architectural structures that solve the problem, including how to balance the many forces at work. The solution might be static, runtime or deployment oriented. The solution for a pattern is determined and described by: A set of element types (for example, data repositories, processes, and objects) A set of interaction mechanisms or connectors (for example, method calls, events, or message bus) A topological layout of the components A set of semantic constraints covering topology, element behavior, and interaction mechanisms Module PatternsStatic Pattern: Layered Pattern Overview: The layered pattern defines layers (groupings of modules that offer a cohesive set of services) and a unidirectional allowed-to-use relation among the layers. The pattern is usually shown graphically by stacking boxes representing layers on top of each other. Suitable for controlling static aspects of architecture. Elements: Layer, a kind of module. The description of a layer should define what modules the layer contains and a characterization of the cohesive set of services that the layer provides. Relations: Allowed to use, which is a specialization of a more generic depends-on relation. The design should define what the layer usage rules are (e.g., “a layer is allowed to use any lower layer” or “a layer is allowed to use only the layer immediately below it”) and any allowable exceptions. Constraints:■ Every piece of software is allocated to exactly one layer.■ There are at least two layers (but usually there are three or more).■ The allowed-to-use relations should not be circular (i.e., a lower layer cannot use a layer above). Weaknesses:■ The addition of layers adds up-front cost and complexity to a system.■ Layers contribute a performance penalty. Component-and-Connector PatternsModel-View-Controller Pattern Context: User interface software is typically the most frequently modifed portion of an interactive application. For this reason it is important to keep modifcations to the user interface software separate from the rest of the system. Problem:• Isolating the UI functionality from the Application functionality.• Maintaining multiple views in the presence of change in the underlying data. Solution: Other Component-Connector Patterns• Pipe and Filter Pattern• Broker Pattern• Client-Server Pattern• Peer-to-Peer Pattern• Service-Oriented Architecture Pattern• Publish-Subscribe Pattern• Shared Data Pattern Deployment/Allocation PatternsContext:– we are concerned with resource use– We might consider flexible deployment of resource– The QAs we care about are sensitive to the pattern of deployment and the use of resources. Allocation: Map-Reduce PatternContext:– We have large quantities of data we wish to treat as “population” data.– This encourages an approach that involves significant amounts of independent processing. Problem: Where for ultra-large data sets doing some individual processing to a portion of the data set and then sorting and analyzing grouped data, map-reduce provides a simple way of doing this processing. Solution: Other Allocation Patterns• Multi-tier architecture pattern• Cloud architectures Relationships between Tactics and PatternsArchitectural patterns and tactics are ways of capturing proven good design structures and making them reusable. Tactics are simpler and more atomic than patterns• Tactics capture one step to take for a particular Quality Attribute to change behaviour with respect to that QA.• use just a single structure or computational mechanism, and they are meant to address a single architectural force.• Tactics can be seen as the building blocks of patterns; Most patterns consist of (are constructed from) several different tactics. TestabilityTestability illustrate QAs from a static perspective. A system or element of a system is testable if it is possible to test it in the way required by a particular development or maintenance process. Testability Concerns• Unlike the other QA (availability, performance and security), testability is concerned with the code structure rather than the connector/component view or deployment view.• The system elements we consider are code modules and the relationships are dependencies involved in building the code for components. Testability General Scenario 举例 Coverage Concrete Scenario• Source: Regression Tester• Stimulus: Completion of maintenance development to repair a critical bug• Artifact: Modules for the full system• Environment: Maintenance Development• Response: Results from path coverage tool• Response Measure: Path coverage is better than 95% of non-looping paths inside modules Testability Tactics adding controllability and observability to the system. Specialized Interfaces Record/Playback Localize State Storage Abstract Data Sources Sandbox Executable Assertions limiting complexity in the system’s designs: If it could be broken into smaller modules with lower complexity that could allow the regression test to achieve higher path coverage. Limit Structural Complexity Limit behavioral complexity - Nondeterminism A Design Checklist for TestabilityAllocation of Responsibilities■ Determine which system responsibilities are most critical and hence need to be most thoroughly tested.■ Ensure that additional system responsibilities have been allocated to do the following: Execute test suite and capture results (external test or self-test)• Capture (log) the activity that resulted in a fault or that resulted in unexpected (perhaps emergent) behavior that was not necessarily a fault• Control and observe relevant system state for testing Make sure the allocation of functionality provides high cohesion, low coupling, strong separation of concerns, and low structural complexity. Coordination ModelEnsure the system’s coordination and communication mechanisms■ Support the execution of a test suite and capture the results within a system or between systems■ Support capturing activity that resulted in a fault within a system or between systems■ Support injection and monitoring of state into the communication channels for use in testing, within a system or between systems■ Do not introduce needless nondeterminism Data ModelDetermine the major data abstractions that must be tested to ensure the correct operation of the system.■ Ensure that it is possible to capture the values of instances of these data abstractions■ Ensure that the values of instances of these data abstractions can be set when state is injected into the system, so that system state leading to a fault may be re-created■ Ensure that the creation, initialization, persistence, manipulation, translation, and destruction of instances of these data abstractions can be exercised and captured Mapping among Architectural Elements■ Determine how to test the possible mappings of architectural elements (especially mappings of processes to processors, threads to processes, and modules to components) so that the desired test response is achieved and potential race conditions identifed.■ In addition, determine whether it is possible to test for illegal mappings of architectural elements. Resource Management■ Ensure there are suffcient resources available to execute a test suite and capture the results.■ Ensure that your test environment is representative of (or better yet, identical to) the environment in which the system will run.■ Ensure that the system provides the means to do the following:• Test resource limits• Capture detailed resource usage for analysis in the event of a failure• Inject new resource limits into the system for the purposes of testing• Provide virtualized resources for testing Binding Time■ Ensure that components that are bound later than compile time can be tested in the late-bound context.■ Ensure that late bindings can be captured in the event of a failure, so that you can re-create the system’s state leading to the failure.■ Ensure that the full range of binding possibilities can be tested Choice of Technology■ Determine what technologies are available to help achieve the testability scenarios that apply to your architecture. Are technologies available to help with regression testing, fault injection, recording and playback, and so on?■ Determine how testable the technologies are that you have chosen (or are considering choosing in the future) and ensure that your chosen technologies support the level of testing appropriate for your system. For example, if your chosen technologies do not make it possible to inject state, it may be diffcult to re-create fault scenarios. ModifiabilityModifiability illustrate QAs from a static perspective. Measure how easy it might be to modify. This is a key area because change incurs cost. Four key questions:– What can change?– How likely is something to change?– When, where, how and by whom will changes be made?– What is the cost of making the change? General Scenario Tactics to control modifiability GPES ExampleVersion 1: General purpose query facility in each GP system.Version 2: Building a specific piece of business logic for each different query.Think about:– What changes can happen?– How likely is a change?– When, where, how and by whom?– How mush will it cost? GPES-relevant Scenario• Source: One of the stakeholders e.g. Medicines and Healthcare Products Regulatory Agency• Stimulus: Wants prescribing data on NSAIDs• Artifacts: Code (but depending on the architecture this could be configuration data)• Environment: Operation• Response: Develop the code• Response Measure: Data available 5 weeks after request GPES Version 1• Design and validate the query with the Medicines agency.• Code the query.• Test on some systems to ensure it does not have bad effects.• Rollout to all systems.• Make the query available to Medicines agency. GPES Version 2• Design and validate the query with Medicines agency.• Negotiate with the GP system providers on the design of the business logic (different in all systems?)• Are the providers the only vendor of such services? Should it go to a procurement?• Validate the queries on each system• Integrate the results• Roll out to all systems• Make the query available to the Medicines Agency It seems likely that the GPES V2 architecture will not pass the modifiability scenario we describe. Are any of the modifiability tactics appropriate to change the architecture to enable it to pass the scenario?■ Reduce Coupling is the category of tactics we need to consider.■ Each of the following offer potential routes with slightly different emphases:• Use an intermediary• Restrict dependencies• Refactor• Abstract common services■ Defer Binding: can we do this later in the process so it is more likely to be done by a computer than a human? Here this is unlikely.■ More on Binding Time• Compile time/Build Time: component replacement, compile time parameters,…• Deployment time: configuration scripts that bind at deployment, …• Initialization time: resource files• Runtime: dynamic lookup, service lookup, name servers, plugins, publish-subscribe, shared repositories, (Maybe just in time compilation fits here too) Design checklist for ModifiabilityAllocation of responsibilitiesWork out how things are likely to change e.g. technical, legal, organisational, social, markets, customers..■ Work out what responsibilities change.■ Try to modularise so a change does not affect responsibilities that span many modules. Coordination modelLook at how changes are likely to affect coordination and try to ensure that the most likely changes impact coordination across a small number of modules Data modelSimilar to coordination model – see how a change impacts on data models and try to esnure data model changes span as few modules as possible. Mapping among architectural elements■ Looking at potential changes to the system, assess whether some may best be responded to by changing the mapping to elements.■ Explore issues such as dependencies between elements, data holdings in elements, assignment of elements to processes, threads or processors. Resource Management■ Determine how a change in responsibility or quality attribute will change resource.■ Attempt to localise resourcing change resulting from a likely change to a small number of modules.■ Look at ways of using policies or configuration to manage resource change more effectively Binding Time■ Control choice of binding times so there are not too many combinations to consider.■ Consider attempting to defer binding to later, balance this against the cost of providing a later binding mechanism. Choice of TechnologyChoose technologies that make the most likely changes easier (e.g. choose a technology that allows runtime alteration of critical parameters rather than one where parameters are chosen at compile time) but balance this agains the cost of the different technologies. Architectural ModellingSoftware Architecture is intended to give us control over Quality Attributes. Ideally we’d like to be able to use Software Architecture to predict Quality Attributes. We should be able to build a predictive model of the Software Architecture and use the model to predict QAs. The current situation is patchy… Some quality attributes, most notably performance and availability, have well-understood, time-tested analytic models that can be used to assist in an analysis. Analytic model means one that supports quantitative analysis. Types of Analysis• Thought experiment: just a sort of discussion using informed people.• Back of the envelope: using very approximate techniques with unreliable assumptions.• Checklist: collated experience.• Analytic Model: based on sound abstractions – heavily dependent on estimates being correct• Simulation: higher level of detail – less analytic, more concrete.• Prototype: approximate system in an experimental setup.• Experiment: fielded system, simulated load• Instrumentation: measuring the variable of interest Analyzing PerformanceModels have parameters, which are values you can set to predict values about the entity being modeled. Model can be used to understand the latency characteristics of an architectural design.Data Needed for the Queuing Model■ We need the following information in order to model effectively:• The distribution for the arrival of service requests• The queuing discipline• The scheduling algorithm• The distribution of service times for service requests• Network characteristics■ The theory places restrictions on the distributions• Arrivals are usually expected to be Poisson Distributions specified by arrival rate• Service times are usually exponentially distributed on the service rate.• Some queuing behaviors are excluded such as reneging or jockying Example: MVC, says nothing about its deployment. That is, there is no specifcation of how the model, the view, and the controller are assigned to processes and processors; that’s not part of the pattern’s concern. These and other design decisions have to be made to transform a pattern into an architecture. Until that happens, one cannot say anything with authority about how an MVC-based implementation will perform.Data for MVC• Rate of service requests: the View component will service them at some rate.• Service requests to the Controller are generated by the View component.• Service requests from the Controller to the View component• Service requests from the Controller to the model• Service requests from the Model to the View Component Modelling MVCWe need estimates of:■ Distribution of external service demands■ Queuing Disciplines within the queues in front of each component.■ Network latencies■ Transfer characteristics:• View – Controller• Controller – View• Controller – Model• Model – View■ Scaling to large numbers of components is an issue Analyzing AvailabilityOne key issue is how long it takes to detect that a failure has taken place. Example is a Broker system.Hot Spare 热备用 (Active Redundancy)• Active and redundant both receive identical request stream.• Synchronous maintenance of broker state.• Fast failover in the event of failure of the active system. Warm Spare (Passive Redundancy)• Warm broker is maintained at the most recent checkpoint state.• In the event of failure the system rolls back to the most recent checkpoint.• This is slower than the hot spare approach Cold Spare• No attempt to synchronise.• In the event of failure the cold spare is started.• The system state is recovered via interaction with other systems (so they have to be resilient to failure in the broker) Analysis at Different Stages of the Life Cycle Architecture in the Life Cycles前面部分关注软件架构的 technical context。这里开始关注 life cycles。The role of software architecture is different for different lifecycles. Balancing Agility and Discipline• Lifecycles generally impose some discipline on the development process.• Software Architectures often feature in Lifecycles as a stage or support for analysis or design• Lifecycles exist because they codify useful patterns of activity and save us time and effort• Agility focusses on getting adequate solutions to stakeholders with less time and effort• We need to balance the discipline of lifecycles against the delivery focus of agility Lifecycles• Lifecycles underpin development processes by ordering stages and activities.• Any good organisation is always looking to improve its processes so there is usually an ongoing process improvement cycle focussed on making the process better. V-Model approach works well when you understand the concept and requirements. Agile Practice• Test-first programming• Refactoring• Continuous integration• Simple Design• Pair Programming• Common Codebase• Coding Standards• Open Work Area Agile vs. Plan Driven Early software development methods that emerged in the 1970s - such as the Waterfall method - is plan-driven and inﬂexible. But having a strong 先期 up-front plan provides for considerable predictability (as long as the requirements don’t change too much) and makes it easier to coordinate large numbers of teams. Agile methods and practitioners, on the other hand, often 轻视 scorn planning, preferring teamwork, frequent face-to-face communication, ﬂexibility, and adaptation. This enhances invention and creativity. • Work top-down and bottom-up simultaneously - balance will depend on the size and complexity of the project.• Top-down does architectural work based on things like patterns, product-line.• Bottom-up develops implementation and environment-specific constraints and solutions.• Focus on QAs, scenarios, tactics and processes to 调和 reconcile competing aspects provides a bottomup/top-down link• Balancing commitment and flexibility Analysis Techniques Product Line ArchitectureOne of the early success areas for Software Architecture was the development of Product Line Architectures. Product Line Architecture is an approach to adopt systematic reuse of architectural elements that involves changes in development process supported by specific practices that encourage reuse. A collection of software-intensive systems sharing a common, managed, set of features that satisfy the specific needs of a market segment or mission that are developed from a set of core assets in a prescribed way. Software Product Lines are directed by business goals in a particular application domain.• The products in a product line share a software product line architecture• Products are structured by the product line architecture and are built from services and components.• Architercture and components are the core assets used to satisfy the business goals.• Product line leverage commonality and limit variability of the product. Benefits to the organisation• Large-scale productivity gains• Improve time to market• Maintian market presence (rapidly evolving variants)• Sustain growth• Improved market agility• Better use of skills• Enable mass customisation• Gain control of configuration• Improve product quality• Better predictability of cost, schedule and quality Costs of a product line• Architecture: flexible enough to support variation in the products• Software components: general enough to support variability• Test plans, cases, data: take account of variation in components• Business cases: must operate at the level of a product family• Project plans: generic and extensible to deal with variation• Tools and processes: must support architecture, variation, configuration, ..• People, skills, training: need to be skilled in architecture and product lines.Product lines spread costs over several products:• Requirements and requirements analysis • Domain model • Architecture and design • Performance engineering • Documentation • Test cases, data, and plans • Skills • Processes, methods and tools • Defect fixing • Components and services Core Process Activities• Core asset development: improving the base components in terms of qualities, products they support, and architecture.• Product development: identifying and building products to meet market need inside the product line.• Management: monitoring and improving the processes, tools and practices. Introducing Product Lines• Proactive: Up-front investment to develop the core assets - need to know the market well (maybe have an already established set of products)• Reactive: Start with one or two products and use them to generate core assets.• Incremental: Develop core assets as the business need evolves. Example: Bosch Gasoline SystemsGoals■ Competitiveness:• Reduced hardware resource consumption• Reduced time to market for new features■ Development efficiency• Reuse: Applications can be used across different generations of system; “core” software is highly configurable and is reused via reconfiguration; “Vehicle functions” can be used across gasoline and diesel engines• Easy configuration of software products• Increased planning accuracy■ Quality• Interface integrity• Reuse of core assets■ Customer needs• Differentiation by individual software solutions• Clear feature-cost mapping Component Redesign■ Focussed on: reuse; simplification of calibration; resource consumption; stabilisation of interfaces (within the architecture)■ Redesign progressed by:• Analysing existing software inventory: features, sources of variability; relation to product line; document interdependency.• Concept development and design of components: simplification; configurability; architecture driven structure; document relations between features and components;• Baselines for variants of software components: document baselines; implement; maintain up-to-date document and implementation. Phased Introduction■ Investigate and customise product line engineering.■ Design and pilot adequate processes and methods.■ Roll out and institutionalise in the standard development process. DevOpsThe line between development and operation becomes more blurred and the use of the live environment to test innovations becomes more common. DevOps is a set of practices that span development and operation. Operations have the direct experience of use of the system– monitoring that use is a way of empirically verifying quality– operations have the data that is used to regulate operations and is essential information for development. Development is responsible for building in the right monitoring to ensure operations can operate effectively. DevOps is a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal operation while ensuring necessary quality. Open Services for Lifecycle Collaboration (OSLC): OSLC is an open and scalable approach to lifecycle integration. It simplifies key integration scenarios across heterogeneous tools. Traditionally we use test as the way of delivering quality change but we can “shepherd” committed change into use by controlling quantities of change, users experiencing change, results of monitoring than this may offer a better way. Delivery mechanism needs to be high quality: reliable, repeatable, available. Critical points• Making the decision to commit the code to be introduced into the system.• Transitioning from being under consideration into part of the production deployment that will be used by all users.• Issues is how to have enough confidence to make each of these transitions. Monitoring is critical.• The question is how to ensure the transitions are as reliable as possible. The extent of the lifecycle• Involves all people involved in the delivery of the service/application• Operations and development people are in continuous interaction.• We need architecture to achieve this.• Microservices architectural pattern is often used. Microservices The term “Microservice Architecture” has sprung up over the last few years to describe a particular way of designing software applications as suites of independently deployable services. The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API.— https://martinfowler.com/articles/microservices.html Attributes of Microservice Architecture• Separately deployed units• Very small service components• Single purpose function or an independent portion of functionality• Distributed• Loosely coupled• Multiple versions are acceptable• Asynchronous• No Orchestration Architecture EvaluationEvaluation by Designer• The consequences of the decision making regulate how much effort to put into the process – more importance means more effort in evaluation.• Try to use iterative approaches that get deeper in order to eliminate unpromising alternatives early.• Don’t strive for perfection, good enough for the context is usually enough. Peer Evaluation• Fix on the QAs to consider as part of the review – may be determined by the process or the business case.• The architect presents the architecture to the reviewers – questions are for information.• The review is driven by the relevant scenarios – the architect talks the review team through a scenario demonstrating the architecture meets the requirements captured in the scenario.• The outcome is a list of potential issues with actions: fix, mitigate, tolerate, … External Evaluation• Means to bring in additional expertise.• May represent some stakeholder interests.• More expensive and difficult to organise so this will often correspond to some major hurdle in the process. The Architecture Tradeoff Analysis Method (ATAM) ATAM is a risk-mitigation process. Its purpose is to help choose a suitable architecture for a software system by discovering trade-offs and sensitivity points, to capture project risks.ATAM is most beneficial when done early in the software development life-cycle, when the cost of changing architectures is minimal. Designed to be usable where:– Evaluators are not expert in the architecture– Evaluators need not be familiar with the business goals.– The system need not be fully developed– There may be large numbers of stakeholders Participants in ATM• The evaluation team: 3-5 people with designated roles (people may have multiple roles). Team members should be seen to be neutral with respect to the project.• Project decision takers: manager of the project, funder of the project, main architect• Architecture stakeholders: developers, testers, integrators, maintainers, performance engineers, … ATAM evaluation team roles and responsibilitiesTeam LeaderSets up the evaluation; coordinates with client, making sure client’s needs are met; establishes evaluation contract; forms evaluation team; sees that final report is produced and delivered (although the writing may be delegated) Evaluation LeaderRuns evaluation; facilitates elicitation of scenarios; administers scenario selection/prioritization process; facilitates evaluation of scenarios against architecture; facilitates onsite analysis Scenario ScribeWrites scenarios on flipchart or whiteboard during scenario elicitation; captures agreed-on wording of each scenario, halting discussion until exact wording is captured Proceedings ScribeCaptures proceedings in electronic form on laptop or workstation, raw scenarios, issue(s) that motivate each scenario (often lost in the wording of the scenario itself), and resolution of each scenario when applied to architecture(s); also generates a printed list of adopted scenarios for handout to all participants TimekeeperHelps evaluation leader stay on schedule; helps control amount of time devoted to each scenario during the evaluation phase Process ObserverKeeps notes on how evaluation process could be improved or deviated from; usually keeps silent but may make discreet process-based suggestions to the evaluation leader during the evaluation; after evaluation, reports on how the process went and lessons learned for future improvement; also responsible for reporting experience to architecture evaluation team at large Process EnforcerHelps evaluation leader remember and carry out the steps of the evaluation method QuestionerRaise issues of architectural interest that stakeholders may not have thought of ATAM Outputs• Concise presentation of the architecture – needs to be presentable in around one hour.• Articulation of the business goals – clearly communicated to all participants• Prioritized QA requirements expressed as scenarios – testable QA requirements.• Risks and non-risks – architecture decision that carries risks (or not).• Risk themes – attempt to identify systemic risk by grouping risks into themes.• Mapping of Architecture Decisions to QA requirements – motivating architecture decisions by QA requirements• Identified sensitivity and tradeoff decisions – critical decisions that have significant impact on QA requirements. Partnership and preparation: Getting the schedule, agendas and list of stakeholders prepared, preparing necessary documents and presentations, and gettting documents to the evaluation team Steps of Evaluation PhaseThe ATAM analysis phases (phase 1 and phase 2) consist of nine steps. Steps 1 through 6 are carried out in phase 1 Presentation of the ATAM approach – remind participants of the approach Business drivers presentation – functions; constraints; business goals; major stakeholders; architectural drivers Architecture presentation: Context for the system Static modular view Component and connector view Deployment view Main QA requirements and how the architecture addresses them: What has been reused Trace of key use cases Trace of key change scenarios Main issues/risks driving architectural change Identify architectural approaches – create a catalogue of patterns and tactics used in the architecture. Generate Quality Attribute Utility Tree this is an approach to identifying architecturally significant requirements (ASR) by looking through the QAs - identifying particular aspects of the QA that are relevant and any requirements related to that aspect of the QA. Each ASR is ranked High, Medium or Low in importance. Analyze architectural approaches – look at the most important QA requirement scenarios as identified at stage 5 and probe how the architecture meets the QA scenario. In phase 2, with all stakeholders present, those steps are summarized Brainstorm prioritization of scenarios – revisit the prioritization for additional scenarios, e.g. a particular stakeholder (performance engineer) might propose a scenario on the response time of the system. Analyze Architectural Approaches – revisit stage 6 but with an expanded and reprioritized set of scenarios Present results – the evaluation group tries to group risks into risk themes to identify systemic issues and results are presented. ATAM Results• Documentation of architectural approaches taken by the project.• Prioritized list of scenarios• Utility tree• Risks discovered• Non-risks identified• Sensitivity and Tradeoff points identified General Practice Extraction Service (GPES)An IT system designed to allow NHS organizations to extract data from GP practice computer systems in England. This data would be used to monitor quality, plan and pay for health services and help medical research.数据的请求和返回不需要实时，更多的是定期的请求，一定时间内返回数据。 General practitioner (GP), 全科医生。在英国，每个人都需要注册一个全科医生的诊所，当人们感到身体不适后首先会去联系的自己的全科医生。全科医生只进行有限的治疗，并建议是否有必要去医院看专科医生。每个 GP 都像小公司一样运作，有自己的 GP 系统，为患者保留病患记录。在英国，各种不同的机构组织可能需要了解GP正在做什么，因此需要从所有这些GP系统中提取数据。GPES 系统允许那些已经得到授权的机构组织，通过 NHSCIC（国家卫生和社会保健信息中心）提取各种GP数据。因为不同机构需要的信息不同，NHSCIC 需要研究制定如何提取指定的数据，并运行 GPES 系统从英国的所有GP系统提取数据。GP 可以从四种不同的 GP 系统中四选一。而 GPES 的挑战在于整合来自各个不同系统的GP的数据。 问题 The project has been significantly delayed and many customers have yet to receive data. Mistakes in the original 采购 procurement and contract management contributed to losses of public funds, through asset write-offs and settlements with suppliers. Only one customer, NHS England has so far received data from GPES. The time needed to design a new type of extract and restrictions in the contracts severely limits HSCIC’s ability to provide data to those who request it. It is unlikely that GPES in its current form can provide the NHS-wide service planned. Data Extract Issue NHS did a technical review of GPES in early 2011, which recommended several significant changes to its design. In the original design, each GP system supplier would use a common query language as part of their extraction system. This would allow the NHSIC to design a single extract centrally using the query tool, which all GP clinical systems could understand. The technical review recommended an alternative where each supplier would be free to develop their own query methods. New queries would no longer be designed in the query tool using a common language, but would instead need to be designed as logical ‘business rules’ and sent to GP system suppliers to implement. The NHSIC decided to abandon both the GPSOC contract approach and the common query language, as they could not agree either with the Department and GP system suppliers. They then procured the extraction systems by negotiating direct with the GP clinical system suppliers. NHSIC is using a non-competitive procurement approach, plus the changes in design, contributed to the restrictive process for designing new extracts. The HSCIC, has continued to use the GPSOC framework to require data sharing between NHS systems. The new framework, effective from 2014, says that principal clinical system suppliers must provide an interface method for third-party systems to use. This would improve interoperability between systems in GP practices and the health community. The HSCIC cannot do the wide range and scale of data extracts the NHS requests, because of the design of the GPES system and restrictions in supplier contracts. Customers have requested over 100 different data extracts from GPES, but the HSCIC estimate they will be able to design only 24 new extracts in 2015-16. Figure shows a summary of the HSCIC’s process to develop a new extract, each of which the supplier designs and programmes from scratch. The HSCIC have limited flexibility to amend extracts once developed, for example to change a time period and the specific organisations it will extract data from. GPES will continue to operate in the short term, as its data is critical for determining payments to GPs. Its coverage of all practices in England cannot currently be replicated by other primary care data extraction systems. However, limited capacity and the difficulty of developing new extracts deters wider use. The HSCIC has acknowledged there is unlikely to be a long-term future for all or part of the GPES. However, they intend to reuse parts for a replacement system if possible. The HSCIC estimate that they will achieve less than two more years of use from the GPES in its current form, in contrast to the five-year minimum lifetime assumed for new IT systems.]]></content>
      <categories>
        <category>学习笔记</category>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
        <tag>UoE</tag>
        <tag>软件架构</tag>
        <tag>软件管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parallel Programming Language and Systems - Informatics - University of Edinburgh 爱丁堡]]></title>
    <url>%2FUoE-ppls%2F</url>
    <content type="text"><![CDATA[Reference:http://www.inf.ed.ac.uk/teaching/courses/ppls/CMU 15213: Introduction to Computer Systems (ICS)Computer Systems: A Programmer’s PerspectiveA Comprehensive MPI Tutorial ResourceA chapter on MPI from Ian Foster’s online Book Designing and Building Parallel Programs Introduction to parallel computer architectureCovering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model. Bridging the gap between the parallel applications and algorithms which we can design and describe in abstract terms and the parallel computer architectures (and their lowest level programming interfaces) which it is practical to construct. The ability to express parallelism (a.k.a concurrency) concisely, correctly and efficiently is important in several contexts:• Performance Computing: parallelism is the means by which the execution time of computationally demanding applications can be reduced. In the era of static (or even falling) clock speeds and increasing core count, this class is entering the computing mainstream.• Distributed Computing: when concurrency is inherent in the nature of the system and we have no choice but to express and control it.• Systems Programming: when it is conceptually simpler to think of a system as being composed of concurrent components, even though these will actually be executed by time-sharing a single processor. Parallel ArchitectureTwo types (mainstream): Shared Memory architectures: in which all processors can physically address the whole memory, usually with support for cache coherency (for example, a quad or oct core chip, or more expensive machines with tens or hundreds of cores) Multicomputer architectures: in which processors can only physically address their “own” memory (for example, a networked cluster of PCs), which interact with messages across the network. Increasingly, systems will span both classes (e.g. cluster of manycore, or network-onchip manycores like the Intel SCC), and incorporate other specialized, constrained parallel hardware such as GPUs. Real parallel machines are complex, with unforseen semantic and performance traps. We need to provide programming tools which simplify things, but without sacrificing too much performance. Shared Memory ArchitecturesUniform Memory Access (UMA) architectures have all memory “equidistant” from all CPUs.For NUMA performance varies with data location. NUMA is also confusingly called Distributed Shared Memory as memory is physically distributed but logically shared. Memory consistency challenge: when, and in what order should one processor public updates to the shared memory? Exactly what and when it is permissible for each processor to see is defined by the Consistency Model, which is effectively a contract between hardware and software, must be respected by application programmers (and compiler/library writers) to ensure program correctness. Different consistency models trade off conceptual simplicity against cost (time/hardware complexity): Sequential consistency: every processor “sees” the same sequential interleaving of the basic reads and writes. This is very intuitive, but expensive to implement. Release consistency: writes are only guaranteed to be visible after program specified synchronization points (triggered by special machine instructions). This is less intuitive, but allows faster implementations. Shared memory architectures also raise tricky performance issues: The unit of transfer between memory and cache is a cache-line or block, containing several words. False sharing occurs when two logically unconnected variables share the same cache-line. Updates to one cause remote copies of the line, including the other variable, to be invalidated. Multicomputer architecturesLack of any hardware integration between cache/memory system and the interconnect. Each processor only accesses its own physical address space, so no consistency issues. Information is shared by explicit, co-operative message passingPerformance/correctness issues include the semantics of synchronization and constraints on message ordering. Parallel Applications and AlgorithmsThree well-known parallel patterns: Bag of Tasks, Pipeline and Interacting Peers. Here using the co, &lt; &gt;, await notation. 在co oc内的代码, 顺序是任意的.1234# 这里暂时用 // 表示并行的代码co a=1; // a=2; // a=3; ## all happen at the same time, What is a in the end?oc To answer the above question, we need to define Atomic Actions: Reads and writes of single variables as being atomic. For more than one statements, if they appear to execute as a single indivisible step with no visible intermediate states, they are atomic, must be enclosed in &lt; &gt;.1234a=0;co a=1; // a=2; // b=a+a; ## what is b?oc The above code has no &lt; &gt;, each value accessed in an expression is a read. Each assignment is a write. Thus, b could be 0, 1, 2, 3, or 4.1234a=0;co a=1; // a=2; // &lt;b=a+a;&gt;oc Now the only outcomes for b are 0, 2 or 4. Sequential memory consistency (SC)To make agreement on such inconsistency, we define the sequential memory consistency (SC), to be consistent with the following rules: ordering of atomic actions (particularly reads and writes to memory) from any one thread have to occur in normal program order atomic actions from different threads are interleaved arbitrarily (ie in an unpredictable sequential order, subject only to rule 1) It doesn’t mean that SC programs have to be executed sequentially!It only means that the results we get must be the same as if the program had been executed in this way. AwaitThe await notation &lt; await (B) S &gt; allows us to indicate that S must appear to be delayed until B is true, and must be executed within the same atomic action as a successful check of B.123456a=0; flag=0;co&#123;a=25; flag=1;&#125;//&lt;await (flag==1) x=a;&gt; ## x = 25oc However, it is not guaranteed that, an await statement is executed right after its condition becomes true. If other atomic actions make the condition false again, before the await executes, it will have to wait for another chance. The Bag-of-TasksExample: Adaptive Quadrature, compute an approximation to the shaded integral by partitioning until the 梯形 trapezoidal approximation is “good enough”, compared with the sum of its two sub-divided trapezoidals’s area.area = quad (a, b, f(a), f(b), (f(a)+f(b))*(b-a)/2);The recursive calls to quad do not interfere with each other. So we can parallelize the program by changing the calls to12345# 简单地并行co larea = quad(left, mid, f(left), f(mid), larea); // rarea = quad(mid, right, f(mid), f(right), rarea);oc In practice, there is very little work directly involved in each call to quad. The work involved in creating and scheduling a process or thread is substantial (much worse than a simple function call), program may be swamped by this overhead. Using the Bag of Tasks pattern: a fixed number of worker processes/threads maintain and process a dynamic collection of homogeneous “tasks”. Execution of a particular task may lead to the creation of more task instances.12345678# Bag of Tasks patternco [w = 1 to P] &#123; while (all tasks not done) &#123; get a task; execute the task; possibly add new tasks to the bag; &#125;&#125; 1, Shared bag: contains task(a, b, f(a), f(b), area)2, Get a task: remove a record from the bag, either: • adds its local area approximation to the total • or creates two more tasks for a better approximation (by adding them to the bag). Advantage:1, It constraints the number of processes/threads to avoid overhead.2, Useful for independent tasks and to implement recursive parallelism3, Naturally load-balanced: each worker will probably complete a different number of tasks, but will do roughly the same amount of work overall. Bag of Tasks Implementation: The challenge is to make accessing the bag much cheaper than creating a new thread. With a shared address space, a simple implementation would make the bag an atomically accessed shared data structure.123456789101112131415161718shared int size = 1, idle = 0;shared double total = 0.0;bag.insert (a, b, f(a), f(b), approxarea);co [w = 1 to P] &#123; while (true) &#123; &lt; idle++; &gt; &lt; await ( size &gt; 0 || idle == P ) ## 检测 termination if (size &gt; 0) &#123; ## get a task bag.remove (left, right ...); size--; idle--; &#125; else break; &gt; ## the work is done mid = (left + right)/2; ..etc.. ## compute larea, etc if (fabs(larea + rarea - lrarea) &gt; EPSILON) &#123; ## create new tasks &lt; bag.insert (left, mid, fleft, fmid, larea); bag.insert (mid, right, fmid, fright, rarea); size = size + 2; &gt; &#125; else &lt; total = total + larea + rarea; &gt; &#125;&#125; Detecting termination:不能仅仅因为 bag 空了就认为可以结束了, 因为还可能有还在工作的 workers 未来会产生新的任务. 所以需要让 workers 有能力把自己的工作完成状况告知 bag. When bag is empty AND all tasks are done; All tasks are done when all workers are waiting to get a new task. If a bag of tasks algorithm has terminated, there are no tasks left. However, the inverse is not true. I.e. no tasks in a bag could mean that one of the workers is still processing a task which can lead to creation of multiple new tasks.To solve this problem, workers could have the ability to notify the master/bag once they finish the current task. As a result, an implementation of bag of tasks can then contain a count of idle and active works to prevent early termination A more sophisticated implementation (with less contention) might internally have a collection of bags, perhaps one per worker, with task-stealing to distribute the load as necessary. With message passing, a simple scheme might allocate an explicit “farmer” node to maintain the bag. Again, a more sophisticated implementation could distribute the bag and the farmer, with task-stealing and termination checking via messages. Pipeline Patterns.Example: The Sieve of Eratosthenes algorithms for finding all prime numbers. To find all prime numbers in the range 2 to N. The algorithm write down all integers in the range, then repeatedly remove all multiples of the smallest remaining number. Before each removal phase, the new smallest remaining number is guaranteed to be prime. Notice that, it is not necessarily to wait one Sieve completed then start another. As long as one Sieve stage finds out one candidate number could not be divided exactly by the sieve number, it could generate a new stage with this candidate number as Sieve. And different sieve just remove the multiples of its own Sieve number.123456789101112131415161718192021# a message-passing style pipeline pseudocodemain() &#123; # the generator spawn the first sieve process; for (i=2; i&lt;=N; i++) &#123; send i to first sieve; &#125; send -1 to first sieve; # a &quot;stop&quot; signal&#125;sieve() &#123; int myprime, candidate; receive myprime from predecessor and record it; do &#123; receive candidate from predecessor; if (candidate == -1) &#123;send -1 to successor if it exists&#125; else if (myprime doesn&apos;t divide candidate exactly) &#123; if (no successor yet) spawn successor sieve process; send candidate to successor sieve process; &#125; &#125; while (candidate != -1)&#125; 每一个数(2-N)都可能作为筛子, 筛掉能整除这个筛子的其他数，而筛子之间是互相独立的，所以可以以流水线模式 pipeline patterns来并行操作，动态生成筛子。最开始最小的数字2会成为筛子。筛子可以理解为不同的工序，其余数字从小到大逐一通过这些工序加工（在 Sieve of Eratosthenes 问题中变为筛选排查），无法被筛子整除的数字会被传递到下个筛子（如果没有下一个筛子，则以这个数字创建新的筛子），这样保证生成的筛子就都是素数了。虽然工序是按顺序过的，但是所有工序可以同时对不同的产品（数字）开工，从而达到并行目的。 For pipeline patterns, the potential concurrency can be exploited by assigning each operation (stage of the pipeline) to a different worker and having them work simultaneously, with the data elements passing from one worker to the next as operations are completed. Despite the dependencies (order constraints) of the processing steps, the pipeline threads can work in parallel by applying their processing step to different data (products). Think of pipeline patterns as the factory assembly line. We need to pick out prime number from a range of numbers N, each number is passed into a sequence of stages, each stages checks a pass in number based on the stages’s Sieve. The numbers that finally pass all stages without being removed is a prime number. Pipelines are composed of a sequence of threads, in which each thread’s input is the previous thread’s output, (Producer-Consumer relationships). The advantages of pipeline patterns is that construction of pipeline stages is dynamic and data-dependent. To allow production and consumption to be loosely synchronized, we will need some buffering in the system. The programming challenges are to ensure that no producer overwrites a buffer entry before a consumer has used it, and that no consumer tries to consume an entry which doesn’t really exist (or re-use an already consumed entry) Interacting Peers PatternModels of physical phenomena are often expressed as a system of partial differential equations. These can be approximately solved by “finite difference methods” which involve iteration on a matrix of points, in an interacting peers pattern. The “compute” step usually involves only a small number of neighbouring points. The termination test looks for convergence. We could use a duplicate grid and barriers to enforce correct synchronization between iterations:12345678910111213141516shared real grid[n+2, n+2], newgrid[n+2, n+2];shared bool converged; local real diff;co [i = 1 to n, j = 1 to n] &#123; initialise grid; do &#123; barrier(); ## before resetting test converged = true; ## provisionally newgrid[i,j] = (grid[i-1,j] + grid[i+1,j] + grid[i,j-1] + grid[i,j+1])/4; ## compute new value diff = abs (newgrid[i,j] - grid[i,j]); ## compute local change barrier(); ## before converged update if (diff &gt; EPSILON) converged = false; ## any one will do grid[i,j] = newgrid[i,j]; ## copy back to real grid barrier(); ## before global check &#125; while (not converged);&#125; A barrier() in ppls makes any thread that arrive here has to wait all the other threads arriving here. 以方腔热对流的模拟计算模型为例，每个网格节点$(i,j)_{t+1}$ 的更新依赖于上一个迭代时间点的$(i,j)_t$以及其临近几个点的值，创建最多跟网格点数量一样的threads，然后并行地计算网格点的新值，更新的值用一个buffer层来缓存，用barrier()来保证所有网格点的更新值都计算完毕，再检查收敛情况，再用一个barrier()保证所有buffer层的值都更新到原网格上，再决定是否进行下一次计算。 Single Program Multiple Data (SPMD): A programming style, all processes execute more or less the same code, but on distinct partitions of the data. Other PatternsOther candidate patterns include MapReduce (championed by Google), Scan, Divide &amp; Conquer, Farm as well as application domain specific operations. Shared Variable ProgrammingIn the shared-memory programming model, tasks share a common address space, which they read and write asynchronously. An advantage of this model from the programmer’s point is that the notion of data “ownership” is lacking, so there is no need to specify explicitly the communication of data between tasks. Program development can often be simplified. There are two fundamentally different synchronization in shared variable programming. Mutual Exclusion and Condition Synchronization. Mutual ExclusionAtomic actions, at most one thread is executing the critical section at a time. Prevent two or more threads from being active concurrently for some period, because their actions may interfere incorrectly. For example, we might require updates to a shared counter (e.g., count++) to execute with mutual exclusion. Critical Sections problemA simple pattern of mutual exclusion occurs in the critical section problem - when n threads execute code of the following form, in which it is essential that at most one thread is executing statements in the critical section at a time (because of potentially unsafe access to shared variables)12345678co [i = 1 to n] &#123; while (something) &#123; lock(l); #entry section critical section; unlock(l); #exit section non-critical section; &#125;&#125; Design code to execute before (entry protocol) and after (exit protocol) the critical section to make the critical section atomic. If one thread lock the critical section, no one(thread) else could lock it or unlock it anymore, until the thread unlock it. Important properties: Mutual exclusion: When a thread is executing in its critical section, no other threads can be executing in their critical sections. Absence of Deadlock (or Livelock): If two or more threads are trying to enter the critical section, at least one succeeds. A deadlock is a state in which each member of a group is waiting for some other member to take action, such as sending a message or more commonly releasing a lock, so that neither of them take action.类似两个人相遇互不相让, 没人肯挪动.Livelock is a condition that takes place when two or more programs change their state continuously, with neither program making progress.类似两个人相遇同时往相同方向避让. Absence of Unnecessary Delay: If a thread is trying to enter its critical section and the other threads are executing their non-critical sections, or have terminated, the first thread is not prevented from entering its critical section. Eventual Entry (No Starvation): A thread that is attempting to enter its critical section will eventually succeed. May not matter in some “performance parallel” programs - as long as we are making progress elsewhere. Simple implementation of each lock with a shared boolean variable: if false, then one locking thread can set it to true and be allowed to proceed. Other attempted locks must be forced to wait.123456789101112# model assumes that the l = false;# write is already atomic# This might fail if the model is more relaxed than SC.lock_t l = false;co [i = 1 to n] &#123; while (something) &#123; &lt; await (!l) l = true; &gt; # guarantee the others waiting critical section; l = false; # unlock the lock, open the critical section non-critical section; &#125;&#125; To implement the &lt; await (!l) l = true; &gt;, we rely on some simpler atomic primitive, implemented with hardware support. There are many possibilities, including “Fetch-and-Add”, “Test-and-Set” and the “Load-Linked, Store-Conditional” pairing. Test-and-Set (TS) instructionBehaving like a call-by-reference function, so that the variable passed in is read from and written to, but in reality it is a single machine instruction. The key feature is that this happens (or at least, appears to happen) atomically.12345678910111213141516# A Test-and-Set (TS) instructionWbool TS (bool v) &#123; &lt; bool initial = v; v = true; return initial; &gt;&#125;lock_t l = false;co [i = 1 to n] &#123; while (something) &#123; while (TS(l)) ; ## spin lock critical section; l = false; non-critical section; &#125;&#125; This is called spin lock, Simple spin locks don’t make good use of the cache (those spinning Test-And-Sets play havoc with contention and coherence performance). A pragmatically better spin locks is known as Test-and-Test-and-Set - mainly spinning on a read rather than a read-write function.1234567... while (something) &#123; while (l || TS(l)); /* only TS() if l was false*/ critical section; ... &#125;... Simply read l until there is a chance that a Test-and-Set might succeed. Spin lock guarantees mutual exclusion, absence of deadlock and absence of delay, but does not guarantee eventual entry. Lamport’s Bakery AlgorithmImplement critical sections using only simple atomic read and simple atomic write instructions (i.e. no need for atomic read-modify-write). 采用商店结账排队机制，顾客就是一个个threads，根据排队码，越小的优先级越高（0 除外，0 代表没有结账需求），最小的可以进入critical section。 The challenge is entry protocal, if a thread intends to access the critical section: 排队取号：It sets its turn turn[i] = max(turn[:])+1 (Threads not at or intend to access the critical section have a turn of 0) 等待叫号：This thread waits until its turn comes up (until it has the smallest turn). 123456789101112int turn[n] = [0, 0, ... 0];co [i=1 to n] &#123; while (true) &#123; turn[i] = max (turn[1..n]) + 1; for (j = 1 to n except i) &#123; while ((turn[j]!=0 and (turn[i] &gt; (turn[j])) skip; &#125; critical section; turn[i] = 0; noncritical section; &#125;&#125; 因为max (turn[1..n]) + 1不是atomic的, 所以会出现问题. 问题一: if turn setting is not atomic then two (or more) threads may claim the same turn. 两个threads在取号阶段turn[i] = max(turn[:])+1出现并发，两个都先max, 之后再+1. 问题二: there is possibility that a thread can claim a lower turn than another thread which enters the critical section before it! 两个threads在取号阶段turn[i] = max(turn[:])+1出现并发, 并且在两个threads分别进行max的间隙, 刚好在CS中的thread完成并退出CS，导致两个thread看到的max值不一样了. 前者比后者看到的大, 但前者却因为更早进行+1操作而提前进入了CS. 举例：假如同时有三个thread A B C, A 已经在CS中(turn(A)&gt;0)： B 先运行max比较(max = turn(A)), C 在 A 退出后(turn(A) = 0)才进行比较(max = 0), B 先进行+1操作(turn(B) = turn(A)+1 &gt; 1), B 进行比较后允许进入CS (此时turn(C)还是0, 0是被忽略的); 之后C才 +1(turn(C) = 0 + 1 = 1); 这样导致B的值虽然比C大, 但B还是比C先进入CS; 之后因为 C 的 turn 比较小， 所以 C 也跟着进入 CS。 问题一解决方案 - 使用线程ID（绝不相同）做二次区分, 在相同 turn 的情况下，具有较低ID的 thread 有限。 问题二解决方案 - 在max (turn[1..n]) + 1之前先turn[i] = 1;.• 这样，任何 threads 想取号都要先标记为 1• 标记后，才有资格跟其他 thread 比较• 以max+1作为号码进入队列，这样任何的可能的 turn 值都必定大于 1• B 无法提前进入CS (此时turn(C)不再是被忽略的0, 而是最小正整数1).12345678# (x, a) &gt; (y,b) means (x&gt;y) || (x==y &amp;&amp; a&gt;b).while (true) &#123; turn[i] = 1; turn[i] = max (turn[1..n]) + 1; for (j = 1 to n except i) &#123; while ((turn[j]!=0 and (turn[i], i) &gt; (turn[j], j)) skip; &#125; ...&#125; Lamport’s algorithm has the strong property of guaranteeing eventual entry (unlike our spin lock versions). The algorithm is too inefficient to be practical if spin-locks are available. Condition SynchronizationDelay an action until some condition (on the shared variables such as in producer-consumer, or with respect to the progress of other threads such as in a Barrier) becomes true. Barrier synchronizationBarrier synchronization is a particular pattern of condition synchronization, a kind of computation-wide waiting:123456co [i = 1 to n] &#123; while (something) &#123; do some work; wait for all n workers to get here; &#125;&#125; A Counter Barriers12&lt;count = count + 1;&gt;&lt;await (count == n);&gt; is fine as a single-use barrier, but things get more complex if (as is more likely) we need the barrier to be reusable. 改良为&lt;await (count == n); count = 0;&gt;也不行: an inter-iteration race, 假如count == n, 那么n个threads都完成了前面的statements并准备执行await, 但其中任何一个 thread 先执行完整个代码都使count = 0,这样剩余的threads就无法通过await条件了. Sense Reversing BarrierA shared variable sense is flipped after each use of the barrier to indicate that all threads may proceed. 关键每个 thread 都有自己的 private variable mySense 和 while spin lock。解决了前面的死锁问题。12345678910111213shared int count = 0; shared boolean sense = false;co [i = 1 to n] &#123; private boolean mySense = !sense; ## one per thread while (something) &#123; do some work; &lt; count = count + 1; if (count == n) &#123; count = 0; sense = mySense; &#125; ## flip sense &gt; while (sense != mySense); ## wait or pass mySense = !mySense; ## flip mySense // 或者使用 &lt; await (sense==mySense) mySense = !sense;&gt; &#125;&#125; 所有thread的local variable mySense开始都被赋值为!sense(true)，前面n-1个thread都得在内循环while那里等着；直到最后一个thread完成工作后, if条件才满足, count重置为0, 反转sense(被赋值为mySense也即是true), 之后所有threads才能结束内部while循环，接着再次反转sense(被赋值为!mySense也即是false), 然后进行下一轮大循环，借此达到重复利用barrier的目的. sense初始值是什么无所谓, 反转才是关键. 缺点：$O(n)$效率，count次数（同步次数）正比于thread数量。 Symmetric BarriersSymmetric barriers are designed to avoid the bottleneck at the counter.通过 pair-threads barriers 多轮同步来构建一个完整的 n-threads barriers，让所有threads都知道大家已经完成任务。总共是$\log_2n$ 轮同步。每个thread在完成必要工作后, 开始进入下面的pairwise同步环节，自己(myid)的初始arrive状态为0:12345678910# arrive[i] == 1 means arrive barrier# there will be log_2 #threads stages,# 每个stage代表一次pairwise同步for [s = 0 to stages-1] &#123; &lt;await (arrive[myid] == 0);&gt; # 1 arrive[myid] = 1; # 2 work out who my friend is at stage s; &lt;await (arrive[friend] == 1);&gt; # 3 arrive[friend] = 0; # 4&#125; 这样保证了，每个thread需要先把自己的arrive状态标记为1(#1，#2)，才可以去看同伴的状态（#3），假如同伴也是1，那么表明自己这一组是都到达了barrier状态（大家都是1），那么就会把对方的状态初始化为0 （#4），进入下一阶段，更换同伴，继续同步比较。When used as a step within a multistage symmetric barrier, 会出现问题：假如有四个thread，那么就会有两个stages：第一次是1和2同步，3和4同步。2一直没到barrier，1一直卡在#3。而3和4 同步完后开始检查1的状况，发现arrive[1] = 1，就运行Lines (3) and (4), 结果1就被初始化了，而2还没是没到barrier。 解决办法是给每个stage分配新的arrive变量。1234567for [s = 0 to stages-1] &#123; &lt;await (arrive[myid][s] == 0);&gt; arrive[myid][s] = 1; work out who my friend is at this stage; &lt;await (arrive[friend][s] == 1);&gt; arrive[friend][s] = 0;&#125; 这样假如出现2一直没到barrier的情况, 那么1会卡在当前stage, 1的stage+1的arrive状态就无法更新为1. Dissemination BarriersIf n isn’t a power of 2, instead of pairwise synchs, we have two partners at each stage for each thread, one incoming and one outgoing. Structured PrimitivesInstead of implementing directly in the user-address space, a number of more structured primitives have been devised for implementation with the assistance of the operating system, so that threads can be directly suspended and resumed by the OS’s scheduler. • Machine code, instructions and data directly understandable by a CPU;• Language primitive, the simplest element provided by a programming language;• Primitive data type, a datatype provided by a programming language. Semaphores 信号灯A semaphore is a special shared variable, accessible only through two atomic operations, P(try to decrease) and V(increase), defined by:P(s): &lt;await (s&gt;0) s=s-1;&gt;V(s): &lt;s=s+1;&gt; Property: A thread executing P() on a 0 valued semaphore will be suspended on a queue until after some other thread has executed a V(). Application: A semaphore appears to be a simple integer. A thread waits for permission to proceed a critical section, and then signals that it has proceeded by performing a P() operation on the semaphore. Binary semaphore: A semaphore whose usage is organised to only ever take the value (0, 1) as a mutex 互斥.Counting(split) semaphore: can take on arbitrary nonnegative values. Semaphores still require careful programming: there is no explicit connection in the program source between “matching” semaphore operations. It is easy to get things wrong. Similarly, there is no obvious indication of how semaphores are being used - some may be for mutual exclusion, others for condition synchronization. Again confusion is possible. Semaphores for Critical Section (mutual exclusion)123456789sem mutex = 1;co [i = 1 to n] &#123; while (whatever) &#123; P(mutex); critical section; V(mutex); noncritical section; &#125;&#125; Semaphores for Barrier Synchronisation实现 symmetric barrier: an array of arrive semaphores for each stage12345for [s = 1 to stages] &#123; V(arrive[myid][s]); work out who my friend is at stage s; P(arrive[friend][s]);&#125; Semaphores for Producer-Consumer Buffering针对单个producer和consumer，控制其接触单个容量的buffer权限：一个semaphores标识buffer已满full，一个标识空empty。这种情况下，只能有一个semaphore是1，故称之为split binary semaphore。 P(full) 执行 wait full &gt; 0 : full -= 1, V(empty)执行empty += 1123456789101112131415161718T buf; sem empty = 1, full = 0;co co [i = 1 to M] &#123; while (whatever) &#123; ...produce new data locally P(empty); buf = data; # producer V(full); &#125; &#125;// co [j = 1 to N] &#123; while (whatever) &#123; P(full); result = buf; # consumer V(empty); ... handle result locally &#125; &#125;oc Bounded Buffer: Control access to a multi-space buffer (the producer can run ahead of the consumer up to some limit) Implement the buffer itself with an array (circular), and two integer indices, indicating the current front and rear of the buffer and use arithmetic modulo n (the buffer size), so that the buffer conceptually becomes circular For a single producer and consumer, we protect the buffer with a split “counting” semaphore, initialised according to the buffer size. Think of full as counting how many space in the buffer are full, and empty as counting how many are empty1234567891011121314151617T buf[n]; int front = 0, rear = 0;sem empty = n, full = 0;co ## Producer while (whatever) &#123; ...produce new data locally P(empty); # empty&gt;0, 才能生产, empty-=1 buf[rear] = data; rear = (rear + 1) % n; V(full); &#125;// ## Consumer while (whatever) &#123; P(full); # full&gt;0, 才能消耗, full-=1 result = buf[front]; front = (front + 1) % n; V(empty); ... handle result locally &#125;oc Multiple Producers/Consumers: Because each producer may access the same pointer to overide each other, so as consumer. Thus we need two levels of protection. Use a split counting semaphore to avoid buffer overflow (or underflow), as previously. Use a mutual exclusion semaphores to prevent interference between producers (and another to prevent interference between consumers). This allows up to one consumer and one producer to be actively simultaneously within a non-empty, non-full buffer. 1234567891011121314151617181920212223T buf[n]; int front = 0, rear = 0; 86sem empty = n, full = 0, mutexP = 1, mutexC = 1;co co [i = 1 to M] &#123; while (whatever) &#123; ...produce new data locally P(empty); P(mutexP); # stop the other producers from accessing the buffer buf[rear] = data; rear = (rear + 1) % n; V(mutexP); V(full);&#125; &#125;// co [j = 1 to N] &#123; while (whatever) &#123; P(full); P(mutexC); result = buf[front]; front = (front + 1) % n; V(mutexC); V(empty); ... handle result locally &#125; &#125;oc Extending Multiple Producers/Consumers: If the buffered items are large and take a long time to read/write, we would like to relax this solution to allow several producers and/or consumers to be active within the buffer simultaneously. We need to ensure that these workers accesse distinct buffer locations, which require the index arithmetic to be kept atomic. Make sure that the producer/consumers wait for that element to be empty/full before actually proceeding. The solution is to have extra semaphores pair for each buffer location. MonitorsThe monitor is a more structured mechanism which allows threads to have both mutual exclusion and the ability to wait (block) for a certain condition to become true. It has a mechanism for signaling other threads that their condition has been met. A monitor consists of a mutex (lock) object and condition variables (cv). A condition variable is basically a container of threads that are waiting for a certain condition. For Mutual Exclusion: i.e. a mutex (lock) object, ensures that at most one thread is active within the monitor at each point in time. 不同线程的下一条即将执行的指令 (suspended) 可能是来自同一个 monitor (由os自行分配), 但同一时间内，至多只能有一个线程执行下一条指令，但可能不同线程各自收到了来自这个 monitor 代码的不同指令. It is as if the body of each monitor method is implicitly surrounded with P() and V() operations on a single hidden binary semaphore, shared by all methods. For Condition Synchronization, using a cv with a monitor to control a queue of delayed threads by a kind of Signal and Continue (SC) scheme.For a condition_variables x; wait(x): Release lock; wait for the condition to become true; reacquire lock upon return (Java wait()) Signal(x): Wake up a waiter, if any (Java notify()) signal-all(x)orBroadcast(x): Wake up all the waiters (Java notifyAll()) For the thread active inside a monitor method - executing in monitor state If the thread could not proceed, it may call the wait(cv) operation to give up the (implicit) lock it holds on the monitor, and being suspended (push to the end of CV queue). Each CV has its unique block queue. Or the thread could calls the operation signal(cv) to release the lock. This allow one previously blocked thread (normally chosen by a FIFO discipline) to become ready for scheduling again (only one will be allowed to enter the monitor entry queue at a time). The signalling thread continues uninterrupted. Or return(). If no threads are waiting, then a signal() is “lost” or “forgotten”, whereas a V() in Semaphores allows a subsequent P() to proceed. Monitor semantics mean that when a thread which was previously blocked on a condition is actually awakened again in the monitor. The point to remember is that when the signal happened, the signalled thread was allowed to try to acquire the monitor lock again). It could be that some other thread acquires the lock first, and does something which negates the condition again (for example, it consumes the “new item” from a monitor protected buffer). Thus it is often necessary, in all but the most tightly constrained situations, to wrap each conditional variable wait() call in a loop which rechecks the condition it was waiting for is still true. Single producer, single consumer bounder buffer12345678910111213141516171819monitor Bounded_Buffer &#123; typeT buf[n]; # an array of some type T int front = 0, # index of first full slot rear = 0; # index of first empty slot count = 0; # number of full slots ## rear == (front + count) % n condition_variables not_full, # signaled when count &lt; n not_empty; # signaled when count &gt; 0 procedure deposit(typeT data) &#123; # 存 while (count == n) wait(not_full); buf[rear] = data; rear = (rear+1) % n; count++; signal(not_empty); &#125; procedure fetch(typeT &amp;result) &#123; # 取 while (count == 0) wait(not_empty); result = buf[front]; front = (front+1) % n; count--; signal(not_full); &#125;&#125; Why the while loop is necessary as a safety check on the wait calls (why not use if)? - 因为notify()只会让正在 wait queue 的 thread 进入准备状态, 但不会直接控制其恢复工作（是否马上开始，谁先开始，都是由os内部控制的）, 所以导致不同 thread 进度不同; 而while可以保证当即使 thread 因为受到notify()而结束wait()开始进入准备状态(entry queue)后, 继续检查 buffer 状态, 这样假如发现自己是最优先安排的那个, 就可以跳出while循环进入工作状态; 假如发现自己优先度不是最高的(while循环条件继续满足), 则继续wait(). The key difference to semaphores: signal() on a condition variable is not “remembered” in the way that V() on a semaphore is. If no threads are waiting, then a signal() is “lost” or “forgotten”, whereas a V() will allow a subsequent P() to proceed. Real Shared Variable Programming SystemsVarious concepts for shared variable programming have been embedded in real programming systems. In particular C’s Posix threads (Pthreads) library and Java’s threads and monitors. POSIX Threads (Pthread)Create a new thread: Threads (type pthread_t) begin by executing a given function, and terminate when that function exits (or when killed off by another thread).12int pthread_create (pthread_t *thread, p_thread_attr_t *attr, void *(*function) (void *), void *arguments); Wait for thread termination: int pthread_join (pthread_t t, void ** result); 12345678910111213141516//一个简单但是有错误的例子，int target;void *adderthread (void *arg) &#123; int i; for (i=0; i&lt;N; i++) &#123; target = target+1; &#125;&#125;int main (int argc, char *argv[]) &#123; int i; pthread_t thread[P]; target = 0; for (i=0; i&lt;P; i++) &#123; pthread_create(&amp;thread[i], NULL, adderthread, NULL); &#125; .....&#125; Variable target is accessible to all threads (shared memory). Its increment is not atomic, so we may get unpredictable results. POSIX provides mechanisms to coordinate accesses including semaphores and building blocks for monitors. Pthreads semaphores12345678910111213141516//用 pthread semaphores 改写前面的代码sem_t lock;void *adderthread (void *arg) &#123; int i; for (i=0; i&lt;N; i++) &#123; sem_wait(&amp;lock); target = target+1; sem_post(&amp;lock); &#125;&#125;int main (int argc, char *argv[]) &#123; target = 0; sem_init(&amp;lock, 0, 1); .....&#125; sem_init(&amp;sem, share, init), where init is the initial value and share is a “boolean” (in the C sense) indicating whether the semaphore will be shared between processes (true) or just threads within a process (false). sem_wait(s), which is the Posix name for P(s) sem_post(s), which is the Posix name for V(s) A Producers &amp; Consumers:1234567891011121314151617181920212223242526272829303132sem_t empty, full; // the global semaphoresint data; // shared bufferint main (int argc, char *argv[]) &#123; pthread_t pid, cid; .... sem_init(&amp;empty, 0, 1); // sem empty = 1 sem_init(&amp;full, 0, 0); // sem full = 0 pthread_create(&amp;pid, &amp;attr, Producer, NULL); pthread_create(&amp;cid, &amp;attr, Consumer, NULL); pthread_join(pid, NULL); pthread_join(cid, NULL);&#125;void *Producer (void *arg) &#123; int produced; for (produced = 0; produced &lt; numIters; produced++) &#123; sem_wait(&amp;empty); data = produced; sem_post(&amp;full); &#125;&#125;void *Consumer (void *arg) &#123; int total = 0, consumed; for (consumed = 0; consumed &lt; numIters; consumed++) &#123; sem_wait(&amp;full); total = total+data; sem_post(&amp;empty); &#125; printf("after %d iterations, the total is %d (should be %d)\n", numIters, total, numIters*(numIters+1)/2);&#125; Pthreads MonitorsPthreads provides locks, of type pthread_mutex_t m;. These can be Initialized with pthread_mutex_init(&amp;m, attr), where attr are attributes concerning scope (as with semaphore creation). If attr is NULL, the default mutex attributes (NONRECURSIVE) are used; Locked with pthread_mutex_lock(&amp;m), which blocks the locking thread if m is already locked. There is also a non-blocking version pthread_mutex_trylock(&amp;m). Unlocked with pthread_mutex_unlock(&amp;m). Only a thread which holds a given lock, should unlock it! Pthreads provides condition variables pthread_cond_t. As well as the usual initialization, these can be: Waited on with pthread_cond_wait(&amp;cv, &amp;mut) where cv is a condition variable, and mut must be a lock already held by this thread, and which is implictly released. Signalled with pthread_cond_signal(&amp;cv) by a thread which should (but doesn’t strictly have to) hold the associated mutex. The semantics are “Signal-and-Continue” as previously discussed. Signalled all with pthread_cond_broadcast(&amp;cv). This is “signal-all” A simple Jacobi grid-iteration program with a re-usable Counter Barrier. To avoid copying between “new” and “old” grids, each iteration performs two Jacobi steps. Convergence testing could be added as before.123456789101112131415161718192021222324252627282930313233343536373839404142434445pthread_mutex_t barrier; // mutex semaphore for the barrierpthread_cond_t go; // condition variable for leavingint numArrived = 0;void Barrier() &#123; pthread_mutex_lock(&amp;barrier); numArrived++; if (numArrived == numWorkers) &#123; numArrived = 0; pthread_cond_broadcast(&amp;go); &#125; else &#123; pthread_cond_wait(&amp;go, &amp;barrier); &#125; pthread_mutex_unlock(&amp;barrier);&#125;int main(int argc, char *argv[]) &#123; pthread_t workerid[MAXWORKERS]; pthread_mutex_init(&amp;barrier, NULL); pthread_cond_init(&amp;go, NULL); InitializeGrids(); for (i = 0; i &lt; numWorkers; i++) pthread_create(&amp;workerid[i], &amp;attr, Worker, (void *) i); for (i = 0; i &lt; numWorkers; i++) pthread_join(workerid[i], NULL);&#125;void *Worker(void *arg) &#123; int myid = (int) arg, rowA = myid*rowshare+1, rowB = rowA+rowshare-1; for (iters = 1; iters &lt;= numIters; iters++) &#123; for (i = rowA; i &lt;= rowB; i++) &#123; for (j = 1; j &lt;= gridSize; j++) &#123; grid2[i][j] = (grid1[i-1][j] + grid1[i+1][j] + grid1[i][j-1] + grid1[i][j+1]) * 0.25; &#125; &#125; Barrier(); for (i = rowA; i &lt;= rowB; i++) &#123; for (j = 1; j &lt;= gridSize; j++) &#123; grid1[i][j] = (grid2[i-1][j] + grid2[i+1][j] + grid2[i][j-1] + grid2[i][j+1]) * 0.25; &#125; &#125; Barrier(); &#125;&#125; Memory Consistency in PthreadsWeak consistency models can wreck naive DIY synchronization attempts! To enable portability, Pthreads mutex, semaphore and condition variable operations implicitly act as memory fences, executing architecture specific instructions. In effect, the C + Pthreads combination guarantees a weak consistency memory model, with the only certainties provided at uses of Pthreads primitives. For example, all writes by a thread which has released some mutex, are guaranteed to be seen by any thread which then acquires it. Nothing can be assumed about the visibility of writes which cannot be seen to be ordered by their relationship to uses of Pthread primitives. The programmer must also be careful to use only thread-safe code, which works irrespective of how many threads are active. Thread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specification without unintended interaction. Implementation is guaranteed to be free of race conditions when accessed by multiple threads simultaneously. Typical problems involve the use of non-local data. For example, imagine a non-thread safe malloc. Unluckily interleaved calls might break the underlying free space data structure. Some libraries will provide thread-safe versions (but of course, which pay an unnecessary performance penalty when used in a single threaded program). Java ConcurrencyJava是一种多线程 multi-threaded 编程语言，其同步模型是基于 monitor 概念，可用于开发多线程程序。多任务 multtasking 就是多个进程共享公共处理资源（如CPU）的时候。多线程将多任务的思想扩展到可以将单个应用程序中的特定操作细分为单独线程的应用程序。每个线程都可以并行运行。操作系统不仅在不同的应用程序之间分配处理时间，而且在应用程序内的每个线程之间分配处理时间。 Java ThreadsThreads can be created from classes which extend java.lang.Thread123456class Simple extends Thread &#123; public void run() &#123; System.out.println("this is a thread"); &#125;&#125;new Simple().start(); // implicitly calls the run() method Or implement java.lang.Runnable (so we can extend some other class too).1234class Bigger extends Whatever implements Runnable &#123; public void run() &#123; .... &#125;&#125;new Thread( new Bigger (...) ).start(); Wait to join with another thread123456789101112131415161718192021222324class Friend extends Thread &#123; private int me; public Friend (int i) &#123; me = i; &#125; public void run() &#123; System.out.println("Hello from thread " + me); &#125;&#125;class Hello throws java.lang.InterruptedException &#123; private static final int n = 5; public static void main(String[] args) &#123; int i; Friend t[] = new Friend[n]; System.out.println ("Hello from the main thread"); for (i=0; i&lt;n; i++) &#123; t[i] = new Friend(i); t[i].start(); &#125; for (i=0; i&lt;n; i++) &#123; t[i].join(); // might throw java.lang.InterruptedException &#125; System.out.println ("Goodbye from the main thread"); &#125;&#125; Java “Monitors”Java provides an implementation of the monitor concept (but doesn’t actually have monitor as a keyword). Any object in a Java program can, in effect, become a monitor, simply by declaring one or more of its methods to be synchronized, or by including a synchronized block of code. Each such object is associated with one, implicit lock. A thread executing any synchronized code must first acquire this lock. This happens implicitly (i.e. there is no source syntax). Similarly, upon leaving the synchronized block the lock is implicitly released. Java’s condition variable mechanism uses Signal-and-Continue semantics (The signalling thread continues uninterrupted). Each synchronizable object is associated with a single implicit condition variable. Manipulated with methods wait(), notify() and notifyAll(). We can only have one conditional variable queue per monitor (hence the absence of any explicit syntax for the condition variable itself). wait(): has three variance, one which waits indefinitely for any other thread to call notify or notifyAll method on the object to wake up the current thread. Other two variances puts the current thread in wait for specific amount of time before they wake up. notify(): wakes up only one thread waiting on the object and that thread starts execution. notifyAll(): wakes up all the threads waiting on the object, although which one will process first depends on the OS implementation. These methods can be used to implement producer consumer problem where consumer threads are waiting for the objects in Queue and producer threads put object in queue and notify the waiting threads. Readers &amp; Writers problem requires control access to some shared resource, such that there may be many concurrent readers, but only one writer (with exclusive access) at a time.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* 2 readers and 2 writers making 5 accesses eachwith concurrent read or exclusive write. */class ReadWrite &#123; // driver program -- two readers and two writers static Database RW = new Database(); // the monitor public static void main(String[] arg) &#123; int rounds = Integer.parseInt(arg[0],10); new Reader(rounds, RW).start(); new Reader(rounds, RW).start(); new Writer(rounds, RW).start(); new Writer(rounds, RW).start(); &#125;&#125;class Reader extends Thread &#123; int rounds; Database RW; private Random generator = new Random(); public Reader(int rounds, Database RW) &#123; this.rounds = rounds; this.RW = RW; &#125; public void run() &#123; for (int i = 0; i&lt;rounds; i++) &#123; try &#123; Thread.sleep(generator.nextInt(500)); &#125; catch (java.lang.InterruptedException e) &#123;&#125; System.out.println("read: " + RW.read()); &#125; &#125;&#125;class Writer extends Thread &#123; int rounds; Database RW; private Random generator = new Random(); public Writer(int rounds, Database RW) &#123; this.rounds = rounds; this.RW = RW; &#125; public void run() &#123; for (int i = 0; i&lt;rounds; i++) &#123; try &#123; Thread.sleep(generator.nextInt(500)); &#125; catch (java.lang.InterruptedException e) &#123;&#125; RW.write(); &#125; &#125;&#125; Implement the “database”. Allowing several readers to be actively concurrently. The last reader to leave will signal a waiting writer. Thus we need to count readers, which implies atomic update of the count. A reader needs two protected sections to achieve this. Notice that while readers are actually reading the data they do not hold the lock.1234567891011121314151617181920212223242526272829303132class Database &#123; private int data = 0; // the data int nr = 0; // synchronized means no more than one thread could do that private synchronized void startRead() &#123; nr++; &#125; private synchronized void endRead() &#123; nr--; if (nr==0) notify(); &#125;// awaken a waiting writer public int read() &#123; int snapshot; startRead(); snapshot = data; // read data endRead(); return snapshot; &#125; public synchronized void write() &#123; int temp; while (nr&gt;0) try &#123; wait(); &#125; catch (InterruptedException ex) &#123;return;&#125; temp = data; // next six lines are the ‘‘database’’ update! data = 99999; // to simulate an inconsistent temporary state try &#123; Thread.sleep(generator.nextInt(500)); // wait a bit, for demo purposes only &#125; catch (java.lang.InterruptedException e) &#123;&#125; data = temp+1; // back to a safe state System.out.println("wrote: " + data); notify(); // awaken another waiting writer &#125;&#125; We could express the same effect with synchronized blocks12345678910public int read() &#123; int snapshot; synchronized(this) &#123; nr++; &#125; // this - the database object snapshot = data; synchronized(this) &#123; nr--; if (nr==0) notify(); // awaken a waiting writer &#125; return snapshot;&#125; Would it be OK to use notifyAll() in read()? - Yes, but with extra transmission cost. Buffer for One Producer - One Consumer1234567891011121314/** (borrowed from Skansholm, Java from the Beginning) */public class Buffer extends Vector &#123; public synchronized void putLast (Object obj) &#123; addElement(obj); // Vectors grow implicitly notify(); &#125; public synchronized Object getFirst () &#123; while (isEmpty()) try &#123;wait();&#125; catch (InterruptedException e) &#123;return null;&#125; Object obj = elementAt(0); removeElementAt(0); return obj; &#125;&#125; The java.util.concurrent packageIncluding a re-usable barrier and semaphores (with P() and V() called acquire() and release()). It also has some thread-safe concurrent data structures (queues, hash tables). The java.util.concurrent.atomic package provides implementations of atomically accessible integers, booleans and so on, with atomic operations like addAndGet, compareAndSet. The java.util.concurrent.locks package provides implementations of locks and condition variables, to allow a finer grained, more explicit control than that provided by the built-in synchronized monitors. Message Passing ProgrammingWhen the underyling archictecture doesn’t support physically shared memory (for example, by distributing the OS and virtual memory system, i.e. Multicomputer architectures), we can make the disjoint nature of the address spaces apparent to the programmer, who must make decisions about data distribution and invoke explicit operations to allow interaction across these. Message passing, which is a approache to abstract and implement such a model, dominates the performance-oriented parallel computing world. Message passing is characterized as requiring the explicit participation of both interacting processes, since each address space can only be directly manipulated by its owner. The basic requirement is thus for send and receive primitives for transferring data out of and into local address spaces. The resulting programs can seem quite fragmented: we express algorithms as a collection of local perspectives. These are often captured in a single program source using Single Program Multiple Data (SPMD) style, with different processes following different paths through the same code, branching with respect to local data values and/or to some process identifier.12345678910111213141516// SPMD Compare-Exchangeco [me = 0 to P-1] &#123; // assumes P is even int a, temp; // these are private to each process now ...... // typical one step within a parallel sorting algorithm if (me%2 == 0) &#123; send (me+1, a); // send from a to process me+1 recv (me+1, temp); // receive into temp from process me+1 a = (a&lt;=temp) ? a : temp; // 取较小值 &#125; else &#123; send (me-1, a); recv (me-1, temp); a = (a&gt;temp) ? a : temp; // 取较大值 &#125; ......&#125; 1, Synchronization: Must a sending process pause until a matching receive has been executed (synchronous), or not (asynchronous)? Asynchronous semantics require the implementation to buffer messages which haven’t yet been, and indeed may never be, received. If we use synchronous semantics, the compare-exchange code above will deadlock. Can you fix it? One way s to make the send be a non-blocking one (MPI_Isend)Another way is to reverse the order of one of the send/receive pairs:12345&#125; else &#123; recv (me-1, temp); send (me-1, a); a = (a&gt;temp) ? a : temp; // 取较大值&#125; ...... 2, Addressing: When we invoke a send (or receive) do we have to specify a unique destination (or source) process or can we use wild-cards? Do we require program-wide process naming, or can we create process groups and aliases?3, Collective Operations: Do we restrict the programmer to single-source, single-destination, point-to-point messages, or do we provide abstractions of more complex data exchanges involving several partners?• Broadcast: Everyone gets a copy of the same value.• Scatter: Data is partitioned and spread across the group.• Gather: Data is gathered from across the group.• Reduction: Combine the gathered values with an associative operation.• Scan (Prefix): Reduce and also compute all the ordered partial reductions. Message Passing Interface (MPI) Message Passing Interface (MPI) is a standardized and portable message-passing standard. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran. Processes can be created statically when the program is invoked (e.g. using the mpirun command) or spawned dynamically. All communications take place within the context of “communication spaces” called communicators, which denote sets of processes, allows the MPI programmer to define modules that encapsulate internal communication structures. A process can belong to many communicators simultaneously. New communicators can be defined dynamically. Simple send/receives operate with respect to other processes in a communicator. Send must specify a target but receive can wild card on matching sender. Messages can be tagged with an extra value to aid disambiguation. Message-passing programming models are by default nondeterministic: the arrival order of messages sent from two processes A and B, to a third process C, is not defined. (However, MPI does guarantee that two messages sent from one process A, to another process B, will arrive in the order sent.) There are many synchronization modes and a range of collective operations. MPI Primitives (6 basics functions)1, int MPI_Init(int *argc, char ***argv): Initiate an MPI computation.2, int MPI_Finalize(): Terminate a computation.These must be called once by every participating process, before/after any other MPI calls. They return MPI_SUCCESS if successful, or an error code. Each process has a unique identifier in each communicator of which it is a member (range 0…members-1). MPI_COMM_WORLD is the built-in global communicator, to which all processes belong by default. A process can find the size of a communicator, and its own rank within it:3, int MPI_Comm_Size (MPI_Comm comm, int *np): Determine number of processes (comm - communicator). The processes in a process group are identified with unique, contiguous integers numbered from 0 to np-1.4, int MPI_Comm_rank (MPI_Comm comm, int *me): Determine my process identifier. 5, MPI_SEND: Send a message.6, MPI_RECV: Receive a message. MPI Task FarmA task farm is bag-of-tasks in which all the tasks are known from the beginning. The challenge is to assign them dynamically to worker processes, to allow for the possibility that some tasks may take much longer to compute than others. To simplify the code, we assume that there are at least as many tasks as processors and that tasks and results are just integers. In a real application these would be more complex data structures. Notice the handling of the characteristic non-determinism in the order of task completion, with tags used to identify tasks and results. We also use a special tag to indicate an “end of tasks” message.1234567891011121314151617181920212223242526272829303132333435363738394041424344/** SPMD style农场主分配任务给工人 */#define MAX_TASKS 100#define NO_MORE_TASKS MAX_TASKS+1#define FARMER 0 // 第一个 process 是farmer,其余是workerint main(int argc, char *argv[]) &#123; int np, rank; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); MPI_Comm_size(MPI_COMM_WORLD, &amp;np); if (rank == FARMER) &#123; farmer(np-1); &#125; else &#123; worker(); &#125; MPI_Finalize();&#125;void farmer (int workers) &#123; int i, task[MAX_TASKS], result[MAX_TASKS], temp, tag, who; MPI_Status status; // 1, 给每个人发送任务 for (i=0; i&lt;workers; i++) &#123; MPI_Send(&amp;task[i], 1, MPI_INT, i+1, i, MPI_COMM_WORLD); &#125; // 2, 收取任务结果, 继续发放剩余任务 while (i&lt;MAX_TASKS) &#123; MPI_Recv(&amp;temp, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status); who = status.MPI_SOURCE; tag = status.MPI_TAG; result[tag] = temp; MPI_Send(&amp;task[i], 1, MPI_INT, who, i, MPI_COMM_WORLD); i++; &#125; // 3, 所有任务已经完成, 收集最后一个任务结果, 并且发出结束任务信号 for (i=0; i&lt;workers; i++) &#123; MPI_Recv(&amp;temp, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status); who = status.MPI_SOURCE; tag = status.MPI_TAG; result[tag] = temp; MPI_Send(&amp;task[i], 1, MPI_INT, who, NO_MORE_TASKS, MPI_COMM_WORLD); &#125;&#125; Notice that the final loop, which gathers the last computed tasks, has a predetermined bound. We know that this loop begins after dispatch of the last uncomputed task, so there must be exactly as many results left to gather as there are workers.12345678910111213void worker() &#123; int task, result, tag; MPI_Status status; MPI_Recv(&amp;task, 1, MPI_INT, FARMER, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status); tag = status.MPI_TAG; while (tag != NO_MORE_TASKS) &#123; result = somefunction(task); MPI_Send(&amp;result, 1, MPI_INT, FARMER, tag, MPI_COMM_WORLD); MPI_Recv(&amp;task, 1, MPI_INT, FARMER, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status); tag = status.MPI_TAG; &#125;&#125; A worker is only concerned with its interaction with the farmer. 这样速度较快的worker可以自动接更多的任务，最终整体上达成 load balance。 Send in standard mode12int MPI_Send(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm) Send count items of given type starting in position buf to process dest in communicator comm, tagging the message with tag (which must be non-negative). There are corresponding datatypes for each basic C type, MPI_INT, MPI_FLOAT etc, and also facilities for constructing derived types which group these together. Are MPI_Send and MPI_Recv synchronous or asynchronous? Receive in standard mode12int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status) Receive count items of given type starting in position buf, from process source in communicator comm, tagged by tag. It attempts to receive a message that has an envelope corresponding to the specified tag, source, and comm, blocking until such a message is available. When the message arrives, elements of the specified datatype are placed into the buffer at address buf. This buffer is guaranteed to be large enough to contain at least count elements. Non-determinism (within a communicator) is achieved with “wild cards”, by naming MPI_ANY_SOURCE and/or MPI_ANY_TAG as the source or tag respectively. A receive can match any available message sent to the receiver which has the specified communicator, tag and source, subject to the constraint that messages sent between any particular pair of processes are guaranteed to appear to be non-overtaking. In other words, a receive cannot match message B in preference to message A if A was sent before B by the same process, the receive will receive the first one which was sent, not the first one to arrive. The status variable can be used subsequently to inquire about the size, tag, and source of the received message. Status information is returned in a structure with status.MPI_SOURCE and status.MPI_TAG fields. This is useful in conjunction with wild card receives, allowing the receiver to determine the actual source and tag associated with the received message. Prime Sieve Generator12345678910111213int main(int argc, char *argv[]) &#123; MPI_Comm nextComm; int candidate = 2, N = atoi(argv[1]); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_spawn("sieve", argv, 1, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &amp;nextComm, MPI_ERRCODES_IGNORE); while (candidate&lt;N) &#123; MPI_Send(&amp;candidate, 1, MPI_INT, 0, 0, nextComm); candidate++; &#125; candidate = -1; MPI_Send(&amp;candidate, 1, MPI_INT, 0, 0, nextComm); MPI_Finalize();&#125; We use MPI_Comm_spawn to dynamically create new sieve processes as we need them, and MPI_Comm_get_parent to find an inter-communicator to the process group which created us.12345678910111213141516171819202122int main(int argc, char *argv[]) &#123; MPI_Comm predComm, succComm; MPI_Status status; int myprime, candidate; int firstoutput = 1; // a C style boolean MPI_Init (&amp;argc, &amp;argv); MPI_Comm_get_parent (&amp;predComm); MPI_Recv(&amp;myprime, 1, MPI_INT, 0, 0, predComm, &amp;status); printf ("%d is a prime\n", myprime); MPI_Recv(&amp;candidate, 1, MPI_INT, 0, 0, predComm, &amp;status); while (candidate!=-1) &#123; if (candidate%myprime != 0) &#123; // not sieved out if (firstoutput) &#123; // create my successor if necessary MPI_Comm_spawn("sieve", argv, 1, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &amp;succComm, MPI_ERRCODES_IGNORE); firstoutput = 0; &#125; MPI_Send(&amp;candidate, 1, MPI_INT, 0, 0, succComm) // pass on the candidate &#125; MPI_Recv(&amp;candidate, 1, MPI_INT, 0, 0, predComm, &amp;status); // next candidate &#125; if (!firstoutput) MPI_Send(&amp;candidate, 1, MPI_INT, 0, 0, succComm); // candidate=-1, shut down MPI_Finalize();&#125; The message flow is insured by the method in which new processes are spawned/created. Every time a new “sieve” process is spawned, MPI creates it in a new group/communicator. succComm is a handle to this new group which always contains only one process. Therefore, when a candidate is sent to the process, there is only one process in the succComm group and it has id 0. The Recv function works in the same way predComm is a handle of the parent group (i.e. group of the process that created this sieve). And because the parent was the only process in this group/communicator, it can be identified by id 0. In conclusion, a process creates at most one successor. This successor is the only process in its group/communicator. The succCom and predComm are handles to the children and parent groups respectively, both of which contain a single process with id 0 which is unique in its own group/communicator. Spawning New MPI Processes12int MPI_Comm_spawn (char *command, char *argv[], int p, MPI_Info info, int root, MPI_Comm comm, MPI_Comm *intercomm, int errcodes[]) This spawns p new processes, each executing a copy of program command, in a new communicator returned as intercomm. To the new processes, intercomm appears as MPI_COMM_WORLD. It must becalled by all processes in comm (it is “collective”), with process root computing the parameters. info and errcodes are used in system dependent ways to control/monitor process placement, errors etc. MPI_Comm_get_parent gives the new processes a reference to the communicator which created them. Synchronization in MPIMPI uses the term blocking in a slightly unconventional way, to refer to the relationship between the caller of a communication operation and the implementation of that operation (i.e. nothing to do with any matching operation).Thus, a blocking send complete only when it is safe to reuse the specified output buffer (because the data has been copied somewhere safe by the system). 注意这里跟前面提到的synchronous概念不一样，synchronous 强调接收成功才是判断发送成功与否的标识，而 blocking 只需要保证缓存可以被安全改写即可。 In contrast, a process calling a non-blocking send continues immediately with unpredictable effects on the value actually sent. Similarly, there is a non-blocking receive operation which allows the calling process to continue immediately, with similar issues concerning the values which appear in the buffer. 意义在于，当需要发送的信息字节非常巨大时，发送和接收耗时都非常久，这时候如果可以不需要等待这些巨量信息的传输而直接继续下一个任务，则能提高效率。 To manage these effects, there are MPI operations for monitoring the progress of non-blocking communications (effectively, to ask, “is it OK to use this variable now?”). - The idea is that with careful use these can allow the process to get on with other useful work even before the user-space buffer has been safely stored. Blocking Communication Semantics in MPIMPI provides different blocking send operations, vary in the level of synchronization they provide. Each makes different demands on the underlying communication protocol (i.e. the implementation). 1, Synchronous mode send (MPI_Ssend) is blocking and synchronous, only complete when a matching receive has been found. 2, Standard mode send (MPI_Send) is blocking. Its synchronicity depends upon the state of the implementation buffers, in that it will be asynchronous unless the relevant buffers are full, in which case it will wait for buffer space (and so may appear to behave in a “semi” synchronous fashion). 3, Buffered mode send (MPI_Bsend) is blocking and asynchronous, but the programmer must previously have made enough buffer space available (otherwise an error is reported). There are associated operations for allocating the buffer space. Receiving with MPI_Recv blocks until a matching message has been completely received into the buffer (so it is blocking and synchronous). MPI also provides non-blocking sends and receives which return immediately (i.e. possibly before it is safe to use/reuse the buffer). There are immediate versions of all the blocking operations (with an extra “I” in the name). For example, MPI_Isend is the standard mode immediate send, and MPI_Irecv is the immediate receive. Non-blocking operations have an extra parameter, called a ‘request’ which is a handle on the communication, used with MPI_Wait and MPI_Test to wait or check for completion of the communication (in the sense of the corresponding blocking version of the operation). Probing for MessagesA receiving process may want to check for a potential receive without actually receiving it. For example, we may not know the incoming message size, and want to create a suitable receiving buffer. int MPI_Probe(int src, int tag, MPI_Comm comm, MPI_Status *status) behaves like MPI_Recv , filling in *status, without actually receiving the message. There is also a version which tests whether a message is available immediately int MPI_Iprobe(int src, int tag, MPI_Comm comm, int *flag, MPI_Status *status) leaving a (C-style) boolean result in *flag (i.e. message/no message). We can then determine the size of the incoming message by inspecting its status information. int MPI_Get_count(MPI_Status *status, MPI_Datatype t, int *count) sets *count to the number of items of type t in message with status *status. We could use these functions to receive (for example) a message containing an unknown number of integers from an unknown source, but with tag 75, in a given communicator comm.12345MPI_Probe(MPI_ANY_SOURCE, 75, comm, &amp;status);MPI_Get_count(&amp;status, MPI_INT, &amp;count);buf = (int *) malloc(count*sizeof(int));source = status.MPI_SOURCE;MPI_Recv(buf, count, MPI_INT, source, 75, comm, &amp;status); Collective OperationsMPI offers a range of more complex operations which would otherwise require complex sequences of sends, receives and computations. These are called collective operations, because they must be called by all processes in a communicator. 1, MPI_Bcast broadcasts count items of type t from buf in root to buf in all other processes in comm:12int MPI_Bcast (void *buf, int count, MPI_Datatype t, int root, MPI_Comm comm) 2, MPI_Scatter is used to divide the contents of a buffer across all processes.12int MPI_Scatter (void *sendbuf, int sendcount, MPI_Datatype sendt, void *recvbuf, int recvcount, MPI_Datatype recvt, int root, MPI_Comm comm) $i^{th}$ chunk (size sendcount) of root‘s sendbuf is sent to recvbuf on process $i$ (including the root process itself). The first three parameters are only significant at the root. Counts, types, root and communicator parameters must match between root and all receivers. 3, MPI_Gather is the inverse of MPI_Scatter. Instead of spreading elements from one process to many processes, MPI_Gather takes elements from many processes and gathers them to one single process. MPI_Gather takes elements from each process and gathers them to the root process. The elements are ordered by the rank of the process from which they were received. Only the root process needs to have a valid receive buffer. The recv_count parameter is the count of elements received per process, not the total summation of counts from all processes.123MPI_Gather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator) 4, MPI_Allreduce computes a reduction, such as adding a collectionof values together. No root, all Processes receive the reduced result.12int MPI_Allreduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype sendt, MPI_Op op, MPI_Comm comm) Reduces elements from all send buffers, point-wise, to count single values, using op, storing result(s) in all receive buffers. The op is chosen from a predefined set (MPI_SUM, MPI_MAX etc) or constructed with user code and MPI_Op_create. MPI_Allreduce is the equivalent of doing MPI_Reduce followed by an MPI_Bcast. Jacobi (1-dimensional wrapped), each neighour is owned by distinct process, thus could not read each other’s data - introduce a layer of message passing, introduce halo as buffer.123456789101112131415161718192021222324252627282930// here for convenience MPI_Sendrecv combines a send and a receive.int main(int argc, char *argv[]) &#123; MPI_Comm_size(MPI_COMM_WORLD, &amp;p); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) read_problem(&amp;n, work); // 数据存在 root - 0号进程 MPI_Bcast(&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD); // 广播数据 mysize = n/p; // assume p divides n, for simplicity local = (float *) malloc(sizeof(float) * (mysize+2)); //include fringe/halo MPI_Scatter(work, mysize, MPI_FLOAT, &amp;local[1], mysize, MPI_FLOAT, 0, MPI_COMM_WORLD); // scatter 分发数据到各process主位置 left = (rank+p-1)%p; // who is my left neighour? right = (rank+1)%p; // who is my right neighour? do &#123; //[0]和[mysize+1]halo MPI_Sendrecv(&amp;local[1], 1, MPI_FLOAT, left, 0, // send this &amp;local[mysize+1], 1, MPI_FLOAT, right, 0, // receive this MPI_COMM_WORLD, &amp;status); // anti-clockwise MPI_Sendrecv(&amp;local[mysize], 1, MPI_FLOAT, right, 0, &amp;local[0], 1, MPI_FLOAT, left, 0, MPI_COMM_WORLD, &amp;status); // clockwise do_one_step(local, &amp;local_error); MPI_Allreduce(&amp;local_error, &amp;global_error, 1, MPI_FLOAT, MPI_MAX, MPI_COMM_WORLD); &#125; while (global_error &gt; acceptable_error); MPI_Gather (&amp;local[1], mysize, MPI_FLOAT, work, mysize, MPI_FLOAT, 0, MPI_COMM_WORLD); if (rank == 0) print_results(n, work);&#125; 12345int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag, void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status *status) CommunicatorsCommunicators define contexts within which groups of processes interact. All processes belong to MPI_COMM_WORLD from the MPI initialisation call onwards. Create new communicators from old ones by collectively callingMPI_Comm_split(MPI_Comm old, int colour, int key, MPI_Comm *newcomm) to create new communicators based on colors and keys:color - control of subset assignment (nonnegative integer). Processes with the same color are in the same new communicator.key - control of rank assignment (integer). Within each new communicator, processes are assigned a new rank in the range $0…p^{\prime} − 1$, where $p^{\prime}$ is the size of the new communicator. Ranks are ordered by (but not necessarily equal to) the value passed in as the key parameter, with ties broken by considering process rank in the parent communicator. This can be helpful in expressing algorithms which contain nested structure. For example, many divide-and-conquer algorithms split the data and machine in half, process recursively within the halves, then unwind to process the recursive results back at the upper level.1234567891011//Divide &amp; Conquer Communicatorsvoid some_DC_algorithm ( ..., MPI_Comm comm) &#123; MPI_Comm_size(comm, &amp;p); MPI_Comm_rank(comm, &amp;myrank); ... pre-recursion work ... if (p&gt;1) &#123; MPI_Comm_split (comm, myrank&lt;(p/2), 0, &amp;subcomm); // two sub-machines some_DC_algorithm (..., subcomm); // recursive step // in both sub-machines &#125; else do_base_case_solution_locally(); ... post-recursion work ...&#125; Task and Pattern Based ModelsProgramming explicitly with threads (or processes) has some drawbacks:• Natural expression of many highly parallel algorithms involves creation of far more threads than there are cores. Thread creation and scheduling have higher overheads than simpler activities like function calls (by a factor of 50-100).• The OS has control over the scheduling of threads to processor cores, but it does not have the application specific knowledge required to make intelligent assignments (for example to optimize cache re-use). Traditional OS concerns for fairness may be irrelevant or even counter-productive. To avoid this, programmers resort to complex scheduling and synchronization of a smaller number of coarser grained threads. How to avoid this? A number of languages and libraries have emerged which• separate the responsibility for identifying potential parallelism, which remains the application programmer’s job, from detailed scheduling of this work to threads and cores, which becomes the language/library run-time’s job.• provide abstractions of common patterns of parallelism, which can be specialized with application specific operations, leaving implementation of the pattern and its inherent synchronization to the system. These are sometimes called task based approaches, in contrast to traditional threaded models. Examples include OpenMP, which is a compiler/language based model, and Intel’s Threading Building Blocks library. Threading Building BlocksThreading Building Blocks (TBB) is a shared variable model, C++ template-based library. It uses generic programming techniques to provide a collection of parallel algorithms, each of which is an abstraction of a parallel pattern. It also provides a direct mechanism for specifying task graphs and a collection of concurrent data structures and synchronization primitives. 泛型程序设计（generic programming）是程序设计语言的一种风格或范式，允许程序员在强类型程序设计语言中编写代码时使用一些以后才指定的类型，在实例化时作为参数指明这些类型。 It handles scheduling of tasks, whether explicit programmed or inferred from pattern calls, to a fixed number of threads internally. In effect, this is a hidden Bag-of-Tasks, leaving the OS with almost nothing to do. Game of Life (cs106b 作业1) Original Code for a Step123456789101112enum State &#123;DEAD,ALIVE&#125; ; // cell statustypedef State **Grid;void NextGen(Grid oldMap, Grid newMap) &#123; int row, col, ncount; State current; for (row = 1; row &lt;= MAXROW; row++) &#123; for (col = 1; col &lt;= MAXCOL; col++) &#123; current = oldMap[row][col]; ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount);&#125; &#125; &#125; TBB parallel_for假设我们想将上面的函数NextGen应用到数组(网格)的每个元素，这个例子是可以放心使用并行处理模式的。函数模板tbb::parallel_for 将此迭代空间(Range)分解为一个个块，并把每个块运行在不同的线程上。要并行化这个循环，第一步是将循环体转换为可以在一个块上运行的形式 - 一个STL风格的函数对象，称为body对象，其中由operator()中处理。Game of Life Step Using parallel_for12345void NextGen(Grid oldMap, Grid newMap) &#123; parallel_for (blocked_range&lt;int&gt;(1, maxrow+1), // Range CompNextGen(oldMap, newMap), // Body affinity_partitioner()); // Partitioner&#125; Range defines a task(iteration) space, and its sub-division (partition) technique;Body defines the code which processes a range;Partitioner (optional parameter) influencing partitioning and scheduling strategy. The parallel_for Template:12template &lt;typename Range, typename Body&gt;void parallel_for(const Range&amp; range, const Body &amp;body); Requires definition of: A range space to iterate over Must define a copy constructor and a destructor a destructor to destroy these copies Defines is_empty() Defines is_divisible() Defines a splitting constructor, R(R &amp;r, split) A body type that operates on the range (or a subrange) Must define a copy constructor, which is invoked to create a separate copy (or copies) for each worker thread. Defines operator() In the C++ programming language, a copy constructor is a special constructor for creating a new object as a copy of an existing object.123456789101112131415161718//通用形式classname (const classname &amp;obj) &#123; // body of constructor&#125;//实例#include &lt;iostream&gt;using namespace std;class Line &#123; public: int getLength( void ); Line( int len ); // simple constructor Line( const Line &amp;obj); // copy constructor ~Line(); // destructor private: int *ptr;&#125;; Range ClassA blocked_range&lt;T&gt; is a template class provided by the library. It describes a one-dimensional iteration space over type T. and be queried for the beginning (r.begin()) and end (r.end()) of the range. The TBB runtime can break a blocked_range into two smaller ranges, each (roughly) half the size. Note that a blocked_range carries no problem data. The values in the range can be used as we choose, for example to index into arrays.Range is Generic:12345R::R (const R&amp;) // Copy constructorR::~R() // Destructorbool R::is_empty() const // True if range is emptybool R::is_divisible() const // True if range can be partitionedR::R (R&amp; r, split) // Splitting constructor; splits r into two subranges Besides the provided blocked_range and blocked_range2d, users can define their own ranges. TBB DIY Range Example: Compute Fibonacci numbers.12345678910class FibRange &#123; public: int n_ ; // represents the range corresponding to fib(n) FibRange(int n) : n_(n) &#123; &#125; FibRange(FibRange&amp; other, split) // split constructor : n_(other.n_ - 2) // initialize the new object &#123; other.n_ = other.n_ - 1;&#125; // reuse the other range object bool is_divisible() const &#123; return (n_ &gt; 10); &#125; // sequential threshold bool is_empty() const &#123; return n_ &lt; 0; &#125;;&#125;; Body Class123456789101112131415class CompNextGen &#123; Grid oldMap, newMap; public: CompNextGen (Grid omap, Grid nmap) : oldMap(omap), newMap(nmap) &#123;&#125; // 分割迭代空间的方式多种多样 void operator()( const blocked_range&lt;int&gt;&amp; r ) const &#123; for (int row = r.begin(); row &lt; r.end(); row++)&#123; // 这里按行分割 for (int col = 1; col &lt;= maxcol; col++) &#123; nState current = oldMap[row][col]; int ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount); &#125; &#125; &#125;&#125; Body is Generic123Body::Body(const Body&amp;) \\ Copy constructorBody::~Body() \\ Destructorvoid Body::operator() (Range&amp; subrange) const \\ Apply the body to subrange. Because the body object might be copied, its operator() should not modify the body hence should be declared const. Otherwise the modification might or might not become visible to the thread that invoked parallel_for, depending upon whether operator() is acting on the original or a copy.Credit from www.threadingbuildingblocks.org parallel_for partitions original range into subranges, and deals out subranges to worker threads in a way that: Balances load, Uses cache efficiently, and Scales. Game of Life 1D with C++11 Lambda Function, an alternative interface to parallel_for allows us to use a C++ lambda expression to avoid writing a body class.123456789void NextGen(Grid oldMap, Grid newMap) &#123; parallel_for (blocked_range&lt;int&gt;(1, maxrow+1), [&amp;](const blocked_range&lt;int&gt;&amp; r)&#123; for (int row = r.begin(); row &lt; r.end(); row++)&#123; for (int col = 1; col &lt;= MAXCOL; col++) &#123; State current = oldMap[row][col]; int ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount); &#125; &#125; &#125; );&#125; [&amp;]引入 lambda 表达式. 该表达式创建一个类似于CompNextGen的函数对象. 当局部变量在 lambda expression 之外声明，但又在lambda表达式内使用时, 它们被”捕获”为函数对象内的字段. [&amp;]指定引用，[=]指定按值捕获. TBB PartitionersTBB supports different partitioning strategy:1, tbb::parallel_for( range, body, tbb::simple_partitioner() ); forces all ranges to be fully partitioned (i.e. until is_divisible() fails).2, tbb::parallel_for( range, body, tbb::auto_partitioner() ); allows the TBB runtime to decide whether to partition the range (to improve granularity).3, tbb::parallel_for( range, body, tbb::affinity_partitioner ); is like auto_partitioner() but also, when the parallel_for is inside a loop, tries to allocate the same range to the same processor core across iterations to improve cache behaviour. Game of Life Using a 2D decomposition123456789101112131415161718void NextGen(Grid oldMap, Grid newMap) &#123; parallel_for (blocked_range2d&lt;int, int&gt; (1, maxrow+1, 1, maxcol+1), // Range CompNextGen(oldMap, newMap)); // Body auto_partitioner()); // Partitioner&#125;class CompNextGen &#123; Grid oldMap, Grid newMap; public: CompNextGen (Grid omap, Grid nmap) : oldMap(omap), newMap(nmap) &#123;&#125; // 二维分割 void operator()( const blocked_range2d&lt;int, int&gt;&amp; r ) const &#123; for (int row = r.rows().begin(); row &lt; r.rows().end(); row++)&#123; for (int col = r.cols().begin(); col &lt; r.cols().end(); col++) &#123; State current = oldMap[row][col]; int ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount); &#125; &#125; &#125;;&#125; blocked_range2d is partitioned in alternating dimensions, level by level. TBB parallel_reduce TemplateTBB parallel_reduce has similar structure to parallel_for but additionally allows bodies to gather results internally as they go along. We could parallelize a loop reduction (iterations are independent), as in a Numerical Integration example, with a parallel_for, but we would need a critical section of some kind to accumulate the partial results. parallel_reduce structures and hides this, with one further generic operation, called join.12template &lt;typename Range, typename Body&gt;void parallel_reduce (const Range&amp; range, Body &amp;body); 1234Body::Body( const Body&amp;, split ) //Splitting constructorBody::~Body() // Destructorvoid Body::operator() (Range&amp; subrange) const // Accumulate results from subrangevoid Body::join( Body&amp; rhs ); // Merge result of rhs into the result of this. When a worker thread is available, as decided by the task scheduler, parallel_reduce invokes the splitting constructor to create a subtask for the worker. When the subtask completes, parallel_reduce uses method join to accumulate the result of the subtask. It reuses Range concept from parallel_for.The Fib Body Class (with operator()), using the DIY range example - FibRange from above123456789101112131415161718class Fib &#123; public: int fsum_ ; Fib() : fsum_(0) &#123; &#125; Fib(Fib&amp; other, split) : fsum_(0) &#123; &#125; // use += since each body may accumulate more than one range void operator() (FibRange&amp; range) &#123; fsum_ += fib(range.n_ ); &#125; int fib(int n) &#123;if (n &lt; 2) return 1; else return fib(n-1)+fib(n-2);&#125; void join(Fib&amp; rhs) &#123; fsum_ += rhs.fsum_; &#125;;&#125;;int main( int argc, char* argv[] ) &#123; task_scheduler_init init(2); Fib f; parallel_reduce(FibRange(FIBSEED), f, simple_partitioner()); cout &lt;&lt; "Fib " &lt;&lt; FIBSEED &lt;&lt; " is " &lt;&lt; f.fsum_ &lt;&lt; "\n"; return 0;&#125; Using a simple_partitioner forces full splitting of the ranges. We could use auto_partitioner to let the TBB run-time system control this. The Task Scheduler如果一个算法不能自然地映射到前面提到的任何其中一种 high-level loop templates，可以使用 task scheduler 直接操作任务, 可以构建新的高级模板。 All of TBB’s parallel pattern constructs are implemented via the same underlying task scheduler, which executes a task graph representing the pattern. TBB also allows the programmer to (carefully!) create task graphs directly. This allows expression of unstructured task graphs, or the implementation and abstraction of further patterns. There are functions to create new tasks as children of existing tasks and to specify the control dependencies between them. How to code Fibonacci using tasks directly? The key method is task::execute, which we override with our application specific behaviour. Recursion is typically used to calculate Fibonacci number but leads to unbalanced task graph. Fibonacci - Task Spawning Solution - Use TBB tasks to thread creation and execution of task graph: Allocate space for the task by a special “overloaded new” and method task::allocate_root - Create root of a task tree. Tasks must be allocated by special methods so that the space can be efficiently recycled when the task completes. Construct task with the constructor FibTask(n, &amp;sum) invoked by new. When the task is run in step 3, it computes the nth Fibonacci number and stores it into *sum. Run the task and wait for completion with task::spawn_root_and_wait. 123456789101112131415161718192021222324252627282930#include "tbb/task.h"...long ParallelFib( long n ) &#123; long sum; FibTask&amp; a = *new(task::allocate_root()) FibTask(n, &amp;sum); task::spawn_root_and_wait(a); return sum;&#125;class FibTask: public task &#123; public: const long n; long* const sum; FibTask( long n_, long* sum_ ) : n(n_), sum(sum_) &#123;&#125; task* execute() &#123; // Overrides virtual function task::execute if( n &lt; CutOff ) &#123; *sum = SerialFib(n); &#125; else &#123; long x, y; FibTask&amp; a = *new( allocate_child() ) FibTask(n-1,&amp;x); FibTask&amp; b = *new( allocate_child() ) FibTask(n-2,&amp;y); set_ref_count(3); // Set to 3 = 2 children + 1 for wait spawn( b ); // Start b running. // Start a running and wait for all children (a and b). spawn_and_wait_for_all(a); *sum = x+y; // Do the sum &#125; return NULL; &#125;&#125;; The TBB scheduler runs tasks in a way that tends to minimize both memory demands and cross-thread communication. The intuition is that a balance must be reached between depth-first and breadth-first execution. At any point in execution, the collection of known tasks is maintained as a shared graph. Each thread maintains its own double-ended queue of tasks (roughly, as pointers into the shared graph). Newly spawned tasks are added to the front of the local queue. 当一条线程参与 task graph 时，它会不断按照优先原则执行下面的规则来获取任务: looks at the front of its local queue, which encourages locality within one thread’s work; 如果 deque 为空，则此规则不适用； 假如失败, steal a task from the back of one other (randomly chosen) thread’s queue, which encourages stealing of big tasks, and discourages locality across threads. LindaLinda presents an alternative conceptual model for parallelism, based around a small library of operations. The Linda model saw something of a revival in distributed java systems programming, under the name JavaSpaces. The key concept is that processes interact through tuple space, a global, content addressable memory, which is thread safe, with no race conditions, therefore does not require explicit locks. Each tuple is an ordered collection of typed data fields. Duplicate tuples are allowed. The tuple space itself acts like a monitor. If a process tries to access a tuplen, it is blocked until a matching tuple becomes available. Semaphore - Linda have tuple (or a set of tuples for a counting semaphore) that represent the locks. If someone needs to enter the lock, it waits until a tuple is available in the bag, pull it out of the bag and inserts it back into the tuple space. Processes run asynchronously and can operate on tuple space with six operations. 1, Add new tuple to tuple space: out(exp1, exp2, ...., expN); - evaluates the expressions in the parameter list before atomically placing a copy of the results as a new tuple in tuple space. It could be considered as an asynchronous send with a wild-card destination in message-passing. out(&quot;sum&quot;, 2, 3), out(&quot;Green&quot;, x*y, square(2)); 2, To take a tuple from tuple space: in(tuple-template); - atomically removes from tuple space a tuple which matches the template. template contains actual values and formal parameters (indicated by ?) to be assigned during the match. 匹配包含与实际值的匹配，以及与形式参数类型 types 相匹配. in is blocking, in the sense that the caller is suspended until a matching tuple becomes available. For example: in(&quot;sum&quot;,?i,?j) matches &quot;sum&quot;, assigns 2 to i and 3 to j and the tuple is removed from the tuple space. in(&quot;Green&quot;, ?y, ?r, FALSE);. We could think of in as a blocking, asynchronous receive, with wild-card source, but with additional constraints implied by the pattern matching. 3, Atomically read a tuple from tuple space with rd(tuple-template); 4, Tuples may also be created with eval(expr, expr, ...) which is like out, but dynamically creates new processes to evaluate each field of the tuple which has been expressed as a function call. The calling process continues immediately, and the resulting tuple enters tuple space atomically when all the newly sparked processes have terminated 5, Finally, there are non-blocking forms inp, rdp (p for “predicate”) which complete “immediately”, returning a boolean indicating whether or not a match occurred. This allow a process to carry on with a different task and then try again later. Bag of Tasks Implementation:同样以前面的 Adaptive Quadrature 为例. Use a (&quot;counts&quot;, x, y) tuple, in effect as a shared variable, to count the number of tasks and number of idle workers. The final field in a task tuple indicates whether this is a real task or a “no more tasks” signal.123456789101112131415161718192021222324252627int main () &#123; out("total", 0.0); out("counts", 1, P); // set initial #task and #idle out("task", a, b, f(a), f(b), approxarea, FALSE); // make initial task for (i = 0; i&lt;P; i++) eval(worker()); // create P workers in ("counts", 0, P); // no tasks left, and P workers idle in ("total", ?total); // get the result out ("task", 0.0, 0.0, 0.0, 0.0, 0.0, TRUE); // indicate no more tasks ... //use total&#125;int worker() &#123; while (true) &#123; in("task", ?left, ?right, ?fleft, ?fright, ?lrarea, ?gameOver); if (gameOver) &#123; // if gameOver == TRUE out ("task", 0.0, 0.0, 0.0, 0.0, 0.0, TRUE); // for others to see break; &#125; in("counts", ?size, ?idle); out("counts", size-1, idle-1); ... usual task calculations ... if (abs (larea + rarea - lrarea) &gt; EPSILON) &#123; // create new tasks out("task", left, mid, fleft, fmid, larea, FALSE); out("task", mid, right, fmid, fright, rarea, FALSE); in("counts", ?size, ?idle); out("counts", size+2, idle+1); &#125; else &#123; in ("total", ?total); out ("total", total+larea+rarea); in("counts", ?size, ?idle); out("counts", size, idle+1); &#125; &#125; &#125; Pipeline Implementation:Use eval() to create the sieve processes dynamically as we need them. The sieve processes eventually turn into part of an “array” of primes in tuple space. We ensure pipelined message flow by tagging tuples with their destination and position in the sequence.12345678910111213141516171819202122232425void main (int argc, char *argv[]) &#123; int i; eval("prime", 1, sieve(1)); // the 1st prime number, the 1st worker for (i=2; i&lt;LIMIT; i++) &#123; out("number", 1, i-1, i); // send number to sieve &#125;&#125;int sieve (int me) &#123; int n, p, in_seq=1, out_seq=1, stop=FALSE; in("number", me, in_seq, ?p); // in_seq = 1, first arrival is prime while (!stop) &#123; in_seq++; in("number", me, in_seq, ?n); // get the next candidate if (n==LIMIT) &#123; stop = TRUE; out("number", me+1, out_seq, n); // pass on the signal &#125; else if (n%p !=0) &#123; // if never created a successor before if (out_seq == 1) eval("prime", me+1, sieve(me+1)); // new sieve out("number", me+1, out_seq, n); // and its first input out_seq++; &#125; &#125; return p;&#125; Tuple SpaceLinda’s powerful matching model sets a demanding implementation challenge, way beyond the associative memory hardware used in on-chip caches. Indexing and hashing techniques adapted from relational database technology can help (e.g. Linda rd and SQL select). Advanced Linda implementations perform considerable compile-time analysis of program specific tuple usage. For example, possible tuples (in a given program) can be categorised into a set of classes by type signature, and stored separately.]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python Digest 之奇技淫巧]]></title>
    <url>%2Fpython-digest%2F</url>
    <content type="text"><![CDATA[What you will get from this Python digest:1, Learn advanced python programming.2, Learn new concepts, patterns, and methods that will expand your programming abilities, helping move you from a novice to an expert programmer.3, Practice going from a problem description to a solution, using a series of assignments. OperatorEmulating numeric typesIn-place operation: One modifies the data-structure itself 12345678910111213object.__iadd__(self, other)object.__isub__(self, other)object.__imul__(self, other)object.__imatmul__(self, other)object.__itruediv__(self, other)object.__ifloordiv__(self, other)object.__imod__(self, other)object.__ipow__(self, other[, modulo])object.__ilshift__(self, other)object.__irshift__(self, other)object.__iand__(self, other)object.__ixor__(self, other)¶object.__ior__(self, other) These methods are called to implement the augmented arithmetic assignments. These methods should attempt to do the operation in-place (modifying self) and return the result (which could be, but does not have to be, self). If x is an instance of a class with an __iadd__() method, x += y is equivalent to x = operator.iadd(x, y) 1234B = np.arange(12).reshape(4,3)for b in B: b += 1print(B) # B will be changed Object oriented ProgrammingClass Name Guidlineunderscore (_):• For storing the value of last expression in interpreter.• For ignoring the specific values. (so-called “I don’t care”)• To use as ‘Internationalization(i18n)’ or ‘Localization(l10n)’ functions.• To separate the digits of number literal value. To give special meanings and functions to name of variables or functions• _single_leading_underscore: weak “internal use” indicator, declaring private variables, functions, methods and classes in a module. Anything with this convention are ignored in from module import *.• single_trailing_underscore_: used by convention to avoid conflicts with Python keyword• double_leading_underscore: when naming a class attribute, invokes name mangling (inside class FooBar, boo becomes _FooBar__boo; see Designing for inheritance)• double_leading_and_trailing_underscore: “magic” objects or attributes that live in user-controlled namespaces. E.g. init, import or file. Never invent such names; only use them as documented. See Magic Attributes Designing for inheritanceIf your class is intended to be subclassed, and you have attributes that you do not want subclasses to use, consider naming them with double leading underscores and no trailing underscores. This invokes Python’s name mangling algorithm, where the name of the class is mangled into the attribute name. This helps avoid attribute name collisions should subclasses inadvertently contain attributes with the same name.• Note 1: Note that only the simple class name is used in the mangled name, so if a subclass chooses both the same class name and attribute name, you can still get name collisions.• Note 2: Name mangling can make certain uses, such as debugging and getattr(), less convenient. However the name mangling algorithm is well documented and easy to perform manually.• Note 3: Not everyone likes name mangling. Try to balance the need to avoid accidental name clashes with potential use by advanced callers. DescriptorMagic Attributes__init__ for initialization purpose. object.__dict__: A dictionary or other mapping object used to store an object’s (writable) attributes. Basically it contains all the attributes which describe the object under question. It can be used to alter or read the attributes. __call__ Is Python call-by-value or call-by-reference?Neither. In Python, (almost) everything is an object. What we commonly refer to as “variables” in Python are more properly called names. A variable is not an alias for a location in memory. Rather, it is simply a binding to a Python object, likewise, “assignment” is really the binding of a name to an object. Each binding has a scope that defines its visibility, usually the block in which the name originates.– https://jeffknupp.com/blog/2012/11/13/is-python-callbyvalue-or-callbyreference-neither/ Python实际上有两种对象。 一种是可变对象，表现出随时间变化的行为。可变对象的变更对与它绑定的所有名称都可见，如 Python list。 一种是不可变对象，值在创建后无法修改。 跟java的 immutable reference类似的是 Python tuple：虽然 tuple 不可变，那仅是针对其自身所绑定固定的对象而言tuple(list1, list2)，但tuple包含的元素对象list1, list2本身有自己的可变属性. 所以Python的方法调用中,foo(bar)只是在foo的作用域内创建一个与参数bar的绑定。 如果bar指向可变对象，当foo更改时，这些更改可以在函数foo的作用域外可见。 如果bar指向一个不可变的对象，foo只能在其自身本地空间中创建一个名称bar并将其绑定到其他对象。 Solving ProblemA general process to solve problem with three steps: understand, specify and design.1, Start with a vague understanding that you refine into a formal specification of a problem. In this step you want to take inventory of the concepts you are dealing with.2, Specify how this problem can be made amenable to being coded. What is the input and output? What output is desirable?3, Design working code ?? —-(1 Vague Understanding)–&gt;Formal specification of a problem —(2 Specify)—&gt;Amendable specification—(3 Design)—&gt;Working Code Program Design and DevelopmentDimensions of programming Correctness, Efficiency, Features, Elegance Each part takes time, learn to make Tradeoff: During the process, generally Correctness comes first. Test But pursuing the 100% Correctness is not the best choice. There is a balance of tradeoff, and sometimes saving some time and efforts to improving the Efficiency or adding more Features may be a better option. Elegance is good for maintaining and improving the program, which means saving for the future. Refactoring - moving along the Elegance direction without changing the other dimensions. DRY: don’t repeat yourself Reuse: save time and code lines, also reduce the possibility of mistakeCoding StyleFor Python, https://www.python.org/dev/peps/pep-0008 has emerged as the style guide that most projects adhere to; it promotes a very readable and eye-pleasing coding style. Here are the most important points extracted: Use 4-space indentation, and no tabs. 4 spaces are a good compromise between small indentation (allows greater nesting depth) and large indentation (easier to read). Tabs introduce confusion, and are best left out. Wrap lines so that they don’t exceed 79 characters. This helps users with small displays and makes it possible to have several code files side-by-side on larger displays. Use blank lines to separate functions and classes, and larger blocks of code inside functions. When possible, put comments on a line of their own. Use docstrings. Use spaces around operators and after commas, but not directly inside bracketing constructs: a = f(1, 2) + g(3, 4). Name your classes and functions consistently; the convention is to use CamelCase for classes and lower_case_with_underscores for functions and methods. Always use self as the name for the first method argument (see A First Look at Classes for more on classes and methods). Don’t use fancy encodings if your code is meant to be used in international environments. Python’s default, UTF-8, or even plain ASCII work best in any case. Likewise, don’t use non-ASCII characters in identifiers if there is only the slightest chance people speaking a different language will read or maintain the code. DocstringAn easy way to associate documentation with a function. Documentation Strings conventions The first line should always be a short, concise summary of the object’s purpose. The second line should be blank The following lines should be one or more paragraphs describing the object’s calling conventions, its side effects, etc. The following Python file shows the declaration of docstrings within a Python source file: 12345678910&quot;&quot;&quot;Assuming this is file mymodule.py, then this string, being thefirst statement in the file, will become the &quot;mymodule&quot; module&apos;sdocstring when the file is imported.&quot;&quot;&quot;class MyClass(object): &quot;&quot;&quot;The class&apos;s docstring&quot;&quot;&quot; def my_method(self): &quot;&quot;&quot;The method&apos;s docstring&quot;&quot;&quot;def my_function(): &quot;&quot;&quot;The function&apos;s docstring&quot;&quot;&quot; The following is an interactive session showing how the docstrings may be accessed: 12345&gt;&gt;&gt; import mymodule&gt;&gt;&gt; help(mymodule)Assuming this is file mymodule.py then this string, being thefirst statement in the file will become the mymodule modulesdocstring when the file is imported&gt;&gt;&gt; help(mymodule.MyClass)The class&apos;s docstring&gt;&gt;&gt; help(mymodule.MyClass.my_method)The method&apos;s docstring&gt;&gt;&gt; help(mymodule.my_function)The function&apos;s docstring&gt;&gt;&gt; TestIt is important that each part of the specification gets turned into a piece of code that implements it and a test that tests it. Extreme valuesassertInsert debugging assertions into a program. Assertions are not a substitute for unit tests or system tests, but rather a complement. Using Assertions Effectively Places to consider putting assertions: checking parameter types, classes, or values checking data structure invariants checking “can’t happen” situations (duplicates in a list, contradictory state variables.) after calling a function, to make sure that its return is reasonableTime Tracking time Track which part of the code is the bottle neck of efficiency >&gt; python -m cProfile file.py import cProfile, cProfile.run(&#39;test()&#39;) Aspect-oriented programming correct efficiency Tracking time: to find out the bottle neck function or algorithm Rethinking the implementation of the bottle neck Fewer Easier/smaller: Divide and Conquer debuggingEach part is done with some line of codes. Instead of mix different part of the code together, it would be better to define them as different function/class. Try to seperate them as much as possible. FunctionThere are many special and useful function implementation and control flow in python: lambda, map, filter, reduce, generator, etc.. Lambdaλ, istead of defining function with def and a specific function name, Lambda provide a convinent way to define a function using its own native logic and methematical expression.The benifits are• A small function could be defined wihtin the same code structure without seperating out a specific def function• Without bothering creating any proper funciton name for a small anonymous function. Lambda implementation1, Like nested function definitions, lambda functions can reference variables from the containing scope, returning a function from another function. This is often used to create function wrappers, such as Python’s decorators.123456789# uses a lambda expression to return a function&gt;&gt;&gt; def make_incrementor(n):... return lambda x: x + n...&gt;&gt;&gt; f = make_incrementor(42) # f is declared as a lambda function "lambda x: x+42" with parameter n = 42&gt;&gt;&gt; f(0) # call f with x=0 to return the42&gt;&gt;&gt; f(1)43 This is like creating a compiler to save process cost: some parameters like default values or initial values are compiled into the compiler, program process these parameter only once, then this compiler as a function could be called many times with other input parameters which varies every time the compiler is being called(like user input values). 2, Pass a small function as an argument, sorting or max by an alternate key1234&gt;&gt;&gt; pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]&gt;&gt;&gt; pairs.sort(key=lambda pair: pair[1])&gt;&gt;&gt; pairs[(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')] 123&gt;&gt;&gt; l =[('x',2),('y',4),('z',0)]&gt;&gt;&gt; max(l, key = lambda x: x[0])&gt;&gt;&gt; ('z', 0) Lambda with logic control flow1Lambda x,y: False if x&lt;y else x+y FilterConstruct a list from the elements of an iterable for which function returns true. If iterable is a string or a tuple, the result also has that type; otherwise it is always a list. filter(function, iterable) equals to if function is None: [item for item in iterable if item] if not: [item for item in iterable if function(item)] mult3 = filter(lambda x: x % 3 == 0, [1, 2, 3, 4, 5, 6, 7, 8, 9]) &gt;&gt;&gt; [3, 6, 9] See itertools.ifilter() and itertools.ifilterfalse() for iterator versions of this function, including a variation that filters for elements where the function returns false. MapApply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel12&gt;&gt;&gt; map(lambda x: x % 2, [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; [1, 0, 1, 0, 1, 0, 1, 0, 1] ReduceApply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. 1234In [1]: reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])Out[1]: 15 # ((((1+2)+3)+4)+5)In [1]: reduce(lambda a, b: &apos;&#123;&#125;, &#123;&#125;&apos;.format(a, b), [1, 2, 3, 4, 5, 6, 7, 8, 9])Out[1]: &apos;1, 2, 3, 4, 5, 6, 7, 8, 9&apos; List/Dict/Set Comprehensions List comprehensions: [ s for r, s in cards if r in &#39;JQK&#39; ] Dictionary comprehensions: {x: x ** 2 for x in range(5) if x % 2 == 0} Set comprehensions: {int(sqrt(x)) for x in range(30)} And in general, we can have any number of for statements, if statements, more for statements, more if statements. The whole is read from left to right Generator ExpressionsUnlike the for loop in the list comprehensions which walk through the whole loop, generator will walk one step in the for loop if a next() is called. The advantage is less indentation stop the loop early easier to edit Implementation of generator: g = (sq(x) for x in range(10) if x%2 == 0). The generator function is a promise, but no computation has been done yet. next(g) to call a one-time calculation. When reaching the end of for-loop in the generator, the next(g) comment will return a false called “StopIteration”. To avoid the “StopIteration” false Use a outer for statement: for xx in g: ... convert the generator to list: list(g) Generator functionsUsing a yield expression in a function definition is sufficient to cause that definition to create a generator function instead of a normal function. Yield expressions The yield expression is only used when defining a generator function, and can only be used in the body of a function definition. Yield implementation 12345def ints(start, end=None): i = start while i &lt;= end or end is None: yield i i = i + 1 1234567def fab(max): n, a, b = 0, 0, 1 while n &lt; max: yield b # print b a, b = b, a + b n = n + 1 IteratorThe true beneath For Statemet is iterable12for x in itmes: print x What the whole truth is:1234567it = iter(items)try: while True: x = next(it) print xexcept StopIteration: pass Overall, Python calls the thing that can be iterated over in a for loop an iterable. Strings and lists are examples of iterables, and so are generators. itertools library - Functions creating iterators for efficient looping.any(iterable): Return True if any element of the iterable is true. If the iterable is empty, return False. Unpacking Argument ListsThe * operator simply unpacks the tuple or list and passes them as the positional arguments to the function.12345&gt;&gt;&gt; list(range(3, 6)) # normal call with separate arguments[3, 4, 5]&gt;&gt;&gt; args = [3, 6]&gt;&gt;&gt; list(range(*args)) # call with arguments unpacked from a list[3, 4, 5] Handling different types of argument (*polymorphism)An argument could be different type: timedcalls(n,fn), if n is int isinstance(n,int), it means controling the how many times fn was called, while n is float, it means controling the total runtime of fn called eval()DecoratorMotivation: when applying a transformation to a function def f(self): ...definition...; f = dec(f), it becomes less readable with longer methods. It also seems less than pythonic to name the function three times for what is conceptually a single declaration. The solution is to place the decoration in the function’s declaration:123@decdef foo(cls): pass @propertyproperty(fget=None, fset=None, fdel=None, doc=None)A property object has three methods, getter(), setter(), and delete() to specify fget, fset and fdel at a later point. some_object = property(get_some_object,set_some_object) equals to123some_object = property() # make empty propertysome_object = some_object.getter(get_some_object) # assign fgetsome_object = some_object.setter(set_some_object) # assign fset Decorator as tools• Debug tool: help developping, count calls times, count excecute time• Performance: make the programme faster, such as dynamic programming algorithm• Expressiveness: doc string, explaining funciton• Trace: help to monitor the execution of the program, such as each level result printed with different indentation Disable decorator: dec = disabled, make the decorator disabled. Regular Expressionimport re Reference: A Regular Expression Matcher In C language, any number start with ‘0’ is interpreted as an octal number( base-8 number system ):‘012’ -&gt; int 10; ‘09’ -&gt; invalid Special characters• * match 0 or more repetitions of the preceding character. ab* will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s.• ? Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab? will match either ‘a’ or ‘ab’.• . (Dot) matches any single character• ^ (Caret) Matches the start of the string• $ Matches the end of the string or just before the newline at the end of the string, foo matches both ‘foo’ and ‘foobar’, while the regular expression foo$ matches only ‘foo’• + match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’. Commonly used expression• Upper case letter &#39;[A-Z]&#39;• Any alphanumeric character [a-zA-Z0-9_]• Decimal digit [0-9]• Non-digit character [^0-9]• Whitespace character [ \t\n\r\f\v] search(string[, pos[, endpos]]): Scan through string looking for a location where this regular expression produces a match, and return a corresponding MatchObject instance. Return None if no position in the string matches the pattern. re.findall(pattern, string, flags=0)：Return all non-overlapping matches of pattern in string, as a list of strings. String FormattingModulo(%): String and Unicode objects have one unique built-in operation: the % operator (modulo). This is also known as the string formatting or interpolation operator. Given format % values (where format is a string or Unicode object), % conversion specifications in format are replaced with zero or more elements of values.%d: Signed integer decimal.%s: String (converts any python object using str()).print &#39;%d: %s&#39; % (1, &#39;animal&#39;) &gt;&gt; 1: animal Python data structureNumpy indexingEllipsis: The same as .... Special value used mostly in conjunction with extended slicing syntax for user-defined container data types. a = [1,2,3], a[...] is actually the same as a None: extends one more demention by further slicing the corresponding c into smallest units.12345t = np.arange(27).reshape(3,3,3), #t shape is (3,3,3)t[None,].shape # (1, 3, 3, 3)t[...,None].shape # (3, 3, 3, 1)t[:, None,:].shape # (3, 1, 3, 3)t[:,:, None].shape # (3, 3, 1, 3) Reference• CS212 Design of Computer Program @Udacity, Course Wiki SyllabusLesson 1: How to think to solve problemLesson 2: Python features; InstrumentationLesson 3: Build function as tools; Define language; GrammarLesson 4: Dealing with Complexity Through SearchLesson 5: Dealing with Uncertainty Through Probability • The Python Tutorial• Open Book Project: How to Think Like a Computer Scientist: Learning with Python]]></content>
      <categories>
        <category>学习笔记</category>
        <category>编程</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software Testing - Informatics - University of Edinburgh 爱丁堡]]></title>
    <url>%2FUoE-st%2F</url>
    <content type="text"><![CDATA[Reference:http://www.inf.ed.ac.uk/teaching/courses/st/2017-18/index.htmlPezze and Young, Software Testing and Analysis: Process, Principles and Techniques, Wiley, 2007. Why Software Testing?1, 软件的漏洞, 错误和失效 Software Faults, Errors &amp; FailuresThe problem start with Faults, Fault(BUG): latent error, mistakes in programming. e.g add(x, y) = x * y.With the Faults in programs, if and only if executing add(x, y) = x * y, the fault being activated, and generate an Errors. Error: An incorrect internal state that is the manifestation of some fault Now we has an effective Error, if and only if we use the values from add(x, y) = x * y to contribute to the program function (such as, assign it to some variables), then we get the Failure. Failure : External, observable incorrect behavior with respect to the requirements or other description of the expected behavior. 总结: 软件的漏洞不一定会导致错误, 错误不一定会导致软件失效. 2, 软件工程需要验证确认 在软件项目管理、软件工程及软件测试中，验证及确认（verification and validation，简称V&amp;V）是指检查软件是否匹配规格及其预期目的的程序。验证及确认也被视为一种软件质量管理，是软件开发过程的一部分，一般归类在软件测试中。 Validation: 是否符合预期的目的，是否满足用户实际需求？ Verification: meets the specification? Verification and Validation （V&amp;V） start at the beginning or even before we decide to build a software product. V&amp;V last far beyond the product delivery as long as the software is in use, to cope with evolution and adaptations to new conditions. The distinction between the two terms is largely to do with the role of specifications. Validation is the process of checking whether the specification captures the customer’s needs, while verification is the process of checking that the software meets the specification. 3, 软件工程的可靠性 Dependability In software engineering, dependability is the ability to provide services that can defensibly be trusted within a time-period Assess the readiness of a product. Different measures of dependability:• Availability measures the quality of service in terms of running versus down time• Mean time between failures (MTBF) measures the quality of the service in terms of time between failures• Reliability indicates the fraction of all attempted operations that complete successfully JUnitsJUnit Terminology• A test runner is software that runs tests and reports results. Many implementations: standalone GUI, command line, integrated into IDE• A test suite is a collection of test cases.• A test case tests the response of a single method to a particular set of inputs.• A unit test is a test of the smallest element of code you can sensibly test, usually a single class. 如何使用请参考Java 测试. Test class@Before public void init(): Creates a test fixture by creating and initialising objects and values. @After public void cleanUp(): Releases any system resources used by the test fixture. Java usually does this for free, but files, network connections etc. might not get tidied up automatically. @Test public void noBadTriangles(), @Test public void scaleneOk(), etc.These methods contain tests for the Triangle constructor and its isScalene() method. Test assertstatic void assertTrue(boolean test),static void assertTrue(String message, boolean test),static void assertFalse(boolean test),static void assertFalse(String message, boolean test) 软件测试的核心问题和解决思路A key problem in software testing is selecting and evaluating test cases. Test case: A test case is a set of inputs, execution conditions, and a pass/fail criterion. Test case specification is a requirement to be satisfied by one or more actual test cases. Test suite: a set of test cases. Adequacy criterion: a predicate that is true (satisfied) or false (not satisfied) of a &lt; program, test suite &gt; pair. Adequacy criterion is a set of test obligations, which can be derived from several sources of information, including• specifications (functional and model-based testing)• detailed design and source code (structural testing),• model of system• hypothesized defects (fault-based testing),• security testing. Test Case Selection and Adequacy CriteriaHow do we know when the test suite is enough?It is impossibal to provide adequate test suite for a system to pass. Instead, design rules to highlight inadequacy of test suites: if outcome break the rule, then there is bugs, if not, then not sure… Test case specification: a requirement to be satisfied by one or more test cases. Test obligation: a partial test case specification, requiring some property deemed important to thorough testing. From:• Functional (black box specification Functional (black box, specification based): from software specifications• Structural (white or glass box): from code• Model-based: from model of system, models used in specification or design, or derived from code• Fault-based: from hypothesized faults (common bugs) Adequacy criterion: set of test obligations, a predicate that is true (satisfied) or false (not satisfied) of a (program, test suite) pair. A test suite satisfies an adequacy criterion if:• all the tests succeed (pass)• every test obligation in the criterion is satisfied by at least one of the test cases in the test suite. SatisfiabilitySometimes no test suite can satisfy a criterion for a given program, e.g. defensive programming style includes “can’t happen” sanity checks. Coping with Unsatisfiability:Approach A, exclude any unsatisfiable obligation from the criterion.• Example: modify statement coverage to require execution only of statements that can be executed - But we can’t know for sure which are executable! Approach B, measure the extent to which a test suite approaches an adequacy criterion• Example: if a test suite satisfies 85 of 100 obligations we have reached 85% coverage. An adequacy criterion is satisfied or not, a coverage measure is the fraction of satisfied obligations Subsumption relationTest adequacy criterion A subsumes test adequacy criterion B iff, for every program P, every test suite satisfying A with respect to P also satisfies B with respect to P. e.g. Exercising all program branches (branch coverage) subsumes exercising all program statements Functional TestingDesign functional test case: Generate test cases from specifications. Specification: A functional specification is a description of intended program behavior. Not based on the internals of the code but program specifications, functional testing is also called specification-based or black-box testing 黑箱測試. The core of functional test is systematic selection of test cases: partitioning the possible behaviors of the program into a finite number of homogeneous classes, where each such class can reasonably be expected to be consistently correct or incorrect. Test each category and boundaries between (experience suggests failures often lie at the boundaries). Functional test case design is an indispensable base of a good test suite, complemented but never replaced by structural and fault-based testing, because there are classes of faults that only functional testing effectively detects. Omission of a feature, for example, is unlikely to be revealed by techniques that refer only to the code structure. Partition StrategiesFailures are sparse in the whole input space, and dense in some specific regions, justified based on specification. Random (uniform):• Pick possible inputs uniformly• Avoids designer bias: The test designer can make the same logical mistakes and bad assumptions as the program designer (especially if they are the same person)• But treats all inputs as equally valuable Systematic (non-uniform, Partition Testing Strategies):• Try to select inputs that are especially valuable• Usually by choosing representatives of classes that are apt to fail often or not at all• (Quasi-)Partition: separates the input space into classes whose union is the entire space (classes may overlap), sampling each class in the quasi-partition selects at least one input that leads to a failure, revealing the fault. Steps of systematic approaches to form test cases from specifications:1, Decompose the specification. If the specification is large, break it into independently testable features (ITF) to be considered in testing:• An ITF is a functionality that can be tested independently of other functionalities of the software under test. It need not correspond to a unit or subsystem of the software.• ITFs are described by identifying all the inputs that form their execution environments.• ITFs are applied at different granularity levels, from unit testing through integration and system testing. The granularity of an ITF depends on the exposed interface and whichever granularity(unit or system) is being tested.2, Identify Representative Classes of Values or Derive a Model• Representative values of each input• Representative behaviors of a model: simple input/output transformations don’t describe a system. We use models in program specification, in program design, and in test design3, Generate Test Case Specifications with constraints: The test case specifications represented by the combinations (cartesian product) of all possible inputs or model behaviors, which must be restricted by ruling out illegal combinations and selecting a practical subset of the legal combinations. Given a specification, there may be one or more techniques well suited for deriving functional test case. For example, the presence of several constraints on the input domain may suggest using a partitioning method with constraints, such as the category-partition method. While unconstrained combinations of values may suggest a pairwise combinatorial approach. If transitions among a finite set of system states are identifiable in the specification, a finite state machine approach may be indicated. Combinatorial approaches Combinatorial approaches to functional testing consist of a manual step of structuring the specification statement into a set of properties or attributes that can be systematically varied and an automatizable step of producing combinations of choices. 总体思路：1, Identify distinct attributes that can be varied: the data, environment, or configuration2, Systematically generate combinations to be tested Rational: test cases should be varied and include possible “corner cases” Environment describes external factors we need to configure in particular ways in order to specify and execute tests to fully exercise the system. Some common options: System memory, Locale. There are three main techniques that are successfully used in industrial environments and represent modern approaches to systematically derive test cases from natural language specifications:• category-partition approach to identifying attributes, relevant values, and possible combinations;• Pairwise (n-way) combination test a large number of potential interactions of attributes with a relatively small number of inputs;• provision of catalogs to systematize the manual aspects of combinatorial testing. Combinatorial approaches 将test cases的粗暴合成分解成一个个步骤，通过解析和综合那些可以量化和监控(并得到工具部分支持)的活动来逐步拆解问题. A combinatorial approach may work well for functional units characterized by a large number of relatively independent inputs, but may be less effective for functional units characterized by complex interrelations among inputs. Category-partition 和 pairwise partition 都是使用上面的总体思路，差别在于最后如何自动生成 test cases。 Category-partition将穷举枚举作为自动生成combinations的基本方法，同时允许测试设计者添加限制组合数量增长的约束条件。当这些约束能够反映应用域中的真实约束（例如，category-partition中的”error”条目）时，能够非常有效地消除许多冗余组合。 Decompose the specification into independently testable features for each feature: identify parameters, environment elements for each parameter and environment element: identify elementary characteristics (categories) Identify relevant/representative values: for each category identify representative (classes of) values normal values boundary values select extreme values within a class ((e.g., maximum and minimum legal values) select values outside but as close as possible to the class select interior (non-extreme) values of the class special values: 0 and 1, might cause unanticipated behavior alone or in combination with particular values of other parameters. error values: values outside the normal domain of the program Ignore interactions among values for different categories (considered in the next step) Introduce constraints: rule out invalid combinations. For single consgtraints, indicates a value class that test designers choose to test only once to reduce the number of test cases. 优点：Category partition testing gave us systematic approach -Identify characteristics and values (the creative step), generate combinations (the mechanical step). 缺点：test suite size grows very rapidly with number of categories. 不适合使用Category partition testing的情况：当缺乏应用领域的实际约束时，测试设计者为了减少组合数量被迫任意添加的约束（例如，”single”条目），此时不能很有效的减少组合数量。 Pairwise combination testingMost failures are triggered by single values or combinations of a few values. 为n个测试类选择组合时，除了简单地枚举所有可能的组合外，更实际的组合方案是在集合n中取出k(k&lt;n)项, 一般是二元组或三元组，总的 test cases 要包含所有 features 的两两（或三三）组合。生成测试用例时，先控制某一个变量逐一改变，记录配对了的变量，后续遇到重复的就可以忽略。这样即使没有加constraints也可以大大减少组合数（但我们也可以加constraints）。 使用低阶组合构建测试用例时，可能会遗漏某些高阶组合的情况。 Befinits of functional testingFunctional testing is the base-line technique for designing test cases:• Timely: Often useful in refining specifications and assessing testability before code is written• Effective: finds some classes of fault (e.g.,missing logic) that can elude other approaches• Widely applicable: to any description of program behavior serving as spec, at any level of granularity from module to system testing.• Economical: typically less expensive to design and execute than structural (code-based) test cases Early functional testing design:• Program code is not necessary: Only a description of intended behavior is needed• Often reveals ambiguities and inconsistency in spec• Useful for assessing testability, and improving test schedule and budget by improving spec• Useful explanation of specification, or in the extreme case (as in Extreme Programming), test cases are the spec Finite Models建模主要解决两个工程问题:• 首先，不能等到实际的产品出来后才分析和测试。• 其次，对实际产品进行彻底的测试是不切实际的，无论是否受制于所有可能的状态和输入。 模型允许我们在开发早期就着手分析，并随着设计的发展重复分析，并允许我们应用比实际情况更广泛的分析方法。更重要的是，这些分析很多都是可以自动化的。 Model program execution, emphasized control. A model is a representation that is simpler than the artifact it represents but preserves (or at least approximates) some important attributes of the actual artifact. A good model is:• compact: A model must be representable and manipulable in a reasonably compact form.• Predictive: well enough to distinguish between “good” and “bad” outcomes of analysis.• Semantically meaningful: interpret analysis results in a way that permits diagnosis of the causes of failure.• Sufficiently general: Models intended for analysis of some important characteristic must be general enough for practical use in the intended domain of application. 模型的表达：使用有向图描述程序模型。通常我们将它们绘制为”方框和箭头”图，由一组节点N的组成的集合和它们间的关系E（即ordered pairs的集合），edges。节点表示某种类型的实体，例如源代码的步骤，类或区域。边表示实体之间的某种关系。 模拟程序执行的模型，是该程序状态空间的抽象。通过抽象函数，程序运行状态空间中的状态与程序运行的finite state 模型中的状态相关联。但抽象函数无法完美呈现程序运行的所有细节，将实际的无限可能的状态折叠成有限必然需要省略一些信息，这就引入了不确定性nondeterminism。 有什么软件模型的基本概念，又有哪些可以应用于测试和分析的模型？ Controal flow graph程序中的单个步骤或方法的 Control flow 可以用 过程内流程图 intraprocedural control flow graph (CFG) 来表示. CFG 模拟通过单个过程或方法的可能运行路径, 是一个有向图，nodes 表示源代码的一个个区域，有向边 directed edges 表示程序可以在哪些代码区域间流转.123456789101112public static String collapseNewlines(String argStr) &#123; char last = argStr.charAt(0); StringBuffer argBuf = new StringBuffer(); for (int cIdx = 0 ; cIdx &lt; argStr.length(); cIdx++) &#123; char ch = argStr.charAt(cIdx); if (ch != '\n' || last != '\n') &#123; argBuf.append(ch); last = ch; &#125; &#125; return argBuf.toString();&#125; 左边是上面代码对应的CFG，右边的表格是Linear Code Sequence and Jump (LCSJ)，表示从一个分支到另一个分支的控制流程图的子路径 Nodes = regions of source code (basic blocks)• Basic block = maximal program region with a single entry and single exit point• Often statements are grouped in single regions to get a compact model• Sometime single statements are broken into more than one node to model control flow within the statementDirected edges = possibility that program execution proceeds from the end of one region directly to the beginning of another 为了便于分析，控制流程图通常会通过其他信息进一步加持。例如，后面介绍的数据流模型 data flow models 就是基于加持了有关变量被程序各个语句访问和修改的信息的CFG模型构建的. Call Graphs过程间流程 Interprocedural control flow 也可以表示为有向图。最基本的模型是调用图 call graphs, nodes represent procedures (methods, C functions, etc.) and edges represent the “calls” relation. 相较于CFG，调用图比有更多设计问题和权衡妥协， 因此基本调用图的表达方式是不固定的，特别是在面向对象的语言中，methods跟对象动态绑定。调用图存在Overapproximation现象，比如尽管方法A.check()永远不会实际调用C.foo()，但是一个典型的调用图会认为这个调用是可能的。 Context-sensitive call graph：调用图模型根据过程被调用的具体位置来表示不同行为。123456789101112131415161718192021public class Context &#123; public static void main(String args[]) &#123; Context c = new Context(); c.foo(3); c.bar(17); &#125; void foo(int n) &#123; int[] myArray = new int[ n ]; depends( myArray, 2) ; &#125; void bar(int n) &#123; int[] myArray = new int[ n ]; depends( myArray, 16) ; &#125; void depends( int[] a, int n ) &#123; a[n] = 42; &#125;&#125; Context sensitive analyses can be more precise than Context-insensitive analyses when the model includes some additional information that is shared or passed among procedures. But sensitive call graphs size grows exponentially, not fit for large program. Finite state machines前面介绍的模型都是都是基于源代码抽象出来的。不过，模型的构建也常常先于或者独立于源代码，有限状态机 finite state machines 就是这种模型。 有限状态机（finite-state machine，FSM）又称有限状态自动机，简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。 最简单的FSM由一个有限的状态集合和状态间的转移动作构成，可以有向图表示，节点表示状态，edges表示在状态间的转移需要的运算、条件或者事件。因为可能存在无限多的程序状态，所以状态节点的有限集合必须是具体编程状态的抽象。 Usually we label the edge to indicate a program operation, condition, or event associated with the transition. We may label transitions with both an external event or a condition and with a program operation that can be thought of as a “response” to the event. Such a finite state machine with event/response labels on transitions is called a Mealy machine. In the theory of computation, a Mealy machine is a finite-state machine whose output values are determined both by its current state and the current inputs. (This is in contrast to a Moore machine, whose output values are determined solely by its current state.) An alternative representation of finite state machines, including Mealy machines, is the state transition table:There is one row in the transition table for each state node and one column for each event or input. If the FSM is complete and deterministic, there should be exactly one transition in each table entry. Since this table is for a Mealy machine, the transition in each table entry indicates both the next state and the response (e.g., d / emit means “emit and then proceed to state d”). Structural TestingJudging test suite thoroughness based on the structure of the program itself, it is still testing product functionality against its specification, but the measure of thoroughness has changed to structural criteria. Also known as “white-box”, “glass-box”, or “codebased” testing. Motivation:1, If part of a program is not executed by any test case in the suite, faults in that part cannot be exposed. The part is a control flow element or combination, statements (or CFG nodes), branches (or CFG edges), fragments and combinations, conditions paths.2, Complements functional testing, another way to recognize cases that are treated differently3, Executing all control flow elements does not guarantee finding all faults: Execution of a faulty statement may not always result in a failure• The state may not be corrupted when the statement is executed with some data values• Corrupt state may not propagate through execution to eventually lead to failure4, Structural coverage: Increases confidence in thoroughness of testing, removes some obvious inadequacies Steps: Create functional test suite first, then measure structural coverage to identify see what is missing Interpret unexecuted elements may be due to natural differences between specification and implementation or may reveal flaws of the software or its development process inadequacy of specifications that do not include cases present in the implementation coding practice that radically diverges from the specification inadequate functional test suites Coverage measurements are convenient progress indicators, sometimes used as a criterion of completion. Control-flow Adequacy (expression coverage)A structural testing strategy that uses the program’s control flow as a model. Control flow elements include statements, branches, conditions, and paths. But a set of correct program executions in which all control flow elements are exercised does not guarantee the absence of faults. Test based on control-flow are concerned with expression coverage. Statement testingAdequacy criterion: each statement (or node in the CFG) must be executed at least once. Because a fault in a statement can only be revealed by executing the faulty statement. Coverage: #(executed statements) / #(statements) Minimizing test suite size is seldom the goal, but small test cases make failure diagnosis easier. Complete statement coverage may not imply executing all branches in a program. Branch testingAdequacy criterion: each branch (edge in the CFG) must be executed at least once. Coverage: #(executed branches) / #(branches)Traversing all edges of a graph causes all nodes to be visited: test suites that satisfy the branch adequacy criterion for a program P also satisfy the statement adequacy criterion for the same program But “All branches” can still miss conditions.Sample fault: missing operator (negation):digit_high == 1 || digit_low == -1, branch adequacy criterion can be satisfied by varying only part of the condition. Condition testingBasic condition adequacy criterion: each basic condition must be executed at least once. Coverage: #(truth values taken by all basic conditions) / 2 * #(basic conditions) Branch and basic condition are not comparable. Basic condition adequacy criterion can be satisfied without satisfying branch coverage Branch and condition adequacy: cover all conditions and all decisions Compound condition adequacy:• Cover all possible evaluations of compound conditions - A compound condition is either an atomic condition or some boolean formula of atomic conditions. For example, in the overall condition “A || (B &amp;&amp; C)“ the set of compound conditions are “A”, “B”, “C&quot;, &quot;B &amp;&amp; C“, “A || (B &amp;&amp; C)“.• Cover all branches of a decision tree.• Number of test cases grows exponentially with the number of basic conditions in a decision ($2^N$). 练习 - Write tests that provide statement, branch, and basic condition coverage over the following code:1234567891011121314int search(string A[], int N, string what)&#123; int index = 0; if ((N == 1) &amp;&amp; (A[0] == what))&#123; return 0; &#125; else if (N == 0)&#123; return -1; &#125; else if (N &gt; 1)&#123; while(index &lt; N)&#123; if (A[index] == what) return index; else index++; &#125; &#125; return -1;&#125; 先画出 CFG 图，再遍历： Modified condition/decision adequacy criterion (MC/DC)Motivation: Effectively test important combinations of conditions, without exponential blow up in test suite size. (Important combinations: Each basic condition shown to independently affect the outcome of each decision) 假如这些组合表明每一个条件都可以独立影响结果，那么就不要穷尽各种条件组合了，对于那些不影响结果的条件组合，测了也没有意义。 Requires:• For each basic condition $C_i$, two test cases• 控制变量，只改变 $C_i$：values of all evaluated conditions except $C_i$ are the same• Compound condition as a whole evaluates to True for one and False for the other，结果的改变表明 $C_i$ 可以独立影响结果 MC/DC:• basic condition coverage (C)• branch coverage (DC)• plus one additional condition (M): every condition must independently affect the decision’s output It is subsumed by compound conditions and subsumes all other criteria discussed so far - stronger than statement and branch coverage. A good balance of thoroughness and test size (and therefore widely used). Path TestingSometimes, a fault is revealed only through exercise of some sequence of decisions (i.e., a particular path through the program). Path coverage requires that all paths through the CFG are covered. In theory, path coverage is the ultimate coverage metric. But in practice, it is impractical if there is loop involed. Adequacy criterion: each path must be executed at least once:Coverage = #(Paths Covered) / #(Total Paths) Practical path coverage criteria:• The number of paths in a program with loops is unbounded - the simple criterion is usually impossible to satisfy• For a feasible criterion: Partition infinite set of paths into a finite number of classes• Useful criteria can be obtained by limiting•• the number of traversals of loops•• the length of the paths to be traversed•• the dependencies among selected paths Boundary Interior CoverageGroups paths that differ only in the subpath they follow when repeating the body of a loop.• Follow each path in the control flow graph up to the first repeated node• The set of paths from the root of the tree to each leaf is the required set of subpaths for boundary/interior coverage把分支拆分为每一条可能的 path. Limitations:1, The number of paths through non-loop branches (conditions) can still be exponential ($2^N$).2, Choosing input data to force execution of one particular path may be very difficult, or even impossible if the conditions are not independent. Loop Boundary CoverageSince coverage of non-looping paths is expensive, we can consider a variant of the boundary/interior criterion that treats loop boundaries similarly but is less stringent with respect to other differences among paths. Criterion: A test suite satisfies the loop boundary adequacy criterion iff for every loop:• In at least one test case, the loop body is iterated zero times• In at least one test case, the loop body is iterated once• In at least one test case, the loop body is iterated more than once For simple loops, write tests that: Skip the loop entirely. Take exactly one pass through the loop. Take two or more passes through the loop. (optional) Choose an upper bound N, and: M passes, where 2 &lt; M &lt; N (N-1), N, and (N+1) passes For Nested Loops: For each level, you should execute similar strategies to simple loops. In addition: Test innermost loop first with outer loops executed minimum number of times. Move one loops out, keep the inner loop at “typical” iteration numbers, and test this layer as you did the previous layer. Continue until the outermost loop tested. For Concatenated Loops, one loop executes. The next line of code starts a new loop: These are generally independent(Most of the time…) If not, follow a similar strategy to nested loops. Start with bottom loop, hold higher loops at minimal iteration numbers. Work up towards the top, holding lower loops at “typical” iteration numbers. Linear Code Sequences and JumpsThere are additional path-oriented coverage criteria that do not explicitly consider loops. Among these are criteria that consider paths up to a fixed length. The most common such criteria are based on Linear Code Sequence and Jump (LCSAJ) - sequential subpath in the CFG starting and ending in a branch. A single LCSAJ is a set of statements that come one after another (meaning no jumps) followed by a single jump. A LCSAJ starts at either the beginning of the function or at a point that can be jumped to. The LCSAJ coverage is what fraction of all LCSAJs in a unit are followed by your test suite. We can require coverage of all sequences of LCSAJs of length N.Stronger criteria can be defined by requiring N consecutive LCSAJs to be covered - $TER_{N+2}$:1, $TER_1$ is equivalent to statement coverage.2, $TER_2$ is equivalent to branch coverage3, $TER_3$ is LCSAJ coverage4, $TER_4$ is how many pairs of LCSAJ covered… Cyclomatic adequacy (Complexity coverage)There are many options for the set of basis subpaths. When testing, count the number of independent paths that have already been covered, and add any new subpaths covered by the new test. You can identify allpaths with a set of independent subpaths of size = the cyclomatic complexity. Cyclomatic coverage counts the number of independent paths that have been exercised, relative to cyclomatic complexity. • A path is representable as a bit vector, where each component of the vector represents an edge• “Dependence” is ordinary linear dependence between (bit) vectors If e = #(edges), n = #(nodes), c = #(connected components) of a graph, it is $e - n + c$ for an arbitrary graph, $e - n + 2$ for a CFG. Cyclomatic Complexity could be used to guess “how much testing is enough”.○ Upper bound on number of tests for branch coverage.○ Lower bound on number of tests for path coverage. And Used to refactor code.○ Components with a complexity &gt; some threshold should be split into smaller modules.○ Based on the belief that more complex code is more fault-prone. Procedure call coverageThe criteria considered to this point measure coverage of control flow within individual procedures - not well suited to integration or system testing, where connections between procedures(calls and returns) should be covered. Choose a coverage granularity commensurate with the granularity of testing - if unit testing has been effective, then faults thatremain to be found in integration testing will be primarily interface faults, and testing effort should focus on interfaces between units rather than their internal details. Procedure Entry and Exit Testing - A single procedure may have several entry and exit points.• In languages with goto statements, labels allow multiple entry points.• Multiple returns mean multiple exit points. Call coverage: The same entry point may be called from many points. Call coverage requires that a test suite executes all possible method calls. Satisfying structural criteriaThe criterion requires execution of statements that cannot be executed as a result of: defensive programming code reuse (reusing code that is more general than strictly required for the application) conditions that cannot be satisfied as a result of interdependent conditions paths that cannot be executed as a result of interdependent decisions Rather than requiring full adequacy, the “degree of adequacy” of a test suite is estimated by coverage measures. Dependence and Data Flow Models前面介绍的 Finite models (Control flow graph, call graph, finite state machines) 只是捕捉程序各部分之间依赖关系的其中一个方面。它们明确地表现控制流程，但不重视程序变量间的信息传递. Data flow models provide a complementary view, emphasizing and making explicit relations involving transmission of information. Models of data flow and dependence in software were originally developed in the field of compiler construction, where they were (and still are) used to detect opportunities for optimization. Definition-Use Pairs (Def-Use Pairs) The most fundamental class of data flow model associates the point in a program where a value is produced (called a “definition”) with the points at which the value may be accessed (called a “use”). Definitions - Variable declaration (often the special value “uninitialized”), Variable initialization, Assignment, Values received by a parameter.Use - Expressions, Conditional statements, Parameter passing, Returns. A Definition-Use pair is formed if and only if there is a definition-clear path between the Definition and the Use. A definition-clear path is a path along the CFG path from a definition to a use of the same variable without another definition of the variable in between. &lt;D,U&gt; pairs coverage: #(pairs covered)/ #(total number of pairs)If instead another definition is present on the path, then the latter definition kills the former. Definition-use pairs record direct data dependence, which can be represented in the form of a graph - (Direct) Data Dependence Graph, with a directed edge for each definition-use pair. The notion of dominators in a rooted, directed graph can be used to make this intuitive notion of “controlling decision” precise. Node M dominates node N if every path from the root of the graph to N passes through M. Analyses: Reaching definitionDefinition-use pairs can be defined in terms of paths in the program control flow graph.• There is an association $(d,u)$ between a definition of variable $v$ at $d$ and a use of variable $v$ at $u$ if and only if there is at least one control flow path from $d$ to $u$ with no intervening definition of $v$.• Definition $v_d$ reaches $u$ ($v_d$ is a reaching definition at $u$).• If a control flow path passes through another definition $e$ of the same variable $v$, we say that $v_e$ kills $v_d$ at that point. Practical algorithms do not search every individual path. Instead, they summarize the reaching definitions at a node over all the paths reaching that node. An algorithm for computing reaching definitions is based on the way reaching definitions at one node are related to reaching definitions at an adjacent node. Suppose we are calculating the reaching definitions of node n, and there is an edge $(p,n)$ from an immediate predecessor node $p$.We observe:• If the predecessor node $p$ can assign a value to variable $v$, then the definition $v_p$ reaches $n$. We say the definition $v_p$ is generated at $p$.• If a definition $v_d$ of variable $v$ reaches a predecessor node $p$, and if $v$ is not redefined at that node, then the definition is propagated on from $p$ to $n$. These observations can be stated in the form of an equation describing sets of reaching definitions.123456789101112/** Euclid's algorithm */public class GCD &#123; public int gcd(int x, int y) &#123; int tmp; // A: def x, y, tmp while (y != 0) &#123; // B: use y tmp = x % y; // C: def tmp; use x, y x = y; // D: def x; use y y = tmp; // E: def y; use tmp &#125; return x; // F: use x &#125;&#125; Reaching definitions at node E are those at node D, except that D adds a definition of x and replaces (kills) an earlier definition of x:$$\begin{equation}\begin{split} Reach(E) &amp;= ReachOut(D) \\ ReachOut(D) &amp;= (Reach(D) \backslash {x_A}) \cup {x_D}\end{split}\end{equation}$$Equations at the head of the while loop - node B, where values may be transmitted both from the beginning of the procedure - node A and through the end of the body of the loop - node E. The beginning of the procedure (node A) is treated as an initial definition of parameters and local variables:$$\begin{equation}\begin{split} Reach(B) &amp;= ReachOut(A) \cup ReachOut(E) \\ ReachOut(A) &amp;= gen(A) = {x_A, y_A, tmp_A } \\ ReachOut(E) &amp;= (Reach(E) \backslash {y_A}) \cup {y_D}\end{split}\end{equation}$$ (If a local variable is declared but not initialized, it is treated as a definition to the special value “uninitialized.”) General equations for Reach analysis:$$\begin{equation} \begin{split}Reach(n) &amp;= \mathop{\cup} \limits_{m \in pred(n)} ReachOut(m) \\ReachOut(n) &amp;=(Reach(n) \backslash kill(n)) \cup gen(n) \\\end{split} \end{equation}$$$gen(n) = v_n$, $v$ is defined or modified at $n$;$kill(n) = v_x$, $v$ is defined or modified at $x, x \ne n$. Reaching definitions calculation: first initializing the reaching definitions at each node in the control flow graph to the empty set, and then applying these equations repeatedly until the results stabilize. Analyses: Live and AvailAvailable expressions is another classical data flow analysis, used in compiler construction to determine when the value of a subexpression can be saved and reused rather than recomputed. An expression is available at a point if, for all paths through the control flow graph from procedure entry to that point, the expression has been computed and not subsequently modified. An expression is generated (becomes available) where it is computed and is killed (ceases to be available) when the value of any part of it changes (e.g., when a new value is assigned to a variable in the expression). The expressions propagation to a node from its predecessors is described by a pair of set equations:$$\begin{equation} \begin{split}Avail(n) &amp;= \mathop{\cap} \limits_{m \in pred(n)} AvailOut(m) \\AvailOut(n) &amp;=(Avail(n) \backslash kill(n)) \cup gen(n) \\\end{split} \end{equation}$$$gen(n)$, available, computed at $n$;$kill(n)$, has variables assigned at $n$. Reaching definitions combines propagated sets using set union, since a definition can reach a use along any execution path. Available expressions combines propagated sets using set intersection, since an expression is considered available at a node only if it reaches that node along all possible execution paths. Reaching definitions is a forward, any-path analysis; Available expressions is a forward, all-paths analysis. Live variables is a backward, any-path analysis that determines whether the value held in a variable may be subsequently used. Backward analyses are useful for determining what happens after an event of interest. A variable is live at a point in the control flow graph if, on some execution path, its current value may be used before it is changed, i.e. there is any possible execution path on which it is used. $$\begin{equation} \begin{split}Live(n) &amp;= \mathop{\cup} \limits_{m \in succ(n)} LiveOut(m) \\LiveOut(n) &amp;=(Live(n) \backslash kill(n)) \cup gen(n) \\end{split} \end{equation}$$$gen(n)$, $v$ is used at $n$;$kill(n)$, $v$ is modified at $n$. One application of live variables analysis is to recognize useless definitions, that is, assigning a value that can never be used. Iterative Solution of Dataflow EquationsInitialize values (first estimate of answer)• For “any path” problems, first guess is “nothing”(empty set) at each node• For “all paths” problems, first guess is “everything” (set of all possible values = union of all “gen” sets) Repeat until nothing changes• Pick some node and recalculate (new estimate) From Execution to Conservative Flow AnalysisWe can use the same data flow algorithms to approximate other dynamic properties• Gen set will be “facts that become true here”• Kill set will be “facts that are no longer true here”• Flow equations will describe propagation Example: Taintedness (in web form processing)• “Taint”: a user-supplied value (e.g., from web form) that has not been validated• Gen: we get this value from an untrusted source here• Kill: we validated to make sure the value is proper Data flow analysis with arrays and pointersThe models and flow analyses described in the preceding section have been limited to simple scalar variables in individual procedures. Arrays and pointers (dynamic references and the potential for aliasing) introduce uncertainty: Do different expressions access the same storage?• a[i] same as a[k] when i = k• a[i] same as b[i] when a = b (aliasing) The uncertainty is accomodated depending on the kind of analysis• Any-path: gen sets should include all potential aliases and kill set should include only what is definitely modified• All path: vice versa Scope of Data Flow Analysis过程内 Intraprocedural: Within a single method or procedure, as described so far. 过程之间 Interprocedural: Across several methods (and classes) or procedures Cost/Precision trade-offs for interprocedural analysis are critical, and difficult: context sensitivity, flow-sensitivity. Many interprocedural flow analyses are flow-insensitive• $O(n^3)$ would not be acceptable for all the statements in a program. Though $O(n^3)$ on each individual procedure might be ok• Often flow-insensitive analysis is good enough… considering type checking as an example Reach, Avail, etc were flow-sensitive sensitive, intraprocedural analyses.• They considered ordering and control flow decisions• Within a single procedure or method, this is (fairly) cheap - $O(n^3)$ for $n$ CFG nodes. Summary of Data flow models Data flow models detect patterns on CFGs: Nodes initiating the pattern Nodes terminating it Nodes that may interrupt it Often, but not always, about flow of information (dependence) Pros: Can be impy g lemented by efficient iterative algorithms Widely applicable (not just for classic “data flow” properties) Limitations: Unable to distinguish feasible from infeasible paths Analyses spanning whole programs (e.g., alias analysis) must trade off precision against computational cost Data Flow TestingIn structural testing,• Node and edge coverage don’t test interactions• Path-based criteria require impractical number of test cases: And only a few paths uncover additional faults, anyway• Need to distinguish “important” paths Data flow testing attempts to distinguish “important” paths: Interactions between statements - Intermediate between simple statement and branch coverage and more expensive path-based structural testing. Intuition: Statements interact through data flow• Value computed in one statement used in another Value computed in one statement, used in another• Bad value computation revealed only when it is used Adequacy criteria:• All DU pairs: Each DU pair is exercised by at least one test case• All DU paths: Each simple (non looping) DU path is exercised by at least one test case• All definitions: For each definition, there is at least one test case which exercises a DU pair containing it - Every computed value is used somewhere Limits: Aliases, infeasible paths - Worst case is bad (undecidable properties, exponential blowup of paths), so 务实的 pragmatic compromises are required Data flow coverage with complex structuresArrays and pointers• Under-estimation of aliases may fail to include some DU pairs• Over-estimation, may introduce unfeasible test obligations For testing, it may be preferrable to accept under-estimation of alias set rather than over-estimation or expensive analysis• 有争议的 Controversial: In other applications (e.g., compilers), a conservative over-estimation of aliases is usually required• Alias analysis may rely on external guidance or other globalanalysis to calculate good estimates• Undisciplined use of dynamic storage, pointer arithmetic, etc.may make the whole analysis infeasible Mutation testingFault-based Testing, directed towards “typical” faults that could occur in a program. Take a program and test suite generated for that program (using other test techniques) Create a number of similar programs (mutants), each differing from the original in one small way, i.e., each possessing a fault The original test data are then run through the mutants Then mutants either: To be dead: test data detect all differences in mutants, the test set is adequate. Remains live if: it is equivalent to the original program (functionally identical although syntactically different - called an equivalent mutant) or, the test set is inadequate to kill the mutant. The test data need to be augmented (by adding one or more new test cases) to kill the live mutant. Numbers of mutants tend to be large (the number of mutation operators is large as they are supposed to capture all possible syntactic variations in a program), hence random sampling, selective mutation operators (Offutt). Coverage - mutation score: #(killed mutants) / #(all non-equivalent mutants) (or random sample). Benifits:• It provides the tester with a clear target (mutants to kill)• It does force the programmer to think of the test data that will expose certain kinds of faults• Probably most useful at unit testing level Mutation operators could be built on• source code (body),• module interfaces (aimed at integration testing),• specifications: Petri-nets, state machines, (aimed at system testing) Tools: MuClipse Model based testingModels used in specification or design have structure• Useful information for selecting representative classes of behavior; behaviors that are treated differently with respect to the model should be tried by a thorough test suite• In combinatorial testing, it is difficult to capture that structure clearly and correctly in constraints Devise test cases to check actual behavior against behavior specified by the model - “Coverage” similar to structural testing, but applied to specification and design models. Deriving test cases from finite state machines: From an informal specification, to a finite state machine, to a test suite “Covering” finite state machines• State coverage: Every state in the model should be visited by at least one test case• Transition coverage •• Every transition between states should be traversed by at least one test case. •• A transition can be thought of as a (precondition, postcondition) pair. Models are useful abstractions• In specification and design, they help us think and communicate about complex artifacts by emphasizing key features and suppressing details• Models convey structure and help us focus on one thing at a time We can use them in systematic testing• If a model divides behavior into classes, we probably want to exercise each of those classes!• Common model-based testing techniques are based on state machines, decision structures, and grammars, but we can apply the same approach to other models. Testing Object Oriented SoftwareTypical OO software characteristics that impact testing• State dependent behavior• Encapsulation• Inheritance• 多态性 Polymorphism and dynamic binding• Abstract and generic classes• Exception handling Procedural software, unit = single program, function, or procedure, more often: a unit of work that may correspond to one or more intertwined functions or programs. Object oriented software:• unit = class or (small) cluster of strongly related classes (e.g., sets of Java classes that correspond to exceptions)• unit testing = 类内测试 intra-class testing• integration testing = 类之间测试 inter-class testing (cluster of classes)• dealing with single methods separately is usually too expensive (complex scaffolding), so methods are usually tested in the context of the class they belong to. Basic approach is orthogonal: Techniques for each major issue (e.g., exception handling, generics, inheritance ) can be applied incrementally and independently. Intraclass State Machine TestingBasic idea:• The state of an object is modified by operations• Methods can be modeled as state transitions• Test cases are sequences of method calls that traverse the state machine model State machine model can be derived from specification (functional testing), code (structural testing), or both. Testing with State Diagrams:• A statechart (called a “state diagram” in UML) may be produced as part of a specification or design - May also be implied by a set of message sequence charts (interaction diagrams), or other modeling formalisms.• Two options:1, Convert (“flatten”) into standard finite-state machine, then derive test cases2, Use state diagram model directly Intraclass data flow testingExercise sequences of methods• From setting or modifying a field value• To using that field value The intraclass control flow graph - control flow through sequences of method calls:• Control flow for each method• node for class• edges: from node class to the start nodes of the methods; from the end nodes of the methods to node class. Interclass TestingThe first level of integration testing for object-oriented software - Focus on interactions between classes Bottom-up integration according to “depends” relation - A depends on B - Build and test B, then A Start from use/include hierarchy - Implementation-level parallel to logical “depends” relation• Class A makes method calls on class B• Class A objects include references to class B methods - but only if reference means “is part of” In software engineering, a class diagram in the Unified Modeling Language (UML) is a type of static structure diagram that describes the structure of a system by showing the system’s classes, their attributes, operations (or methods), and the relationships among objects. Dependency is a weaker form of bond that indicates that one class depends on another because it uses it at some point in time. One class depends on another if the independent class is a parameter variable or local variable of a method of the dependent class. Interactions in Interclass Tests: Proceed bottom-up Consider all combinations of interactions example: a test case for class Order includes a call to a method of class Model, and the called method calls a method of class Slot, exercise all possible relevant states of the different classes. problem: combinatorial explosion of cases so select a subset of interactions: arbitrary or random selection plus all significant interaction scenarios that have been previously identified in design and analysis: sequence + collaboration diagrams Using Structural Information:• Start with functional testing: the specification (formal or informal) is the first source of information• Then add information from the code (structural testing) Interclass structural testingWorking “bottom up” in dependence hierarchy• Dependence is not the same as class hierarchy; not always the same as call or inclusion relation.• May match bottom-up build order Starting from leaf classes, then classes that use leaf classes,… Summarize effect of each method: Changing or using object state, or both - Treating a whole object as a variable (not just primitive types) Polymorphism and dynamic bindingOne variable potentially bound to methods of different (sub-)classes. The combinatorial approach: identify a set of combinations that cover all pairwise combinations of dynamic bindings. InheritanceWhen testing a subclass, We would like to re-test only what has not been thoroughly tested in the parent class. But we should test any method whose behavior may have changed. Reusing Tests with the Testing History Approach: Track test suites and test executions determine which new tests are needed determine which old tests must be re-executed New and changed behavior … new methods must be tested redefined methods must be tested, but we can partially reuse test suites defined for the ancestor other inherited methods do not have to be retested Abstract methods (and classes) - Design test cases when abstract method is introduced (even if it can t be executed yet) Behavior changes• Should we consider a method “redefined” if another new or redefined method changes its behavior?• The standard “testing history” approach does not do this• It might be reasonable combination of data flow (structural) OO testing with the (functional) testing history approach Testing exception handlingExceptions create implicit control flows and may be handled by different handlers. Impractical to treat exceptions like normal flow• too many flows: every array subscript reference, every memory, allocation, every cast, …• multiplied by matching them to every handler that could appear immediately above them on the call stack.• many actually impossible So we separate testing exceptions, and ignore program error exceptions (test to prevent them, not to handle them) What we do test: Each exception handler, and each explicit throw or re-throw of an exception. Integration TestingUnit (module) testing is a foundation, unit level has maximum controllability and visibility. Integration testing may serve as a process check• If module faults are revealed in integration testing, they signal inadequate unit testing• If integration faults occur in interfaces between correctly implemented modules, the errors can be traced to module breakdown and interface specifications.Integration test plan drives and is driven by the project “build plan” Structural orientation: Modules constructed, integrated and tested based on a hierarchical project structure - Top-down, Bottom-up, Sandwich, Backbone Functional orientation: Modules integrated according to application characteristics or features - Threads, Critical module. A “thread” is a portion of several modules that together provide a user-visible program feature. Component-based software testingWorking Definition of Component• Reusable unit of deployment and composition• Characterized by an interface or contract• Often larger grain than objects or packages - A complete database system may be a component Framework• Skeleton or micro-architecture of an application• May be packaged and reused as a component, with “挂钩 hooks” or “插槽 slots” in the interface contract Design patterns• Logical design fragments• Frameworks often implement patterns, but patterns are not frameworks. Frameworks are concrete, patterns are abstract Component-based system• A system composed primarily by assembling components, often “Commercial off-the-shelf” (COTS) components• Usually includes application-specific “glue code” Component Interface Contracts• Application programming interface (API) is distinct from implementation• Interface includes everything that must be known to use the component: More than just method signatures, exceptions, etc; May include non-functional characteristics like performance, capacity, security; May include dependence on other components. Testing a Component: Producer View• Thorough unit and subsystem testing• Thorough acceptance testing: Includes stress and capacity testing Testing a Component: User View• Major question: Is the component suitable for this application?• Reducing risk: Trial integration early System, Acceptance, and Regression Testing System TestingCharacteristics:• Comprehensive (the whole system, the whole spec)• Based on specification of observable behavior: Verification against a requirements specification, not validation, and not opinions• Independent of design and implementation Independence: Avoid repeating software design errors in system test design. Maximizing independence:• Independent V&amp;V: System (and acceptance) test performed by a different organization.• Independence without changing staff: Develop system test cases early System tests are often used to measure progress. As project progresses, the system passes more and more system tests. Features exposed at top level as they are developed. System testing is the only opportunity to verify Global Properties - Performance, latency, reliability, … Especially to find unanticipated effects, e.g., an unexpected performance bottleneck. Context-Dependent Properties is beyond system-global: Some properties depend on the system context and use, Example:• Performance properties depend on environment and configuration• Privacy depends both on system and how it is used• Security depends on threat profiles Stress TestingWhen a property (e.g., performance or real-time response) is parameterized by use - requests per second, size of database,… Extensive stress testing is required - varying parameters within the envelope, near the bounds, and beyond. Often requires extensive simulation of the execution environment, and requires more resources (human and machine) than typical test cases - Separate from regular feature tests, Run less often, with more manual control. Capacity, Security, Performance, Compliance, Documentation Testing. Acceptance testingEstimating dependability, measuring quality, not searching for faults. Requires valid statistical samples from operational profile(model), and a clear, precise definition of what is being measured. Quantitative dependability goals are statistical:• Reliability: Survival Probability - when function is critical during the mission time.• Availability: The fraction of time a system meets its specification - Good when continuous service is important but it can be delayed or denied• Failsafe: System fails to a known safe state• Dependability: Generalisation - System does the right thing at right time Usability, Reliability, Availability/Reparability Testing System ReliabilityThe reliability $R_F(t)$ of a system is the probability that no fault of the class $F$ occurs (i.e. system survives) during time $t \sim (t_{init}, t_{failure})$. Failure Probability $Q_F(t) = 1 -R_F(t)$. When the lifetime of a system is exponentially distributed, the reliability of the system is: $R(t) = e^{-\lambda t}$ where the parameter $\lambda$ is called the failure rate. MTTF: Mean Time To (first) Failure, or Expected Life.$ MTTF = E(t_f) = \int_0^\infty R(t)dt = \frac{1}{\lambda}$ Serial System Reliability: Serially Connected Components. Assuming the failure rates of components are statistically independent, The overall system reliability:$$R_{ser}(t) = \prod_{i=1}^n R_i(t) = e^{-t(\lambda_{ser})} = e^{-t(\sum_{i=1}^n \lambda_i)}$$$R_i(t) = e^{-\lambda_i t}$ is reliability of a single component $i$. Parallel System Reliability: Parallel Connected Components.$$R_{par}(t) = 1 - Q_{par}(t) = 1 - \prod_{i=1}^n Q_i(t) = 1 - \prod_{i=1}^n (1 - e^{-\lambda_i t}) = 1 - \prod_{i=1}^n (1 - R_i(t)) $$ For example:· if one is to build a serial system with 100 components each of which had a reliability of 0.999, the overall system reliability would be $0.999^{100} = 0.905$.· Consider 4 identical modules are connected in parallel, System will operate correctly provided at least one module is operational. If the reliability of each module is 0.95, the overall system reliability is $1-(1-0.95)^4 = 0.99999375$. Statistical testing is necessary for critical systems (safety critical, infrastructure, …), but difficult or impossible when operational profile is unavailable or just a guess, or when reliability requirement is very high. Process-based MeasuresBased on similarity with prior projects, less rigorous than statistical testing. System testing process - Expected history of bugs found and resolved:• Alpha testing: Real users, controlled environment• Beta testing: Real users, real (uncontrolled) environment• May statistically sample users rather than uses• Expected history of bug reports Regression TestingIdeally, software should improve over time. But changes can both• Improve software, adding features and fixing bugs• Break software, introducing new bugs - regressions Tests must be re-run after any changes. Make use of different techniques for selecting a subset of all tests to reduce the time and cost for regression testing. Regression Test SelectionFrom the entire test suite, only select subset of test cases whose execution is relevant to changes. Code-based Regression Test Selection: Only execute test cases that execute changed or new code. Control-flow and Data-flow Regression Test Selection: Re-run test cases only if they include changed elements – elements may be modified control flow nodes and edges, or definition-use (DU) pairs in data flow. To automate selection:• Tools record changed elements touched by each test case - stored in database of regression test cases• Tools note changes in program• Check test-case database for overlap Specification-based Regression Test Selection:• Specification-based prioritization: Execute all test cases, but start with those that related to changed and added features. Test Set MinimizationIdentify test cases that are redundant and remove them from the test suite to reduce its size.• Maximize coverage with minimum number of test cases.• Stop after a pre-defined number of iterations• Obtain an approximate solution by using a greedy heuristic Test Set Prioritisation• Sort test cases in order of increasing cost per additional coverage• Select the first test case• Repeat the above two steps until k test cases are selected or max cost is reached (whichever is first). Prioritized Rotating Selection: Execute some sooner than others, eventually execute all test cases. Possible priority schemes:• Round robin: Priority to least-recently-run test cases• Track record: Priority to test cases that have detected faults before - They probably execute code with a high fault density• Structural: Priority for executing elements that have not been recently executed - Can be coarse-grained: Features, methods, files. Test-Driven Development (TDD)Test-Driven Development (or test driven design) is a methodology. • Short development iterations.• Based on requirement and pre-written test cases.• Produces code necessary to pass that iteration’s test.• Refactor both code and tests.• The goal is to produce working clean code that fulfills requirements. Principle of TDD - Kent Beck defines:• Never write a single line of code unless you have a failing automated test.• Eliminate duplication TDD uses Black-box Unit test：1， 明确功能需求。2， 为功能需求编写 test。3， 运行测试，按理应该无法通过测试（因为还没写功能程序）。4， 编写实现该功能的代码，通过测试。5， 可选：重构代码（和 test cases），使其更快，更整洁等等。6， 可选：循环此步骤 Automating Test ExecutionDesigning test cases and test suites is creative, but executing test cases should be automatic. Example Tool Chain for Test Case Generation &amp; Execution:Combine …• A combinatorial test case generation (genpairs.py) to create test data• DDSteps to convert from spreadsheet data to JUnit test cases• JUnit to execute concrete test cases ScaffoldingCode to support development and testing.• Test driver: A “main” program for running a test• Test stubs: Substitute for called functions/methods/objects. Stub is an object that holds predefined data and uses it to answer calls during tests. It is used when we cannot or don’t want to involve objects that would answer with real data or have undesirable side effects. 代指那些包含了预定义好的数据并且在测试时返回给调用者的对象。Stub 常被用于我们不希望返回真实数据或者造成其他副作用的场景。 • Test harness: Substitutes for other parts of the deployed environment • Comparison-based oracle: need predicted output for each input. Fine for a small number of hand-generated test cases, e.g. hand-written JUnit test cases. • Self-Checking Code as Oracle: oracle written as self-checks, possible to judge correctness without predicting results. Advantages and limits: Usable with large, automaticallygenerated test suites, but often only a partial check. • Capture and Replay: If human interaction is required, capture the manually run test case, replay it automatically. With a comparison-based test oracle, behavior same as previously accepted behavior. Security Testing“Regular” testing aims to ensure that the program meets customer requirements in terms of features and functionality. Tests “normal” use cases - Test with regards to common expected usage patterns. Security testing aims to ensure that program fulfills security requirements. Often non-functional. More interested in misuse cases. Two common approaches:• Test for known vulnerability types• Attempt directed or random search of program state space to uncover the “weird corner cases” Penetration testing• Manually try to “break” software• Typically involves looking for known common problems. Fuzz testingSend semi-valid input to a program and observe its behavior.• Black-box testing - System Under Test (SUT) treated as a “black-box”• The only feedback is the output and/or externally observablebehavior of SUT. Input generation• Mutation based fuzzing: Start with a valid seed input, and “mutate” it. Can typically only find the “low-hanging fruit” - shallow bugs that are easy to find.• Generation based fuzzing: Use a specification of the input format (e.g. a grammar) to automatically generate semi-valid inputs - Very long strings, empty strings, Strings with format specifiers, “extreme” format strings, Very large or small values, values close to max or min for data type, Negative values. Almost invariably gives better coverage, but requires much more manual effort. The Dispatcher: running the SUT on each input generated by fuzzer module. The Assessor: automatically assess observed SUT behavior to determine if a fault was triggered. Concolic testingConcolic execution workflow:1, Execute the program for real on some input, and record path taken.2, Encode path as query to SMT solver and negate one branch condition3, Ask the solver to find new satisfying input that will give a different path. White-box testing method.• Input generated from control-structure of code to systematically explore different paths of the program.• Generational search (“whitebox fuzzing”): Performs concolic testing, but prioritizes paths based on how much they improve coverage. Greybox fuzzing▪ Coverage-guided semi-random input generation.▪ High speed sometimes beats e.g. concolic testing, but shares some limitations with mutation-based fuzzing (e.g. magic constants, checksums). Software Process Models - Software DevelopmentWaterfall model: Sequential, no feedback1, Requirements2, Design3, Implementation4, Testing5, Release and maintenance V-model: modified version of the waterfall model• Tests are created at the point the activity they validate is being carried out. So, for example, the acceptance test is created when the systems analysis is carried out.• Failure to meet the test requires a further iteration beginning with the activity that has failed the validation Boehm’s Spiral Model: focuse on controlling project risk and attempting formally to address project risk throughout the lifecycle.• V&amp;V activity is spread through the lifecycle with more explicit validation of the preliminary specification and the early stages of design. The goal here is to subject the early stages of design to V&amp;V activity.• At the early stages there may be no code available so we are working with models of the system and environment and verifying that the model exhibits the required behaviours. Extreme Programming (XP): one of [Agile Processes]• Advocates working directly with code almost all the time.• The 12 principles of XP summarise the approach. 1, Test-driven development; 2, The planning game; 3, On-site customer; 4, Pair programming; 5, Continuous integration; 6, Refactoring; 7, Small releases; 8, Simple design; 9, System metaphor; 10, Collective code ownership; 11, Coding standards; 12, 40-hour work week; • Development is test-driven.• Tests play a central role in refactoring activity.• “Agile” development mantra: Embrace Change. Facebook’s Process Model Perpetual development - a continuous development model. In this model, software will never be considered a finished product. Instead features are continuously added and adapted and shipped to users. Fast iteration is considered to support rapid innovation. Planning and Monitoring the ProcessMonitoring: Judging progress against the plan. Quality process: Set of activities and responsibilities. Follows the overall software process in which it is embedded.• Example: waterfall software process ––&gt; “V model”: unit testing starts with implementation and finishes before integration• Example: XP and agile methods ––&gt; emphasis on unit testing and rapid iteration for acceptance testing by customers. Strategies vs. PlansTest and Analysis Strategy:• Lessons of past experience: an organizational asset built and refined over time• Body of explicit knowledge: amenable to improvement, reduces vulnerability to organizational change (e.g., loss of key individuals) Elements of a Strategy:• Common quality requirements that apply to all or most products - unambiguous definition and measures• Set of documents normally produced during the quality process - contents and relationships• Activities prescribed by the overall process - standard tools and practices• Guidelines for project staffing and assignment of roles and responsibilities Main Elements of a Plan:• Items and features to be verified - Scope and target of the plan• Activities and resources - Constraints imposed by resources on activities• Approaches to be followed - Methods and tools• Criteria for evaluating results Schedule Risk• Critical path = chain of activities that must be completed in sequence and that have maximum overall duration• Critical dependence = task on a critical path scheduled immediately after some other task on the critical path Risk Planning• Generic management risk: personnel, technology, schedule• Quality risk: development, execution, requirements Contingency Plan• Derives from risk analysis• Defines actions in response to bad news - Plan B at the ready Process Monitoring• Identify deviations from the quality plan as early as possible and take corrective action Process ImprovementOrthogonal Defect Classification (ODC)• Accurate classification schema: for very large projects, to distill an unmanageable amount of detailed information• Two main steps1, Fault classification: when faults are detected, when faults are fixed.2, Fault analysis Root Cause Analysis (RCA)• Technique for identifying and eliminating process faults• Four main steps1, What are the faults?2, When did faults occur? When, and when were they found?3, Why did faults occur?4, How could faults be prevented?]]></content>
      <categories>
        <category>学习笔记</category>
        <category>软件工程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Natural Language Understanding - Informatics - University of Edinburgh 爱丁堡大学]]></title>
    <url>%2FUoE-nlu%2F</url>
    <content type="text"><![CDATA[References:Natural language understandingCS224n: Natural Language Processing with Deep LearningLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning 循环神经网络 RNNs For a sequence of input data (sequence of words, or speech) and sequence of output problem (many to many):· feed input $x_{t}$ into the RNN: feeding one word (represented as vector) at a time, e.g one word in a sentence from left to right, $x_1$ corresponds to the second word of a sentence. · $s_t$ is the hidden state at time step t. It is calculated based on the previous hidden state and the input at the current step:$s_t = f(Ux_t + Ws_{t-1})$. Function f is the activation. · o(t) is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $o_t = softmax(Vs_t)$. RNN shares the same parameters ($U, V, W$ above) across all steps. In addition to the above normal many to many structure RNNs, there are other non-sequence input or output: Many to one, e.g. when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. One to many: Music generation. 除了应用于语言模型, RNNs 还可以应用于· tagging, e.g. part-of-speech tagging, named entity recognition (many to many RNNs)· sentence classification, e.g. sentiment classification (many to one RNNs)· generate text, e.g. speech recognition, machine translation, summarization RNNs BackpropagationBackpropagation Through Time (BPTT): Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. RNNs trained with BPTT have difficulties learning long-term dependencies (e.g. dependencies between steps that are far apart) due to what is called the vanishing/exploding gradient problem. 梯度消失与爆炸The Vanishing/Exploding Gradient problem。 RNNs shares the same matrix (w, u, etc.) at each time step during forward prop and backprop. 求导数时, 根据链式法则, loss对各参数的导数会转换为loss对输出y的导数, 乘以y对隐含层的导数, 乘以隐含层相对隐含层之间的导数, 再乘以隐含层对参数的导数. 不同隐含层（举例如$h_t$和$h_k$）之间如果相隔太远, $h_t$对$h_k$的导数就变成多个jacobian矩阵的相乘， 对各个jacobian范数（norms）进行分析后，发现$h_t$对$h_k$的导数值在训练过程中会很快变得很极端（非常小或者非常大）。 Gradient作为传导误差以帮助系统纠正参数的关键角色，如果本身变得接近于0或者nan，那么我们就无法判断t和t+n的数据的依赖性（是没有依赖？还是因为vanish of gradient？还是因为参数设置错误？）。梯度衰减会直接降低模型学习长距离依赖关系的能力，给定一个时间序列，例如文本序列，循环神经网络较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。 在使用RNN学习language model的时候，非常容易出现梯度爆炸，解决办法是使用 gradient clipping 梯度裁剪，就是通过把梯度映射到另一个大小的空间，以限制梯度范数的最大值On the difficulty of training Recurrent Neural Networks。 虽然梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。一个缓解梯度衰减的方案是使用更好的参数初始化方案和激活函数（ReLUs）A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. 不过更主流的解决梯度衰减的方案是使用更复杂的rnn隐含单元: Gated Recurrent Units (GRU) introduced by Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation and LSTMs. Long Short-Term MemoryOutside of the vanilla RNNs work flow, LSTMs use gated cells as memory to chose what error to be remembered. The cells take as input the previous state $s_{t-1}$ and current input $x_t$. Thus help to solve the long-term dependencies. Whether the gated cell let information flow through (open) or not (closed) depends on its inner sigmoid activation layer with a pointwise multiplication operation. A sigmoid function values between 0 and 1, it could be used to describe how much information is allowed to through the cell. Take a most basic sequence problem as example - predict next word: the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject. LSTM的细胞的直观理解，就是细胞可以完整地保存信息，而新的输入可以诱发细胞对旧信息的遗忘，细胞自行决定记忆哪些新信息。 LSTM 用遗忘门来决定从 cell state 中丢弃哪些信息。Forget gate: Control how much information of pervious state $h_{t-1}$ should be forgetten in the current internal cell. Learned by a sigmoid layer called the “forget gate layer” 用输入门 Input gate 来决定有多少新信息是值得储存的（记忆）。Control how much new information is going to be remembered by internal state cell in current step t.1, an input gate (a sigmoid hidden layer) decides which values we’ll update.2, a hidden(tanh/relu) layer creates a vector of new candidate values $\hat{C}_t$, that could be added to the state. 下一步就可以更新旧的 cell sate $C_{t-1}$.Input and forget gates together allow the network to control what information is stored and overwritten at each step. Combine the forget and remember information together to update the previous cell state. 最后，用一个输出门 Output gate 来决定要输出的内容。1, Run a sigmoid layer to decide what parts of the cell state we’re going to output.2, put the cell state through tanh pointwise operation (to push the values to be between −1 and 1) and multiply it by the output of the output gate, so that we only output the parts decided by the output gate. 总的来说, LSTM有输入门、遗忘门和输出门。这三个门形式上，都是关于旧隐含状态和新输入向量的 Sigmoid 隐含神经网络层, 只是各自有各自的参数矩阵. Gated Recurring UnitGRU combines the forget and remember gates into one single gate. This combination leads to a simpler LSTMs model. This combined gate is called update gate. GRU first computes the update gate $z_t$ (another layer) based on current input word vector and hidden state. Then there is a reset gate r similarly but with different weights. The new memory content $\hat{h}_t$; Final memory $h_t$ at time step combines current and previous time steps: GRU intuition 重置门赋予了模型丢弃与未来无关的信息的能力。若重置门接近于0，则忽略之前的记忆，仅储存新加入的信息. 更新门控制过去的状态对现在的影响程度（即决定更新多少），如果接近于1，则 ht=zt*ht-1, 等同于把过去的信息完整复制到未来，相应地缓解梯度衰减。 短距离依赖的单元，过去的信息仅保留很短的时间，重置门一般很活跃，也就是数值在0和1之间频繁变动。 长距离依赖的单元，重置门较稳定（保留过去的记忆较长时间），而更新门较活跃。 不同RNNs变种的比较Vanilla RNNs Execution: Read the whole register h Update the whole register GRU Execution: Select a readable subset Read the subset Select a writable subset Update the subset 门控循环神经网络的训练 把参数矩阵初始化为正交 把遗忘门的bias初始化为1，默认不遗忘 别忘了梯度裁剪 注意dropout在RNNs中的应用不同于DNN和CNN Bidirectional RNNsBidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs. Meaning representations意思的表达有很多方法。一种有效的表示单词的含义的方法是 distributional semantic. Semantics (from Ancient Greek: σημαντικός sēmantikos, “significant”) is the linguistic and philosophical study of meaning, in language, programming languages, formal logics, and semiotics. 语义学 Semantics 在语言学中的研究目的在于找出语义表达的规律性、内在解释、不同语言在语义表达方面的个性以及共性；与计算机科学相关的语义学研究在于机器对自然语言的理解。 Tradition solution of usable meaning in a computer: Use e.g. WordNet, a resource containing lists of synonym sets and hypernyms. To convert natural language into values that computer understands, represent words as discrete symbols: Words can be represented by one-hot vectors, Vector dimension is the vocabulary. But there is no natural notion of similarity for one-hot vectors! So learn to encode similarity in the vectors themselves. The core idea is representing words by their context, building a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. Distributional models of meaning = vector-­space models of meaning = vector semantics.word vectors = word embeddings = word representations. Four kinds of vector modelsSparse vector representations:1, Mutual-­information weighted word co-­occurrence matrices Dense vector representations:2, Singular value decomposition (SVD): A special case of this is called LSA - Latent Semantic Analysis3, Neural­‐network­‐inspired models (skip­‐grams, CBOW)4, Brown clusters Prediction-­based models learn embeddings as part of the process of word prediction. Train a neural network to predict neighboring words. The advantages:· Fast, easy to train (much faster than SVD)· Available online in the word2vec package· Including sets of pretrained embeddings Word representation and Word2vecWord2vec is a framework for learning word vectors representation.Idea:1, We have a large corpus of text2, Every word in a fixed vocabulary is represented by a vector3, Go through each position t in the text, which has a center word c and context (“outside”) words o4, Use the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa)5, Keep adjusting the word vectors to maximize this probability 在上面第四点, 如果是给定中心词，计算上下文词, 那么就是 Skip-grams model, 比如 Given word wt, in a context window of 2C words, predict 4 context words [wt-2, wt-1, wt+1, wt+2]Skip-grams 给予模型跳词能力，比如 “I hit the tennis ball” 有三个trigrams: “I hit the”, “hit the tennis”, “the tennis ball”. 但是，这个句子也同样包含一个同样重要但是N-Gram无法提取的trigram:”hit the ball”. 而使用 skip-grams 允许我们跳过 “tennis” 生成这个trigram. 反之，给定 bag-of-words context, predict target word, 那就是 Continuous Bag of Words, CBOW model. 缺点：因为output size 等于 vocabulary，而 softmax 分母中需要求和每一个词的 output size × hidden units 的内积， 计算会非常昂贵。解决办法是使用负采样 negative sampling。 Word2vec的本质是遍历语料库的每一个词$w_i$，捕捉$w_i$与其上下文位置目标词的同时出现的概率。 目标函数 Obejective funtion (cost or loss function) J(θ):For each position $t = 1, … , T$, predict context words within a window of fixed size m, given center word, use chain rule to multiply all the probability to get the likelihood $L(θ)$:The θ is the vectors representations, which is the only parameters we needs to optimize(其实还有其他hyperparameters，这里暂时忽略). The loss function is the (average) negative log likelihood: Minimizing objective function ⟺ Maximizing predictive accuracy. The problem is how to calculate $P(w_{t+j} \mid w_t; θ)$: 每个词由两个向量表示（Easier optimization. Average both at the end）：$v_w$ when w is a center word, $u_w$ when w is a context word. Then for a center word c and a “outside” word o:The numerator contains dot product, compares similarity of o and c, larger dot product = larger probability. The denominator works as a normalization over entire vocabulary. 高频词二次采样 subsampling二次采样是指当决定是否选取一个词作为样本时，它被选择的概率反比于它出现的概率，这样不仅可以降低无意义但高频的词(“the”, “a”等)的重要性，也可以加快采样速度。$$P(w_i) = (\sqrt{\frac{z(w_i)}{0.001}} + 1) \cdot \frac{0.001}{z(w_i)}$$ $z(w_i)$ 是词$w_i$在语料库中的占比，如果”peanut”在10亿语料库中出现了1,000次, 那么z(“peanut”) = 1e-6. Negative sampling负采样是指每个训练样本仅更新模型权重的一小部分：only the output that represents the positive class(1) + other few randomly selected classes(0) are evaluated.该论文指出 负采样5-20个单词适用于较小的数据集，对于大型数据集只需要2-5个单词。 修改目标函数，选择k个负样本（即除了概率最高的那个目标词之外的其他词）： 这样可以最大化真正的外部词出现的概率，最小化随机负采样的词概率。 负面样本的选择是基于 unigram 分布 $f(w_i)$: 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。$$P(w_i) = \frac{ {f(w_i)}^{3/4} }{\sum_{j=0}^{n}\left( {f(w_j)}^{3/4} \right) }$$负采样的优点是：· Training speed is independent of the vocabulary size· Allowing parallelism.· 模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。 与传统的NLP方法比较在word2vec出现之前，NLP使用经典且直观的共生矩阵（co-occurrence matrix）来统计词语两两同时出现的频率，参考ANLP - Distributional semantic models。缺点也明显，词汇量的增加导致矩阵增大，需要大量内存，随之而来的分类模型出现稀疏性问题，模型不稳定。虽然可以使用SVD来降维，但是一个n×m矩阵的计算成本是O(mn2)浮点数（当n&lt;m），还是非常大的。而且很难并入新词或新文档。 目前融合了两种方法的优点的Glove是最常用的。 TODO(Glove)Morphological Recursive Neural Network (morphoRNN)Limitation of word2vec:• Closed vocabulary assumption• Cannot exploit functional relationships in learning: 如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher To walk closer to open vocabulary, use compositional representations based on morphemes. Instead of word embedding, embed morphemes - the smallest meaningful unit of language. Compute representation recursively from morphemes, word embedding 由 morphemes embedding 拼接而来. 与基础版的morphoRNN结构相同，Context-insensitive Morphological RNN model (cimRNN) 考察 morphoRNN 在不参考任何上下文信息情况下， 仅仅用 morphemic representation 构造词向量的能力。训练时，给每个词xi定义损失函数s(xi)为新构造的词向量pc(xi)和参考词向量pr(xi)之间的欧几里得距离平方 该cimRNN模型没有机会改进可能被估计不足的罕见词的表达. Context-sensitive Morphological RNN (csmRNN) 在学习语素组成时同时参考语境信息，在训练过程中，神经网络顶层的更新将一直反向传播直至底层的语素层。 Compositional character representations在自然语言处理中使用 word 作为基本单位的问题在于词汇量太大了，所以几乎所有主流模型都会省略很多词，比如Bengio的RNNs语言模型就把所有出现频率&lt;3的单词统一标记为一个特殊词。但这样的操作也只是把词汇量降到了16,383。又比如word2vec模型只考虑出现频率最高的30,000个词。 所以寻找其他有限集合的语言单位成为替代选择，比如字母 character（更确切地说是 unicode code points），比如前面提到的 Morphemes，还有其他比如 Character n-grams，Morphological analysis等，这些可以统称为 subwords units。 然后再通过 subwords 来重构 word representation，进而构建整个文本的meaning representation. 构建 word representation 最简单的方法就是把 subwords vectors 相加、平均或者拼接等，但更好的是使用非线性的方法，比如 Bidirectional LSTMs, Convolutional NNs 等。 哪种方式构建 subword representations 比较好？在 word representation 的重构中，涉及了几个变量:1, Subword Unit 2, Composition Function• Linear Vector operation• Bi-LSTMs• Convolutional NNs 3, Language Typology Type example Morphology analysis Fusional (English) “reads” read-s read-3SG.SG Agglutinative (Turkish) “If I read …” oku-r-sa-m read-AOR.COND.1SG Root&amp;Pattern (Arabic) “he wrote” k(a)t(a)b(a) write-PST.3SG.M Reduplication (Indonesian) “children” anak~anak child-PL 除了语言模型外, 其他NLP任务如SQuAd问答数据集上的很多优秀模型，也会加入character embedding. 但目前 Character-level models 并不具有触及实际 morphology 的模型预测能力。 Multi-word language representationsNeural bag-of-words models:· Simply average (or just sum) word vectors,· Can improve effectiveness by putting output through 1+ fully connected layers (DANs)· Recurrent neural networks(LSTM/GRU): cannot capture phrases without prefix context, and empirically, representations capture too much of last words in final vector – focus is LM next word prediction· Convolutional Neural Network: compute vectors for every h-word phrase, often for several values of h. Example: “the country of my birth” computes vectors for: the country, country of, of my, my birth, the country of, country of my, of my birth, the country of my, country of my birth. Not very linguistic, but you get everything! Data-dependent composition:Recursion is natural for describing language, Phrases correspond to semantic units of language. How to map longer phrases into the same vector space?利用复合性原理 principle of compositionality: 在数学、语义学和语言哲学中，复合性原理是指，一个复杂表达式的意义是由其各组成部分的意义以及用以结合它们的规则来决定的。 Recursive neural nets, a tree structure.For Structure Prediction:Inputs: two candidate children’s representationsOutputs:1, The semantic representation if the two nodes are merged.2, Score of how plausible the new node would be. 神经网络语言模型如何构建一个神经网络语言模型?语言模型的目的是输入一串字符, 输出下一个字符的概率分布, 可以使用 fixed-window neural Language Model, 类似于N-Gram, 仅考虑前(n-1)个窗口长度序列, “as the proctor started the clock the students opened their _“ 得到定长的输入序列, 而 Feedforward neural networks 的输入就是要求固定长度的向量. 用前馈神经网络做语言模型的优点（相对于N-Gram）就是没有了稀疏性问题，而且模型的大小也控制在 O(n)（N-Gram是O(exp(n))） 固定长度的前馈神经网络的固有缺陷就是它要求输入和输出都是固定长度的, 仅考虑前的(n-1)长度的序列, 很多时候会丢失NLP中的长距离依赖信息, 跟N-Gram的有一样的缺陷。而且实际的应用中语句的长度是不固定的，最好有一个神经网络可以接受任意长度的输入序列, 输出任意长度的序列。循环神经网络 (Recurrent neural networks, aka RNNs) 就可以解决这个问题. 循环神经网络语言模型不同于前馈神经网络使用输入序列的每一个词单独训练一行(或一列, 取决于矩阵的设计)参数矩阵, RNNs的设计核心是用输入序列的每一个词, 反复地训练同一个参数, 即”共享参数”. 因为参数共享:1, 模型大小不会随着输入序列长度增加而增加。2, 每一步的计算，理论上都使用到了之前的历史信息，所以理论上可以更好的捕捉长距离依赖（但实际上表现并不好，看后面的梯度消失与爆炸）.3, 模型有更好的泛化能力 使用基于Softmax的RNNs语言模型等同于解决矩阵分解问题, 参考Breaking the Softmax Bottleneck: A High-Rank RNN Language Model。 循环神经网络语言模型使用损失函数评估模型表现: 损失函数 loss function on step t is usual 交叉熵 cross-entropy between predicted probability distribution and the true next word. 传统的统计语言模型使用困惑度(perplexity)来评估模型表现，但其实降低困惑度等价于减小损失函数. 神经网络语言模型的学习能力Character models are good at reduplication (no oracle, though), works well on language with reduplication patterns like Indonesian, Malay. Character NLMs learn word boundaries, memorize POS tags. What do NLMs learn about morphology?1, Character-level NLMs work across typologies, but especially well for agglutinative morphology.2, predictive accuracy is not as good as model with explicit knowledge of morphology (or POS).3, They actually learn orthographic similarity of affixes, and forget meaning of root morphemes accordong to qualitative analyses.4, More generally, they appear to memorize frequent subpatterns 总的来说，神经网络处理自然语言的能力并不特殊，表现的性能，跟神经网络本身的长处相匹配，如泛化、模式匹配、端到端应用的能力等。 Dependency parsing语言学里有两种角度看待语法结构 - Constituency and Dependency： Constituency: phrase structure grammar, 从句子成分构造的角度看，capture the configurational patterns of sentences，即把句子的语法理解为词组成分的递归嵌套. 可以用 context-free grammars (CFGs) 来表达语法规则，就是语法树。 Dependency syntax: 主要是从语义的角度来看，显示哪些单词依赖于（一般指修改或作为参数其参数）哪些单词。特别用于区分动词的主格（subject position or with nominative inflection）宾格（object position or with accusative inflection）. Dependencies can be identified even in non-configurational languages. A sentence dependency structure explains the dependency relation between its words: represented as a graph with the words as its nodes, linked by directed, labeled edges, with the following properties:• connected: every node is related to at least one other node, and (through transitivity) to ROOT;• single headed: every node (except ROOT) has exactly one incoming edge (from its head);• acyclic: the graph cannot contain cycles of directed edges. Dependency trees 有两种，如果dependency graph中有edges交叉则是non-projective, 反之则是 projective。更确切的定义是：A dependency tree is projective wrt. a particular linear order of its nodes if, for all edges h → d and nodes w, w occurs between h and d in linear order only if w is dominated by h. A non-projective dependency grammar is not context-free. Motivation for Dependency parsing:• context-free parsing algorithms base their decisions on adjacency;• in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective);• we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present). Evaluation: accuracy (# correct dependencies with or ignore label)). Graph-based dependency parsingBased on maximum spanning trees (MST parser), views syntactic structure as a set of constraints Intuition as tagging problem: since each word has exactly one parent, the possible tags are the other words in the sentence (or a dummy node called root). If we edge factorize the score of a tree so that it is simply the product of its edge scores, then we can simply select the best incoming edge for each word. The tartget function is to find the highest scoring dependency tree in the space of all possible trees for a sentence. The score of dependency tree y for sentence x is:$$s(x,y) = \sum_{(i,j)\in y} s(i,j)$$$x = x_1…x_n, y$ is a set of dependency edges, with $(i, j) ∈ y$ if there is an edge from $x_i$ to $x_j$. Scoring edges with a neural networkThe function g(aj, ai) computes an association score telling us how much word wi prefers word wj as its head. Association scores are a useful way to select from a dynamic group of candidates, 跟注意力机制的similarity score 异曲同工，方程的形式也很相似。 Parsing 算法： start with a totally connected graph G, i.e., assume a directed edge between every pair of words; find the maximum spanning tree (MST) of G, i.e., the directed tree with the highest overall score that includes all nodes of G; this is possible in O(n2) time using the Chu-Liu-Edmonds algorithm; it finds a MST which is not guaranteed to be projective; 1, Each node j in the graph greedily selects the incoming edge with the highest score s(i,j) 2, If result were a tree, it would have to be the maximum spanning tree; If not, there must be a cycle. 3, Break the cycle by replacing a single incoming edge to one of the nodes in the cycle. To choose the node, decide recursively by identifying the cycle and contract it into a single node and recalculate scores of incoming and outgoing edges. Now call CLE recursively on the contracted graph. MST on the contracted graph is equivalent to MST on the original graph. 这里是指先识别出循环体saw ⇄ john②，然后在这个循环体范围内，使用CLE找出 root 进出这个循环体的最大概率路线(root → saw → john = 40) &gt; (root → john → saw = 29)③； 4, Greedily collect incoming edges to all nodes, find out to be a tree and thus the MST of the graph. 把循环体以及其包含的nodes合并为一个node wjs，并且已经有了进出wjs的最大概率路径，这样就可以在整个图上继续运行CLE算法找出最大概率路线(root → wjs → mary = 70) &gt; (root → mary → wjs = 40)④. Chu-Liu-Edmonds (CLE) Algorithm: In graph theory, Edmonds’ algorithm or Chu–Liu/Edmonds’ algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). It is the directed analog of the minimum spanning tree problem Transition-based dependency parsingAn extension of shift-reduce parsing (MALT parser), views syntactic structure as the actions of an automaton:• for a given parse state, the transition system defines a set of actions T which the parser can take;• if more than one action is applicable, a machine learning classifier is used to decide which action to take;• just like in the MST model, this requires a mechanism to compute scores over a set of (possibly dynamic) candidates.if si is the ith top element on stack, and bi the ith element on buffer, then we have the following transitions:• LEFT-ARC(l): adds arc s1 → s2 with label l and removes s2 from stack (|s| ≥ 2);• RIGHT-ARC(l): adds arc s2 → s1 with label l and removes s1 from stack (|s| ≥ 2);• SHIFT: moves b1 from buffer to stack; recondition: |b| ≥ 1.总的来说就是：父节点保留在stack中; 从始至终 root 一直都是父节点；从 buffer 中把候选词一个一个 push 到stack中，根据 classifier 预测的结果，分辨出哪个候选词是子节点，并把子节点 pop 出 stack；直到清空 buffer，stack 中只剩下 root。 Comparing MST and transition-based parsers:Both require dynamic classifiers, and these can be implemented using neural networks, conditioned on bidirectional RNN encodings of the sentence. The MST parser selects the globally optimal tree, given a set of edges with scores;• it can naturally handle projective and non-projective trees; A transition-based parser makes a sequence of local decisions about the best parse action;• it can be extended to projective dependency trees by changing the transition set; Accuracies are similar, but transition-based is faster; Recurrent neural network grammars (RNNGs)Widespread phenomenon: Polarity items can only appear in certain contexts, e.g. “anybody”. In linguistics, a polarity item is a lexical item that can appear only in environments associated with a particular grammatical polarity – affirmative or negative. A polarity item that appears in affirmative (positive) contexts is called a positive polarity item (PPI), and one that appears in negative contexts is a negative polarity item (NPI). The environment in which a polarity item is permitted to appear is called a “licensing context“. The lecture that I gave did not appeal to anybody;The lecture that I gave appealed to anybody. 也许”anybody”出现的条件是前面出现过”not”，那么应该可以使用 RNNs 模型来解码这点信息。然而:The lecture that I did not give appealed to anybody. 这说明 Language is hierarchical: The licensing context depends on recursive structure (syntax)。不能简单根据”not”是否出现来判断，而是需要看”not”修饰的成分，也就是说要考虑语法的合理。这就给文本生成任务（或者说构建语言模型）带来挑战。 Recurrent neural network grammars (Dyer et al. 2016)提出了一种具有明确短语结构的语言模型 RNNGs。 RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. 就是在使用 RNNs 构建语言模型，除了考虑历史词信息, 还会生成历史的语法结构, 并以此为参考预测语法结构和词语,以保证生成的语言符合语法结构。这里的语法是针对 phrase structure (constituency) grammars，所以 RNNGs 也是一种 constituency parsing： Generate symbols sequentially using an RNN Add some “control symbols” to rewrite the history periodically Periodically “compress” a sequence into a single “constituent” Augment RNN with an operation to compress recent history into a single vector (-&gt; “reduce”) RNN predicts next symbol based on the history of compressed elements and non-compressed terminals (“shift” or “generate”) RNN must also predict “control symbols” that decide how big constituents are 首先注意到，如果有序地去遍历语法树，得出的就是一个序列： What information can we use to predict the next action, and how can we encode it with an RNN? Use an RNN for each of: Previous terminal symbols Previous actions Current stack contents最后得出的 stack 就是完整的语法树（以序列的形式）。 Syntactic Composition人们通过较小元素的语义组合来解释较大文本单元的含义 - 实体，描述性词语，事实，论据，故事.When compressing “The hungry cat” into a single composite symbol, use Bi-LSTM to encode (NP The hungry cat). 基于此可以递归地解码更复杂的短语，比如(NP The (ADJP very hungry) cat), 只需要把原来的hungry替换为(ADJP very hungry)即可。 这种递归地堆栈符号的构建行为映射了符号对应的树结构 除此了使用 Bi-LSTM 解码，还可以使用 Attention：Replace composition with one that computes attention over objects in the composed sequence, using embedding of NT for similarity. Implement RNNGsStack RNNs Augment a sequential RNN with a stack pointer Two constant-time operations push - read input, add to top of stack, connect to current location of the stack pointer pop - move stack pointer to its parent A summary of stack contents is obtained by accessing the output of the RNN at location of the stack pointer Training RNNs: Each word is conditioned on history represented by a trio of RNNs backpropagate through these three RNNs, and recursively through the phrase structure S → NP VP. 完整的RNNGs模型，用 softmax 计算下一个 action 的概率分布： Parameter EstimationRNNGs jointly model sequences of words together with a “tree structure”. Any parse tree can be converted to a sequence of actions (depth first traversal) and vice versa (subject to wellformedness constraints). Inference problems of RNNGsAn RNNG is a joint distribution p(x,y) over strings (x) and parse trees (y), i.e. it jointly predicts the word, and the parse context together. So the model will still generate the syntactic information and the next word but we can discard the additional outputs if all we want is the language model. Two inference questions:• What is $p(x)$ for a given x? - language modeling• What is $argmax_yp(y | x)$ for a given x? - parsing The model predicts the next action (NT() GEN() or REDUCE in generative mode, NT() SHIFT or REDUCE in discriminative mode). The set of actions completely determines the string and tree structure, so we can get their joint probability by multiplying over the probabilities of all actions. In discriminative mode, the input is a string of words, and the model cannot generate words, but instead “consumes” the words in the input buffer. The model can be used as a parser (find the maximum prob. tree, i.e., $argmax_yP(y \mid x)$). In generative mode, there is a respective GEN() action for every word, so the word is predicted with the action. To be a language model (find the maximum prob. sentence/assign probabilities to a sentence, i.e., $p(x)$), we must marginalize over trees to get the probability of the sentence. This is intractable so is approximated with importance sampling by sampling from a discriminatively trained model. importance samplingAssume we”ve got a conditional distribution $q(y | x)$s.t. (i) $p(x, y) &gt; 0 \Rightarrow q(y | x) &gt; 0$(ii) $y \sim q(y | x)$ is tractable and(iii) $q(y | x)$ is tractable The importance weights $w(x,y) = \frac{p(x, y)}{q(y | x)}$ 从句子到语法树的seq2seq模型其实从句子到语法的映射类似于一个seq2seq模型。而直接的把语法树以字符序列的形式表达，使用简单的 RNNs 直接构建句子到语法序列的 seq2seq 模型效果也不错，比如：input: The hungry cat meows .output: S( NP( _ _ _ ) VP( _ ) _ )Vanilla RNNs 在模式匹配和计数方面非常出色，经验证明，训练有素的 seq2seq 模型通常会输出格式良好的字符串，见这篇文章 section 3.2 但潜在的问题是，seq2seq 模型并不要求输出是有正确括号字符（数量对齐，位置正确）。另外，理论上单个RNN也只能记忆括号结构一定的有限深度，因为 RNNs 只有固定的有限数量的隐藏单元。例如，它将为这些输出分配非零概率：S( NP( _ _ ) VP ( _ ) _ )S( NP( _ _ _ ) VP ( _ ) _ ) ) ) 理想情况下，模型应该给任何不完整的输出分配零概率。使用 RNNGs 是因为它本身能够履行这些限制， 保证生成完整正确的语法树。 从中可以看出，seq2seq模型可以用于快速原型和 baseline 搭建，但如果遇到要求输出遵守某些约束条件的问题，则需要直接执行这些约束条件。 ParsingParsing is a fundamental task in NLP. But what is parsing actually good for? Parsing breaks up sentences into meaningful parts or finds meaningful relationships, which can then feed into downstream semantic tasks:• semantic role labeling (figure out who did what do whom);• semantic parsing (turn a sentence into a logical form);• word sense disambiguation (figure out what the words in a sentence mean);• compositional semantics (compute the meaning of a sentence based on the meaning of its parts). Semantic role labeling (SRL)虽然可以使用 Distributional semantics 表达含义，只是 Distributional semantics 比较擅长处理相似度，且无法很明确地处理复合性 Compositionality。 在数学、语义学和语言哲学中，复合性原理是指，一个复杂表达式的意义是由其各组成部分的意义以及用以结合它们的规则来决定的。 为了能够处理复合性和推理，我们需要象征性和结构化的意义表示。 虽然语言是无穷无尽的，句子是无限的集合，而人脑的能力却是有限的，但人们总能够理解一个句子的含义（假如人们熟知表达句子的语言）. 因此, 对于 semantics, 语义肯定是有限的集合, 这样才能确定句子的确切意义. In generative grammar, a central principle of formal semantics is that the relation between syntax and semantics is compositional. The principle of compositionality (Fregean Principle): The meaning of a complex expression is determined by the meanings of its parts and the way they are syntactically combined. Semantic role labeling means identifying the arguments (frame elements) that participate in a prototypical situation (frame) and labeling them with their roles; SRL task is typically broken down into a sequence of sub-tasks: parse the training corpus; match frame elements to constituents; extract features from the parse tree; train a probabilistic model on the features. 所谓 frame elements 是针对 Frame Semantics 而言的。 SRL provides a shallow semantic analysis that can benefit various NLP applications; no parsing needed, no handcrafted features. Frame Semantics表达词义，除了 Firth, J.R. (1957) 的 “a word is characterized by the company it keeps”（也即是 Distributional semantics）之外, 还有 Charles J. Fillmore 的 Frame Semantics. The basic idea is that one cannot understand the meaning of a single word without access to all the essential knowledge that relates to that word. A semantic frame is a collection of facts that specify “characteristic features, attributes, and functions of a denotatum, and its characteristic interactions with things necessarily or typically associated with it.” A semantic frame can also be defined as a coherent structure of related concepts that are related such that without knowledge of all of them, one does not have complete knowledge of any one; they are in that sense types of gestalt. Proposition Bank完整的句子表达了命题 propositions, 也即一个主张. 比如”John smokes”这个句子的命题如果是真的,那么”John”在这里一定是某个”smokes”的人, 也就是必须是NP. 在现代哲学、逻辑学、语言学中，命题是指一个判断（陈述）的语义（实际表达的概念），这个概念是可以被定义并观察的现象。命题不是指判断（陈述）本身。当相异判断（陈述）具有相同语义的时候，他们表达相同的命题。例如，雪是白的（汉语）和Snow is white（英语）是相异的判断（陈述），但它们表达的命题是相同的。在同一种语言中，两个相异判断（陈述）也可能表达相同命题。例如，刚才的命题也可以说成冰的小结晶是白的，不过，之所以是相同命题，取决于冰的小结晶可视为雪的有效定义。 PropBank is a version of the Penn Treebank annotated with semantic roles. More coarse-grained than Frame Semantics: End-to-end SRL system基本的结构单元是Bi-LSTM，用法是：· a standard LSTM layer processes the input in forward direction;· the output of this LSTM layer is the input to another LSTM layer, but in reverse direction;这些Bi-LSTM单元可以叠加起来构造更深层的神经网络. The input (processed word by word) features are:• argument and predicate: the argument is the word being processed, the predicate is the word it depends on;• predicate context (ctx-p): the words around the predicate; also used to distinguish multiple instances of the same predicate;• region mark (mr): indicates if the argument is in the predicate context region or not;• if a sequence has np predicates it is processed np times. Output: semantic role label for the predicate/argument pair using IOB tags (inside, outside, beginning). Training:• Word embeddings are used as input, not raw words;• the embeddings for arguments, predicate, and ctx-p, as well as mr are concatenated and used as input for the Bi-LSTM;• the output is passed through a conditional random field (CRF); allows to model dependencies between output labels;• Viterbi decoding is used to compute the best output sequence Model learns “syntax”(Maybe): it associates argument and predicate words using the forget gate: Semantic ParsingSemantic Parsing 指语义分析，把文本解析为任意的逻辑形式(一种 meaning representation)，比如 first-order logic(FOL).Sam likes Casey - likes(Sam, Casey);Anna&#39;s dog Mr. PeanutButter misses her - misses(MrPB, Anna) ∧ dog(MrPB);Kim likes everyone - ∀x.likes(x, Kim).Predicate-argument structure is a good match for FOL, as well as structures with argument-like elements (e.g. NPs).Determiners, quantifiers (e.g. “everyone”, “anyone”), and negation can be expressed in FOL. However, much of natural language is unverifiable, ambiguous, non-canonical. That makes it hard to represent the wide-coverage meaning of arbitrary NL. Closed domains are easier, and can sometimes be harvested automatically, e.g. GEOQUERY dataset. This leads to a proliferation of domain-specific MRs.· Pairs of NL sentences with structured MR can be collected, e.g. IFTTT dataset (Quirk et al. 2015).· WikiTableQuestions· Google’s knowledge graph Viewing MR as a string, semantic parsing is just conditional language modeling. Trainable alternative to compositional approaches: encoder-decoder neural models. The encoder and decoder can be mixed and matched: RNN, top-down tree RNN. Works well on small, closed domains if we have training data, but there are many unsolved phenomena/ problems in semantics. Abstract meaning representation (AMR)• The edges (ARG0 and ARG1) are relations• Each node in the graph has a variable• They are labeled with concepts• d / dog means “d is an instance of dog“The dog is eating a bone(e / eat-01&nbsp;&nbsp;&nbsp;&nbsp;:ARG0 (d / dog)&nbsp;&nbsp;&nbsp;&nbsp;:ARG1 (b / bone)) The dog wants to eat the bone(want-01&nbsp;&nbsp;&nbsp;&nbsp;:ARG0 (d / dog)&nbsp;&nbsp;&nbsp;&nbsp;:ARG1 (e / eat-01&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:ARG0 d&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:ARG1 (b / bone))) CoreferenceCharles just graduated, and now Bob wants Anna to give him a job.Q: who does him refer to? MetonymyWestminster decided to distribute funds throughout England, Wales, Northern Island, and Scotlanddecided(Parliament, …) ImplicatureThat cake looks delicious - I would like a piece of that cake. Even more phenomena…• Abbreviations (e.g. National Health Service=NHS)• Nicknames (JLaw=Jennifer Lawrence)• Metaphor (crime is a virus infecting the city)• Time expressions and change of state• Many others TODO(指代消解 Coreference Resolution)Unsupervised Part-of-Speech TaggingParts-of-speech(POS), word classes, or syntactic categories, 一般指八个词性：noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, 有时候是 numeral, article or determiner.1, noun 名詞 ( n. )2, pronoun 代名詞 ( pron. )3, verb 動詞 ( v. )4, adjective 形容詞 ( adj. )5, adverb 副詞 ( adv. )6, preposition 介系詞 ( prep. )7, conjunction 連接詞 ( conj. )8, interjection 感歎詞 ( int. ) Tagging is a task that take a sentence, assign each word a label indicating its syntactic category (part of speech). One common standard label is Penn Treebank PoS tagset. DT - Determiner 定语IN - Preposition or subord. conjunctionNN - Noun, singular or massNNS - Noun, pluralNNP - Proper noun, singularRB - AdverbTO - toVB - Verb, base formVBZ - Verb, 3rd person singular present In supervised POS tagging, the input is the text and a set of allowed POS labels. The training data contains input and output examples. The output is a guess, for each word in the test data, which POS label it should have. A common approach is to use an HMM. To train it, choose parameters θ that maximize $P(x,y \mid θ)$, the probability of the training data given the parameters. This is maximum likelihood estimation and it was covered in ANLP. You can use the model to predict y for each x in the test data by solving $P(y \mid x,θ)$ using the Viterbi algorithm. A consequence of supervised training with MLE is that the model will only learn non-zero probability for tag-word pairs that actually appear in the data. Hence, if “the” is only ever tagged with DT in the training data, then the model will learn that the probability of producing “the” from any other tag is zero. This means that many word tokens will be (empirically) unambiguous, which is one of the things that makes supervised POS tagging easy. RNNs 虽然也可以处理序列模型, 但是神经网络需要目标函数, 没有目标无法计算损失, 就无法调整参数, 也就是”监督学习”. Current PoS taggers are highly accurate (97% accuracy on Penn Treebank). But they require manually labelled training data, which for many major language is not available. Hence motivated for unsupervised PoS tagging. In unsupervised POS tagging, the input is the text and the number of clusters. The training data contains only input examples. The output is a guess, for each word in the text, which cluster the word belongs to. For example:123Number of clusters: 50Input x: The hungry cat meowsOutput y: 23 45 7 18 What we hope is that the cluster labels will correlate with true POS labels; that is, that tokens labeled 23 will tend to be determiners, that clusters label 45 will tend to be adjectives, and so on. 这个时候可以使用隐马尔科夫模型, 这个”隐”就是针对没有目标可以参考这种情况. Hidden Markov ModelsThe unsupervised tagging models here are based on Hidden Markov Models (HMMs).To train it, choose parameters θ that maximize $P(x \mid θ)$, the probability of the training data given the parameters. The parameters θ = (τ, ω) define:• τ : the probability distribution over tag-tag transitions;• ω: the probability distribution over word-tag outputs.The parameters are sets of multinomial distributions:• $ω = ω^{(1)} . . . ω^{(T)}$: the output distributions for each tag;• $τ = τ^{(1)} . . . τ^{(T)}$: the transition distributions for each tag;• $ω^{(t)} = ω_1^{(t)}. . . ω_W^{(t)}$: the output distribution from tag $t$;• $τ^{(t)} = τ_1^{(t)}. . . τ_T^{(t)}$: the transition distribution from tag $t$. Another way to write the model, often used in statistics and machine learning: $w_i | t_i = t ∼ Multinomial(ω^{(t)})$ So as tag, given that $t_{i−1} = t$, the value of $t_i$ is drawn from a multinomial distribution with parameters $τ^{(t)}$. How to estimate ω and τ without supervision. This is still maximum likelihood estimation, but notice that it’s more difficult because the tags y are unobserved, so you must marginalize them out. For estimation (i.e., training the model, determining its parameters), we need a procedure to set θ based on data. Rely on Bayes Rule:\begin{equation}\begin{split} P(θ|w)&amp;=\frac{P(w|θ)P(θ)}{P(w)}\\ &amp;∝P(w|θ)P(θ)\\\end{split}\end{equation}Choose the θ that maximize the likelihood $P(w|θ)$. Basically, we ignore the prior. In most cases, this is equivalent to assuming a uniform prior. To do this, you can use expectation maximization (EM), a variant of MLE that can cope with unobserved data, which was also covered in ANLP. For examples, forward-backward algorithm for HMMs, inside-outside algorithm for PCFGs, k-means clustering. For inference (i.e., decoding, applying the model at test time), we need to know θ and then we can compute $P(t, w)$: E-step: use current estimate of θ to compute expected counts of hidden events ($n(t,t^{\prime})$, $n(t,w)$).M-step: recompute θ using expected counts. You can then use the trained model to predict y for each x in the test data by solving $P(y \mid x,θ)$ using the Viterbi algorithm. But EM often fails, even very small amounts of training data have been show to work better than EM. One consequence of unsupervised training with EM is that every word can be assigned to any cluster label. This makes things really difficult, because it means every word is ambiguous. The basic assumptions of EM (that any tag-word or tag-tag distribution is equally likely) make this even more difficult. Instead, use Bayesian HMM with Gibbs sampling. Bayesian HMMWhen training HMM model, we are not actually interested in the value of θ, we could simply integrate it out. This approach is called Bayesian integration. Integrating over θ gives us an average over all possible parameters values. The Bayesian HMM is simply an alternative way to solve the unsupervised POS tagging problem. The input and output is the same. But instead of learning θ, we directly solve $P(y \mid x)$. Note that we don’t need to learn θ (though we could) - in this setting, we integrate it out, after first supplying some information about the tag-tag and word-tag distributions encoded in θ. Specifically, we tell the model that a sparse distribution is much more likely than a uniform distribution. We do this by defining a distribution $P(θ)$, and this gives us a new model, $P(y,x \mid θ)×P(θ)$. By integrating out θ we can solve the unsupervised tagging problem directly. Example: we want to predict a spinner result will be “a” or not?• Parameter θ indicates spinner result: $P(θ = a) = .45$, $P(θ = b) = .35$, $P(θ = c) = .2$;• define t = 1: result is “a”, t = 0: result is not “a”;• make a prediction about one random variable (t) based on the value of another random variable (θ). Maximum likelihood approach: choose most probable θ, $\hat{θ} = a$, and $P(t = 1|\hat{θ}) = 1$, so we predict $t = 1$. Bayesian approach:average over θ,$P(t = 1) = \sum_θ P(t = 1|θ)P(θ) = 1(.45) + 0(.35) + 0(0.2) = .45$, predict t = 0. Advantages of Bayesian integration:• accounts for uncertainty as to the exact value of θ;• models the shape of the distribution over θ;• increases robustness: there may be a range of good values of θ;• we can use priors favoring sparse solutions (more on this later). Dirichlet distributionChoosing the right prior can make integration easier. A $K$-dimensional Dirichlet with parameters $α = α_1 . . . α_K$ is defined as: $$ P(θ) = \frac{1}{Z} \prod_{j=1}^K θ_j^{α_j−1} $$ We usually only use symmetric Dirichlets, where $α_1 . . . α_K$ are all equal to β. We write Dirichlet(β) to mean $Dirichlet(β, . . . , β)$. 注意到这是一个二维的概率密度图. $β&gt;1$意味着更喜欢均值分布, 此时$θ$大概率落在$0.5$附近,因为$θ_1+θ_2=1$, 所以此时$θ_1, θ_2$概率均等. 如果$β=1$, $θ_1$的任何取值是等概率的, 等于说任何$θ_1,θ_2$的组合概率都是均等的. To Bayesianize the HMM, we augment with it with symmetric Dirichlet priors: To simplify things, use a bigram version of the Bayesian HMM; If we integrate out the parameters θ = (τ, ω), we get: Use these distributions to find $P(t|w)$ using an estimation method called Gibbs sampling. Results: Integrating over parameters is useful in itself, even with uninformative priors $(α = β = 1)$; 总结：· Bayesian HMM improves performance by averaging out uncertainty;· allows us to use priors that favor sparse solutions as they occur in language data.· Using a tag dictionary is also really helpful. We still have no labeled training data, but if we only allow each word to be tagged with one of the labels that appears in the dictionary, then most word-tag pairs will have probability zero. So this is a very different way of supplying information to the unsupervised model that is very effective. Bias in NLPThe social impact of NLPOutcome of an NLP experiment can have a direct effect on people’s lives, e.g. 频繁出现亚马逊 Alexa 突然发出诡异笑声，给多名用户造成困惑和恐慌, 因为人们谈话中偶然包含 trigger 词：”Alexa, laugh” 而发出 - 亚马逊的解决方案是把 trigger 改为更难触发的 “Alexa, can you laugh” Chatbot 对于人们敏感问题的不恰当回答, 比如 “Should I kill myself?” - “Yes.”，这些回答对患有心理障碍的人群或者青少年儿童带来非常大的危害。 Microsoft 的 AI chatbot 上线仅一天, 就通过 twitter 和人交谈并学会涉及种族, 性别歧视等的话语, 典型的 “garbage in, garbage out” 现象. 其他涉及数据隐私等问题 语言的特性，导致NLP涉及的社会伦理问题非常多, 而且影响非常大：· 语言传递着信息、偏见，是政治性的、权力的工具, 同时比其他技术带有更明显的拟人化、人格化倾向，这可能给个人生活带来不便或危害，给整个社会带来舆论影响。· Any dataset carries demographic bias: latent information about the demographics of the people that produced it. That excludes people from other demographics. 同时人类本身的认知容易加深偏见:The availability heuristic: the more knowledge people have about a specific topic, the more important they think it must be. Topic overexposure creates biases that can lead to discrimination and reinforcement of existing biases. E.g. NLP focused on English may be self-reinforcing. NLP 实验本身容易加深偏见：• Advanced grammar analysis can improve search and educational NLP, but also reinforce prescriptive linguistic norms.• Stylometric analysis can help discover provenance of historical documents, but also unmask anonymous political dissenters. NLP 技术可能被不恰当地使用：• Text classification and IR can help identify information of interest, but also aid censors.• NLP can be used to discriminate fake reviews and news, and also to generate them. Word embeddings contain human-like biasesword2vec learns semantic/ syntactic relationships, also keep company with unsavoury stereotypes and biases?• Man:Woman - King:Queen• Man:Doctor - Woman:Nurse• Man:Computer Programmer - Woman:Homemaker Measure bias using implicit association tests:1, Compute similarity of group1 and stereotype1 word embeddings. Cosine similarity is use to measure association (in place of reaction time).2, Compute similarity of group1 and stereotype 2 word embeddings.3, Null hypothesis: if group1 is not more strongly associated to one of the stereotypes, there will be no difference in the means.4, Effect size measured using Cohen’s d.5, Repeat for group 2. Experiments• Uses GloVe trained on Common Crawl—a large-scale crawl of the web.• Removed low frequency names.• Removed names that were least “name-like” (e.g. Will) algorithmically.• Each concept is represented using a small set of words, designed for previous experiments in the psychology literature. Result:· flowers associate with pleasant, insects associate with unpleasant. $p &lt; 10^{−7}$· Men’s names associate with career, women’s names associate with family. $p &lt; 10^{−3}$· European American names associate with pleasant, African American names associate with unpleasant. $p &lt; 10^{−8}$ 这些结果的确真实地反映人类社会的现状。但大部分性别方面的偏见其实是反映了目前的社会分工，无所谓高低贵贱；人种的偏见倒是反映了历史问题对现在的影响，这种偏见是不符合道德的。人对于其他生物的偏见，虽然是没必要的，但人类的确倾向于喜爱行为”可爱”，外形”美好”的生物，比如大熊猫就是比鳄鱼受欢迎。 偏见的存在不一定合理。哪些偏见是不合理的，才是人们更应该去思考和讨论的地方。 Debiasing word embeddingsBolukbasi. et. al., 2016. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings提供了一个思路: 确认偏见的方向 中和抵消偏见: 对于非定性的词（如”医生”），通过投射来消除偏见 等价：让father - mother和boy - girl等距，让定性词间的距离只有性别的距离；或者让doctor - woman和doctor - man等距，消除非定性词的性别偏见。 什么词需要抵消偏见: 训练一个线性分类器来确定词是非定性还是非定性的, 结果当然是大部分英语词都是非定性的. If analogies reveal a gender dimension, use analogies on specific seed pairs to find it.y 轴下面的词属于定性词, 不需要中性化, 而y轴之上的词则需要进行中性化处理. 不同的偏见, 需要不同的 seed words; 一种偏见, 可以有多种 seed words 选择: 除了用”She-He”作为性别偏见的基准, 还有其他选择. 编码器—解码器 Sequence-to-sequence 和注意力机制当输入输出都是不定长序列时, 比如机器翻译这种任务，需要使用 Sequence-to-sequence（seq2seq）或者 encoder-decoder 神经网络结构。这种结构可以通过一种方法叫注意力机制来显著提高性能。 编码器—解码器 Sequence-to-sequence（seq2seq）编码器：所谓编码，就是把不定长的输入序列输入RNN，以得出某种定长的编码信息。解码器：所谓解码，就是把编码器编码后的信息（一般取编码器的RNN最终时刻的隐含层变量）输入到解码器的RNN中，每个t时刻的输出既取决于之前时刻（t-1）的输出又取决于编码信息。等同于一个以解码信息作为条件概率生成目标语言句子的语言模型。 所以 seq2seq 本质是一个条件概率语言模型：语言模型是指解码器每次会预测下一个出现的单词，条件概率是指预测是基于编码后的源句子。 注意力在传统的seq2seq模型中，解码器各个时刻都使用相同的编码信息，这就要求解码器把源输入序列的所有信息都解码并整合到最后时刻的隐含状态中，这个是很大的信息瓶颈。而人们知道，在实际任务中，比如机器翻译，目标句子的不同单词，一般只对应源句子的某一部分而已。如果能够让解码器在解码时，在不同时刻专注于源输入序列的不同部分，那么就可以突破这个瓶颈。 对于解码器的每一时间步的隐含状态st，可以衡量其与编码器的所有时间步隐含状态h0……et的相似性(或score评分) e = α(s, h)，简单的评分方式是元素间相乘, e = s*h（Bahanau的论文提供了更复杂的形式), 也可以参考论文Effective Approaches to Attention-based Neural Machine Translation探讨的集中评分方式, 这篇论文提供了一种 Bilinear 形式的相似性评分法, 就是在s和h之间以点乘的形式插入一个交互矩阵 interaction matrix. 对得出的评分求加权平均a = softmax(e), 得出的权值分布也称注意力权重 通过注意力权重把编码器隐含状态加权求和，得到注意力输出 A = Σah 最后把注意力输出和对应时间步的解码器隐含状态st拼接在一起 [A;st]，作为解码器rnn的隐含层.]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理快速入门 | 02 N-Gram 语言模型 - ANLP UoE 爱丁堡]]></title>
    <url>%2FNOTE-ANLP-02-n-gram-model%2F</url>
    <content type="text"><![CDATA[生成模型在语言模型中的应用包括 N-Gram语言模型，朴素贝叶斯分类器，隐马尔可夫模型。 在概率统计理论中, 生成模型是指能够生成观测数据的模型，尤其是在给定某些隐含参数的条件下。它给观测值和标注数据序列指定一个联合概率分布。 生成模型的定义与判别模型相对应：生成模型是所有变量的全概率模型，而判别模型是在给定观测变量值前提下目标变量条件概率模型。因此生成模型能够用于模拟（即生成）模型中任意变量的分布情况，而判别模型只能根据观测变量得到目标变量的采样。判别模型不对观测变量的分布建模，因此它不能够表达观测变量与目标变量之间更复杂的关系。因此，生成模型更适用于无监督的任务，如分类和聚类。 N-Gram 语言模型如何训练一个语言模型? 在神经网络大热之前, 人们普遍使用N-Gram语言模型。就是收集不同n-gram频率的统计数据，并用它们预测下一个单词，概率模型基于前面提到的马尔可夫简化假设：若使用N-Gram 来预测下一个单词，出现概率仅取决于前面的(N-1)个单词. 一个N-Gram就是n个连续的单词 Unigram: “the”, “students”, “opened”, ”their” Bigram: “the students”, “students opened”, “opened their” trigrams: “the students opened”, “students opened their” 4-grams: “the students opened their” 通过计数来估计统计概率：P(wi| prefixes) = count(prefixes, wi)/count(prefixes) = count(“the students”)/count(“the students opened”) 在实际中，使用 log 转换来避免数值下溢，并且log 转换可以把乘法转换为加法, 计算更快. N-Gram模型的缺点很明显： 无法很好地解决NLP中的长距离依赖现象 N-gram只是在测试语料库与训练语料库比较相似时表现才比较好。 稀疏问题1：大多数高阶N-Gram几乎不会出现，我们不能简单地把这些定义为0概率的，因为语言是千变万化的，有些词组虽然少见但不代表不存在 稀疏问题2：少部分低阶n-gram在测试集中出现了但是在训练集中没有。比如需要预测 “students opened their _”, 但是训练集中没出现过“students opened their”。 一般而言，N越高，模型表现越好，但是更大的N使稀疏问题变得更糟。通常人们不会取大于5的N。 需要存储所有可能的N-Gram，所以模型的大小是 O(exp(n)),需要大量的内存. 针对数据稀疏问题, 可以使用各种平滑处理. Add alpha smoothing Assign equal probability to all unseen events. Applied in text classification, or domains where zeros probability is not common. Backoff smoothing Use information from lower order N-grams (shorter histories) Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram. Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. 对于像网络数据这种非常大的N-gram，使用stupid backoff. Interpolation smoothing Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts Simple interpolation: P(w3|w1,w2)=1P(w3|w1,w2)+λ2P(w3|w2)+λ3P(w3), Σλ=1. λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data Kneser-Ney smoothing这是目前表现最好的平滑方案. Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams. Use a better estimate for probabilities of lower-order unigrams, the continuation probability, P_continuatin(w) is how likely is w to appear as a novel continutaion. For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w. Every bigram type was a novel continuation the first time it was seen. normalized by the total number of word bigram types. To lower the probability of some fix bigram like “San Franscio” For bigram, Pkn(wi|wi-1)=max(count(wi-1,wi)-d, 0)/c(wi-1) +λ(wi-1)P_continuatin(wi), λ(wi-1) = d{w:count(wi-1,w)&gt;0}/c(wi-1), where {w:count(wi-1,w)&gt;0} is the number of word types that can follow wi-1, also is the # of word types we discounted, also is the # of times we applied normalized discount. For general N-gram,]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理快速入门 | 01 概率模型 - 语言模型 - ANLP UoE 爱丁堡]]></title>
    <url>%2FNOTE-ANLP-01-probability-model%2F</url>
    <content type="text"><![CDATA[爱丁堡大学的自然语言处理入门课程笔记。 References:Accelerated natural language processingANLP revision guideLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning 概率模型 Probability model概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率 估算概率 Probability estimation相关频率/最大似然估计Relative frequency / maximum likelihood estimation p(X) = Count(x)/N 平滑处理 Smoothing一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。 加一（Laplace）平滑最简单的平滑法，为所有事件（不管有没出现过）的频次加一，这样保证了没有0概率事件出现。这种平滑效果很差，因为齐夫定律Zipf&#39;s law的关系: 在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。 会有很多长尾单词很少甚至几乎没有出现过, 所以在总数为1的概率池子了, 为了给这些长尾单词分配至少频次1的概率, 需要从真正出现的单词(所谓真实发生的事件)中分走很多概率. 更多高级的平滑方案参考N-gram部分。 语言模型 Language modeling语言模型: 一种用于计算连续的单词（就是句子）或者任何其他序列数据（比如语音）出现的概率的模型，最基本的应用是基于某种语言模型，预测下一个单词出现的概率 P(w|w1, w2, w3...)。 语言模型本身即是一种概率模型(或者说人们选择用概率模型来描述为语言建模). 因为近现代的自然语言处理主要集中在信息沟通传输方面(比如密码,语音识别, 机器翻译, 校正等), 而香农的信息传输模型使用条件概率来描述鉴定噪音中的真实信息. 如何表达: 一个句子发生的概率就是里面各个单词的概率的乘积。依赖于概率的链式法则, 一个位置的单词的概率，条件于该位置之前的句子部分的概率。所以通过链式法则得出的语言模型概率是冗长的条件概率乘积。 但我们可以通过马尔可夫性质 Markov property 把语言模型简化，一个位置单词出现的条件概率可以通过某种近似来逼近: 仅考虑离它最近的（若干）单词，得到P(the| water is so transparent that) ≈ P(the| that). Evaluation：一般通过困惑度（Perplexity）来衡量语言模型的好坏。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java | Hash @Override equals() hashcode()]]></title>
    <url>%2FJava-hashcode-equals%2F</url>
    <content type="text"><![CDATA[主要介绍： Hashcode（哈希码）与 equals（判断相等）的关系 Hashcode 方法的底层实现原理 开发中需要掌握的原则和方法 HashSet, HashMap, HashTableHashSet底层是调用HashMap. HashMap 使用hashCode和equals来进行对象比较。拿HashSet和add()举例(其余的数据结构,和 remove, contains等方法类似):假设HashSet里面已经有了obj1, 那么当调用HashSet.add(obj2)时: if (obj1 == obj2), 那么没有必要调用 hashCode(), 已经有了这个对象, 没必要添加了 else, if hashCode 不同，那么可以直接添加了, 没必要进一步调用 obj1.equals(obj2) 来判断对象是否相等 else hashCode 相同，那么需要进一步调用obj1.equals(obj2) 下面这段代码虽然 HashSet 只存了 a 对象，但当检查是否包含 b 对象时，返回true。12345HashSet&lt;String&gt; wordSet = new HashSet&lt;String&gt;();String a = "hello";String b = "hello";wordSet.add(a);return wordSet.contains(b); // return true 根据Javadoc for Set. adds the specified element e to this set if the set contains no element e2 such that (e==null ? e2==null : e.equals(e2)). 根据Javadoc for String.equals Compares this string to the specified object. The result is true if and only if the argument is not null and is a String object that represents the same sequence of characters as this object. Java的set是使用它包含的元素（对象）的 equals()来比较 b 和 a 的。这里 String 类的equals()method 是比较字符串值是否相等(准确的说，是先检查是不是引用同一个对象，再看是不是同一个类，再比较值)，而不是引用的对象是否一样，故b.equals(a)是 true。 同样的，remove 和 add 也会先进行类似检查。 问题是，为何 hashCode 不同，就没有进一步调用equals()的必要呢？因为有一个前提是代码遵守The hashCode contract。 Hashcode and equals在Java中，每个对象都有一个hashCode，它有时容易被人遗忘或误用。有以下三点需要注意，避免掉入常见的陷阱。 The hashCode contract根据 The hashCode contract: Objects that are equal must have the same hash code within a running process. 除了字面意思，也有其他隐含的意思: 不相等的对象的hashcode也可能一样; 具有相同 hash code 的对象不一定相等. You must override hashCode() in every class that overrides equals(). Failure to do so will result in a violation of the general contract for Object.hashCode(), which will prevent your class from functioning properly in conjunction with all hash-based collections, including HashMap, HashSet, and Hashtable. — Effective Java, by Joshua Bloch 根据这个contract，可以延伸出以下实践原则： 一、 每当你 override equals 时，也要 override hashCode假如你需要使用不一样的equals判断标准，那么就需要重写equals。但假如仅仅重写equals，而不重写hashcode()，就可能会违背 The hashCode contract。 为什么？因为 hashCode method 需要同时适配真正使用到的 equals method 的判断标准。通过重写equals，我们重新声明了一种判断对象是否相等的标准，但原始的 hashCode method还是会将所有对象视为不同的对象。所以如果没有不重写hashcode，那么根据@Override equals 判断为相同的对象将拥有不同的hashcode（可能）。这样，即使已经有了这个object，在HashMap上调用 contains() 也会返回false。 例子：在Java的创建街道street这个类，在判断两条街道是否相同时，我们有自定义的规则 - 只要是在同一个城市，有同样的街道名，那么两个street就相等，即使他们是存放在不同内存位置的两个对象（Java 的 Object 原生的equals是根据引用的对象内存地址来比较判断的）。1234567891011121314151617181920212223242526272829public class Street &#123; private String name; private String city; // ... @Override public boolean equals(Object obj) &#123; if (!(obj instanceof Street)) return false; if (obj == this) return true; Street rhs = (Street) obj; return new EqualsBuilder(). // if deriving: appendSuper(super.equals(obj)). append(name, rhs.name). append(age, rhs.city). isEquals(); &#125; @Override public int hashCode() &#123; return new HashCodeBuilder(17, 31). // two randomly chosen prime numbers // if deriving: appendSuper(super.hashCode()). append(name). append(city). toHashCode(); &#125;&#125; 如果没有重写hashCode()， 那么两个名字和所在城市一样的，但引用不同地址的street就会按照默认的 hashcode() 返回不一样的code，但是根据重写的equals(), 他们是一样的, 这样就违背了 hashCode contract。 为了安全起见，让Eclipse IDE 生成 equals 和 hashCode 函数：Source &gt; Generate hashCode() and equals()... 为了提醒自己, 还可以配置Eclipse以检测是否有违反此规则的情况，并为仅重写了equals但没重写hashCode的情况显示错误：Preferences &gt; Java &gt; Compiler &gt; Errors/Warnings, then use the quick filter to search for “hashcode” HashCode collisionsHashCode collisions 指两个不同的对象具有相同的hashcode这种情况, 这不是什么严重的问题. 只是会导致更多的搜索步骤，太多collisions就可能会降低系统性能 但是，如果将HashCode错误地用作对象的唯一句柄，例如将其用作Map中的key，那么有时会得到错误的对象。虽然collisions一般很少见，但却是不可避免的。例如，字符串“Aa”和“BB”产生相同的hashCode：2112. 因此衍生出第二个原则 二、永远不要把hashcode当做key来使用 Java中有4,294,967,296个（232)可能的int值）。既然拥有40亿个插槽，collisions似乎几乎不可能对吧？ 但事实上，也不是那么不可能。试想，一个房间里有23名随机人员。你如何估计里面有两个人生日一样的概率？很低？因为一年有365天？事实上，概率约为50％！这种现象被称为生日问题(悖论)。 如果一个房间里有23个或23个以上的人，那么至少有两个人的生日相同的概率要大于50%。 问题的本质是”23人之中两两之间存在生日相同的概率””,而不是”其他22人与其中一个人的生日相同的概率”. 类比到hashcode里，这意味着有77,163个不同的对象，collisions概率是50%（假设有一个理想的hashCode函数，将对象均匀分布在所有可用的buckets中）。 HashCodes 会变HashCode 不保证在不同的执行过程中总能返回相同的code。根据JavaDoc：Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application. 这种情况并不常见，实际上，库中的某些类甚至指定了用于计算hashcode的精确公式（例如String）。对于这些类，hashcode总是相同的。但是，尽管大多数的hashCode方法提供了稳定的值，但我们不能依赖它。正如这篇文章所指出的那样，Java库实际上在不同的进程中返回不同的hashCode值，这往往会让人们感到困惑。 Google的Protocol Buffers就是一个例子。 因此，您不应该在分布式应用程序中使用hash code。即使两者相等，远程对象的 hash code 也可能与本地的不同。 三、不要在分布式应用程序中使用 hashCode此外，要意识到，hashCode函数的实现可能会随着版本的更改而改变。因此我们的代码最好不依赖任何特定的hash code 值。例如，你不应该使用hash code来保持某种状态，不然下次运行时，“相同”对象的hash code可能会不同。 所以最好的建议可能是：除非自己创建了基于 hashcode 算法，否则根本就不要使用 hashCode 呵呵…… 总结在依赖于 HashSet, HashMap, HashTable … 等数据结构的程序中： 仅重写 equals()，会导致业务出错 仅重写 hashcode(), 在比较两个对象时不会强制Java忽略内存地址 如果不涉及对象比较(比如仅仅是iteration), 那么不需要hashCode and/or equals 参考：https://eclipsesource.com/blogs/2012/09/04/the-3-things-you-should-know-about-hashcode/https://stackoverflow.com/questions/27581/what-issues-should-be-considered-when-overriding-equals-and-hashcode-in-java]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 13 实现继承 Implementation Inheritance - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-13-Implementation-Inheritance%2F</url>
    <content type="text"><![CDATA[Implementation Inheritance除了signature之外，Java也允许subclass继承具体的实现方法。实现继承是subclass从superclass继承实现的关系。 缺省方法从 Java 8开始支持 Default method。 我们可以在List中列出已实现的method。这些方法就是 default method，定义了List hypernyms的一些默认行为：default public void method() { ... }. 我们可以自由调用interface中定义的方法，而不用操心具体的实现。Default method 适用于实现接口的任何类型的对象！子类可以直接调用，而不必重新实现 default method。1234// Listdefault public void print() &#123; ...&#125; 不过，我们仍然可以override default method，在子类中重新定义该方法。这样，只要我们在LinkedLList上调用print()，它就会调用子类override的方案，而不是父类的。12345// LinkedList@Overridepublic void print() &#123; ...&#125; Dynamic typeJava是通过一个叫“dynamic method selection”的特性，来确定要调用 default method 还是已经被子类override的method。 当实例声明List&lt;String&gt; l = new LinkedList&lt;String&gt;();, 则指明l是 static 类型的 List。由 new 生成的 object 是LinkedList类型，也从属于 List 类型。但是，因为这个对象本身是使用 LinkedList 构造函数实例化的，所以我们称之为 dynamic type。 Dynamic type 的名称源于: 当l被重新分配指向另一种类型的对象时，比如说一个 ArrayList 对象，l的动态类型现在就变为 ArrayList. 因为它根据当前引用的对象的类型而改变, 所以是动态的。 Static vs. Dynamic Type: Java 每个变量都有一个static type （compile-time type），这是变量声明时指定的类型，在编译时会检查。 每个变量也有一个 Dynamic Type（run-time type），此类型在变量实例化（new）时指定，并在运行时检查。等同于地址指向的对象的类型。 当Java运行一个被overriden的方法时，会根据该实例的dynamic type 匹配对应的 method。 注意，如果是overload:123456public static void peek(List&lt;String&gt; list) &#123; ...&#125;public static void peek(LinkedList&lt;String&gt; list) &#123; ...&#125; 对于上面的实例化的l, 当Java检查要调用哪个方法时，它会检查 static type (此时是List)并使用相同类型的参数调用该方法，也就是使用List作为签名的那个方法。 区别 Interface Inheritance 与 Implementation InheritanceInterface Inheritance 接口继承（what）：指定 subclass 应该实现的功能，即只提供 method signature。 Implementation Inheritance 实现继承（how）：提供功能的实现方案，即提供 method implementation。允许代码再利用，也给subclass设计者提供了更多的自由度，由他们自行决定是否override default method。 Implementation inheritance 也有一些缺点： 人会犯错。我们有可能忘了自己曾经override过一个方法。 如果两个接口给出冲突的 default method，则可能很难解决冲突。 无形中鼓励代码复杂化。 Breaks encapsulation! 最后，注意从属和拥有的区别：subclass 和 superclass 是上下级从属分类，而不是拥有与被拥有的关系，不要跟 nested class 混淆。 Interface Methods: Default methods and abstract methods in interfaces are inherited like instance methods. However, when the supertypes of a class or interface provide multiple default methods with the same signature, the Java compiler follows inheritance rules to resolve the name conflict.– https://docs.oracle.com/javase/tutorial/java/IandI/index.html]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 12 接口继承 Interface Inheritance - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-12-Interface-Inheritance%2F</url>
    <content type="text"><![CDATA[我们前面创建的 LinkedList and ArrayList 其实很相似 - 所有的method都一样. 如果我们需要写一个需要用到 list 的类比如WordUtils class, 我们如何让它既可以使用LinkedList又可以用ArrayList？简单的方法及时写两个同名不同参数的methods。即所谓method overloading。public static String longest(LinkedList&lt;String&gt; list)public static String longest(ArrayList&lt;String&gt; list) 但 overload 有几个缺点: 重复冗余，写两个几乎相同的代码块。 产生更多需要维护的代码，那意味着如果你想对的方法做一个小优化或debug，你需要在对应每种list的方法中改变它。 如果我们想要适配更多的列表类型，不得不复制每个新列表类的方法。 为避免以上问题，我们一般希望能尽量把两个功能近似的方法合并，但要保证其足够广泛的适用场景。此时可以使用接口 interface。 上位词，下义词和接口继承Hypernyms, Hyponyms, and Interface Inheritance 首先要理解，上位词和下位词是语言学的定义，直接沿用到编程语言中。就像狗是哈士奇的上位词，哈士奇是狗的下义词，在Java把这种关系形式化：如果LinkedList是List的Hyponyms，那么LinkedList类是List的subclass，而List类是LinkedList类的superclass(超类/父类)。 在Java中，为了表达这种层次结构，我们需要： 定义通用列表 List interface。 把LinkedList和ArrayList指定为 List 的 hyponyms。 1234public interface List&lt;Item&gt; &#123; public void addFirst(Item x); ...&#125; 这里的 List 是Java中的 interface 接口。本质上是一个指定list必须能够做什么的合约，具体如何做并不是它关心的。123456public class ArrayList&lt;Item&gt; implements List&lt;Item&gt;&#123; // 具体的执行 public void addFirst(Item x) &#123; insert(x, 0); &#125;&#125; 指定ArrayList是List的hyponyms. implements List&lt;Item&gt;类似一种承诺 - 保证具备 List interface 中制定的所有属性（变量）和行为（方法），并提供具体实现方案。 这样就可以同时适配多种list：123456789101112131415public class WordUtils &#123; /** Returns the length of the longest word. */ public static String longest(List&lt;String&gt; list) &#123; ... return list.get(maxDex); &#125; public static void main(String[] args) &#123; ArrayList&lt;String&gt; someList = new ArrayList&lt;&gt;(); //or LinkedList&lt;String&gt; someList = new LinkedList&lt;&gt;(); ... System.out.println(longest(someList)); &#125;&#125; OverridingSubclass 可以覆盖 override superclass的方法。 区分 Override 与 重载 overloaded：Override 的方法 signature 相同；overloaded的方法同名但不同signature。 在子类中实现合约指定的功能时，需要在method的signature顶部包含@Override标签。1234@Overridepublic void addFirst(Item x) &#123; ...&#125; 但即使不包含这个@Override，我们仍然override了这个方法。所以从技术上来说，@Override并不是必须的。但是，它可以作为一个保障, 提醒编译器我们打算override此方法, 如果过程中出现问题, 编译器可以提醒。假设当我们想 override addLast，却不小心写成addLsat。此时如果不包含@Override，那么可能无法发现错误。如果有了@Override，编译器就会提示我们修复错误。 总结：Interface Inheritance接口继承是指subclass继承superclass的所有方法/行为的关系： 子类继承父类 Interfaces 接口列出所有方法的签名，就像‘合约’，但没有具体的实现 根据‘合约’，由子类来实现且必须实现（override）每一个method，否则无法通过编译 继承关系可以延续多代。例如，B可以继承A，C可以继承B. GRoE根据Java的Golden Rule of Equals，每一个赋值a = b，本质上是把b中的bits拷贝到a中，着要求b和a的类型相同。 同理, 假设public static String longest(List&lt;String&gt; list)既接受List, 也接受ArrayList和LinkedList，但是由于ArrayList和List是不同的类，那怎么遵守GRoE呢？ 因为ArrayList与List有着上下位包含的关系，这意味着ArrayList应该能够赋值给List的内存位中.1234public static void main(String[] args) &#123; List&lt;String&gt; someList = new SLList&lt;String&gt;(); someList.addFirst("elk");&#125; 这段代码运行时，会创建SLList并将其地址存储在someList变量中。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Accelerated Natural Language Processing - Informatics - University of Edinburgh]]></title>
    <url>%2FUoE-anlp%2F</url>
    <content type="text"><![CDATA[References:Accelerated natural language processingANLP revision guideLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning 概率模型 Probability model概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率 估算概率 Probability estimation相关频率/最大似然估计Relative frequency / maximum likelihood estimation p(X) = Count(x)/N 平滑 Smoothing一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。 Language modeling What: To compute the probability of sentence /sequence of words P(w1, w2, w3…), or to predict upcomming words P(w|w1, w2, w3…)… a language model is also a probability model. Why: the motivation is that probability is essential in identifying information in noisy, ambiguous inputs: speech recognition, machine translation, spelling correction… How: rely on chain rule of probability, the products of a sequence of conditional probability. Simplified by Markov Assumption: approximate the conditional probability by only accounting several prefixes,P(the| water is so transparent that) ≈ P(the| that) Evaluation: how good is the model GENERATIVE PROBABILISTIC MODELSGenerative(joint) models palce probabilities P(c,d) over both observed data d and the hidden variables c (generate the obersved data from hidden stuff). N-Gram Language Model Unigram P(w1,w2,w3..) ≈ P(w1)*P(w2)*P(w3) Bigram P(wn| w1,w2,w3..) ≈ P(wn| wn-1) Estimate probability by counting:P(wi| prefixes) = count(prefixes, wi)/count(prefixes) In practice, use log space to avoid underflow, and adding is faster than multiplying. Insufficient: long-distance dependencies N-grams only work well for word prediction if the test corpus looks like the training corpus. To deal with 0 probability, commonly use Kneser-Ney smoothing, for very large N-grams like web, use stupid backoff. Add alpha smoothing Assign equal probability to all unseen events. Applied in text classification, or domains where zeros probability is not common. Backoff smoothing Use information from lower order N-grams (shorter histories) Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram. Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. Interpolation smoothing Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts Simple interpolation: P(w3|w1,w2)=1P(w3|w1,w2)+λ2P(w3|w2)+λ3P(w3), Σλ=1. λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data Kneser-Ney smoothing Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams. Use a better estimate for probabilities of lower-order unigrams, the continuation probability, P_continuatin(w) is how likely is w to appear as a novel continutaion. For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w. Every bigram type was a novel continuation the first time it was seen. normalized by the total number of word bigram types. To lower the probability of some fix bigram like “San Franscio” For bigram, Pkn(wi|wi-1)=max(count(wi-1,wi)-d, 0)/c(wi-1) +λ(wi-1)P_continuatin(wi), λ(wi-1) = d{w:count(wi-1,w)&gt;0}/c(wi-1), where {w:count(wi-1,w)&gt;0} is the number of word types that can follow wi-1, also is the # of word types we discounted, also is the # of times we applied normalized discount. For general N-gram, Naive Bayes classifier Application: Text classification, to classify a text, we calculate each class probability given the test sequence, and choose the biggest one. Evaluation: precision, recall, F-measure Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 Text classificationOr text categorization, method is not limited to NB, see lab7.Spam email, gender/authorship/language identification, sentiments analysis,(opinion extraction, subjectivity analysis)… Sentiments analysis For sentiment(or other text classification), word occurrence may matter more than word frequency. Thus it often improves performance to clip the word counts in each document at 1. This variant binary NB is called binary multinominal naive Bayes or binary NB. Remove duplicates in each data sample - bag of words representation, boolean features. Binarized seems to work better than full word counts. Deal with negation: like, not like, A very simple baseline that is commonly used in sentiment to deal with negation is during text normalization to prepend the prefix NOT_ to every word after a token of logical negation Sentiment lexicons: lists of words that are preannotated with positive or negative sentiment. To deal with insufficient labeled training data. A common way to use lexicons in the classifier is to use as one feature the totalcount of occurrences of any words in the positive lexicon, and as a second feature the total count of occurrences of words in the negative lexicon. Using just two features results in classifiers that are much less sparse to small amounts of training data, and may generalize better. See lab8. Naive Bayes Assumptions Bags of words: a set of unordered words/features with its frequency in the documents, their order was ignored. Conditional independence: the probabilities P(w|C) are independence given the class, thus a sequence of words(w1,w2,w3…) probability coculd be estimate via prducts of each P(wi|C) by walking through every pisition of the sequence, noted that the orders in the sequnce does not matter. NB Training Each classes’ prior probability P(C) is the percentage of the classes in the training set. For the test set, its probability as a class j, is the products of its sequence probability P(w1, w2, w3…|Cj) and P(Cj), normalized by the sequence probability P(w1, w2, w3…), which could be calculated by summing all P(w1, w2, w3…|Cj)*P(Cj). The joint features probability P(w1, w2, w3…|C) of each class is calculated by naively multiplying each word’s MLE given that class. In practice, to deal with 0 probability, we dun use MLE, instead we use add alpha smoothing. Why 0 probability matters? Because it makes the whole sequence probability P(w1, w2, w3…|C) 0, then all the other features as evidence for the class are eliminated too. How: first extract all the vocabulary V in the training set. Then, for each feature/word k, its add alpha smoothing probability estimation within a class j is (Njk + alpha)/(Nj+V*alpha). This is not the actual probability, but just the numerator. Naive bayes relationship to language modelling When using all of the words as features for naive bayes, then each class in naive bayes is a unigram languange model. For each word, assign probability P(word|C), For each sentence, assign probability P(S|C) = P(w1,w2,w3…|C) Running multiple languange models(classes) to assign probabilities, and pick out the highest language model. Hidden Markov Model What: The HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence. parameter λ: A Transition probability matrix, B Emission probability Application: part-of-speech tagging, name entity recognition(NEr), parse tree, speech recognition Hidden: these tags, trees or words is not observed(hidden) The three fundamental problems of HMM: decoding: discover the best hidden state sequnce via Viterbi algorithm Probability of the observation: Given an HMM with know parameters λ and an observation sequence O, determine the likelihood P(O| λ) (a language model regardless of tags) via Forward algorithm Learning: Given only the observed sequence, learn the best(MLE) HMM parameters λ via forward-backward algorithm, thus training a HMM is an unsupervised learning task. Part-of-speech tagging Part-of-speech(POS), word classes, or syntactic categories, a description of eight parts-of-speech: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner. noun 名詞 (代號 n. ) pronoun 代名詞 (代號 pron. ) verb 動詞 (代號 v. ) adjective 形容詞 (代號 adj. ) adverb 副詞 (代號 adv. ) preposition 介系詞 (代號 prep. ) conjunction 連接詞 (代號 conj. ) interjection 感歎詞 (代號 int. ) Motivation: Use model to find the best tag sequnce T for an untagged senetnce S: argmax P(T|S) -&gt; argmax P(S|T)*P(T), where P(T) is the transition (prior) probabilities, P(S|T) is the emission (likelihood) probabilities. Parts-of-speech can be divided into two broad supercategories: closed class types and open class types Search for the best tag sequnce: Viterbi algorithm evaluation: tag accuracy Transition probability matrix Tags or states Each (i,j) represent the probability of moving from state i to j When estimated from sequnces, should include beginning and end markers. Tag transition probability matrix: the probability of tag i followed by j Emission probability Also called observation likelihoods, each expressing the probability of an observation j being generated from a states i. Word/symbol Penn Treebank Viterbi algorithm Decoding task: the task of determining which sequence of variables is the underlying source of some sequence of observations. Intuition: The probability of words w1 followed by w2 with tag/state i and j (i,j is index of all Tags), is the chain rule of the probability of i followed by j and the probability of i output wi P(w1|i) and P(w2 |j), then choose the maximum from all the possible i j. Then using chain rule to multiply the whole sequence of words. The value of each cell Vt(j) is computed by recursively taking the most probable path that could lead us to this cell from left columns to right. See exampls in tutorial 2 Since HMM based on Markov Assumptions, so the present column Vt is only related with the nearby left column Vt-1. Forward algorithm Compute the likelihood of a particular observation sequence. Implementation is almost the same as Viterbi. Yet Viterbi takes the max over the previous path probabilities whereas the forward algorithm takes the sum. HMM Traininglearning the parameters of an HMM Forward-backward algorithm inputs: just the observed sequence output: the converged λ(A,B). For each interation k until λ converged: Compute expected counts using λ(k-1) Set λ(k) using MLE on the expected counts. Context-free grammarCFG(phrase-structure grammar) consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered toLexicon gether, and a lexicon of words and symbols. Probabilistic Context-Free GrammarPCFG(Stochastic Context-Free Grammar SCFG (SCFG)), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability. G = (T,N,S,R,P) T, N: Terminal and Non-terminal S: starts symbol R: Derive rule/grammar, N -&gt; N/C P: a probability function, for a given N, ΣP(N-&gt;Ni/Ci)=1. Normally P(S-&gt;NP VP)=1, because this is the only rule for S. PCFG could generates a sentence/tree, thus it is a language model, assigns a probability to the string of words constituting a sentence The probability of a tree t is the product of the probabilities of the rules used to generate it. The probability of the string s is the sum of the probabilities of the trees/parses which have that string as their yield. The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence. Application: Probabilistic parsing Shortage: lack the lexicalization of a trigram model, i.e only a small fraction of the rules contains information about words. To solve this problem, use lexicalized PCFGs Lexicalization of PCFGs The head word of phrase gives a good representation of the phrase’s structure and meaning Puts the properties of words back into a PCFG Word to word affinities are useful for certain ambiguities, because we know the probability of rule with words and words now, e.g. PP attachment ambiguity Recursive Descent Parsing It is a top-down, depth-first parser: Blindly expand nonterminals until reaching a terminal (word). If multiple options available, choose one but store current stateas a backtrack point (in a stack to ensure depth-first.) If terminal matches next input word, continue; else, backtrack can be massively inefficient (exponential in sentence length) if faced with local ambiguity infinite loop CKY parsingDynamic programmingWell-formed substring tableFor parsing, subproblems are analyses of substrings, memoized in well-formed substring table(WFST, chart). Chart entries are indexed by start and end positions in the sentence, and correspond to: either a complete constituent (sub-tree) spanning those positions (if working bottom-up), or a prediction about what complete constituent might be found (if working top-down). The chart is a matrix where cell [i, j] holds information about the word span from position i to position j: The root node of any constituent(s) spanning those words Pointers to its sub-constituents (Depending on parsing method,) predictions about whatconstituents might follow the substring. Probability CKY parsing Noisy channel model: The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. a probability model using Bayesian inference, input -&gt; noisy/errorful encoding -&gt; output, see an observation x (a misspelled word) and our job is to find the word w that generated this misspelled word. P(w|x) = P(x|w)\*P(w)/P(x) Noisy channel model of spelling using naive bayes The noisy channel model is to maximize the product of likelihood(probability estimation) P(s|w) and the prior probability of correct words P(w). Intuitively it is modleing the noisy channel that turn a correct word ‘w’ to the misspelling. The likelihood(probability estimation) P(s|w) is called the the channel/error model, telling if it was the word ‘w’, how likely it was to generate this exact error. The P(w) is called the language model DISCRIMINATIVE PROBABILISTIC MODELSDiscriminative(conditional) models take the data as given, and put a probability over hidden structure given the data, P(c|d). Exponential (Log-linear, MaxEnt, Logistic) modelsMake probability model from the linear combination of weights λ and features f as votes, normalized by the total votes. It is a probabilistic distribution: it estimates a probability for each class/label, aka Softmax. It is a classifier, choose the highest probability label. Application: dependency parsing actions prediction, text classification, Word sense disambiguation Topics categorizationTraining discriminative model Features in NLP are more general, they specify indicator function(a yes/no[0,1] boolean matching function) of properties of the input and each class. Weights: low possibility features will associate with low/negative weight, vise versa. Define features: Pick sets of data points d which are distinctive enough to deserve model parameters: related words, words contians #, words end with ing, etc. Regularization in discriminative modelThe issue of scale: Lots of features sparsity: easily overfitting: need smoothing Many features seen in training never occur again in test Optimization problem: feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. See tutorial 4. Solution: Early stopping Smooth the parameter via L2 regularization. Smooth the data, like the add alpha smoothing, but hard to know what artificial data to create Generative vs. Discriminative Models Navie bayes models multi-count correlated evidence: each feature is multipled in, even when you have multiple features telling the same informaiton. Maxent: solve this issue by weighting features so that model expectations match the observed(empirical) expectations. Basic Text ProcessingRegular Expressionsa language for specifying text search strings. Word tokenizationNLP task needs to do text normalizaGon: Segmenting/tokenizing words in running text Normalizing word formats Segmenting sentences in running text they lay back on the San Francisco grass and looked at the stars and their Type: an element of the vocabulary. Token: an instance of that type in the actual text. LINGUISTIC AND REPRESENTATIONAL CONCEPTSParsing Parsing is a combination of recognizing an input string and assigning a correct linguistic structure/tree to it based on a grammar. The Syntactic, Statistical parsing are constituent-based representations(context-free grammars). The Dependency Parsing are based on dependency structure(dependency grammars). Syntactic ParsingSyntactic parsing, is the task of recognizing a sentence and assigning a correct syntactic structure to it. Syntactic parsing can be viewed as a search search space: all possible trees generated by the grammar search guided by the structure of the space and the input. search direction top-down: start with root category (S), choose expansions, build down to words. bottom-up: build subtrees over words, build up to S. Search algorithm/strategy: DFS, BFS, Recursive descent parsing, CKY Parsing Challenge: Structual Ambiguity Statistical ParsingOr probabilistic parsing, Build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers. motivation: to solve the problem of disambiguation algorithm: probability CKY parsing evaluation: Compare the output constituency parser with golden standard tree, a constituent(part of the output parser) marked as correct if it spans the same sentence positions with the corresponding constituent in golder standard tree. Then we get the precision, recall and F1 measure. constituency: S-(0:10), NP-(0:2), VP-(0:9)… Precission = (# correct constituents)/(# in parser output), recall = (# correct constituents)/(# in gold standard) Not a good evaluation, because it higher order constituent is marked wrong simply it contains a lower level wrong constituent. Dependency ParsingConstituencyPhrase structure, organizes words into nested constituents. Groups of words behaving as a single units, or constituents. Noun phrase(NP), a sequence of words surrounding at least one noun. While the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase Preposed or Postposed constructions. While the entire phrase can be placed differently, the individual words making up the phrase cannot be. Fallback: In languages with free word order, phrase structure(constituency) grammars don’t make as much sense. Headed phrase structure: many phrase has head, VP-&gt;VB, NP-&gt;NN, the other symbols excepct the head is modifyer. Dependency syntaxDependency structure shows which words depend on (modify or are arguments of) which other words. A fully lexicalized formalism without phrasal constituents and phrase-structure rules: binary, asymmetric grammatical relations between words. More specific, head-dependent relations, with edges point from heads to their dependents. Motivation: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. E.g. we may need both S → NP VP and S → VP NP, but could not tell too much information simply looking at the rule. Dependencies: Identifies syntactic relations directly. The syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words. Relation between phrase structure and dependency structure Convert phrase structure annotations to dependencies via head rules. (Convenient if we already have a phrase structure treebank.): For a given lexicalized constituency parse(CFG tree), remove the phrasal categories, remove the (duplicated) terminals, and collapse chains of duplicates. The closure of dependencies give constituency from a dependency tree Dependency parsing Motivation: context-free parsing algorithms base their decisions on adjacency; in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective); we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present). Approach: Transition-based dependency parsing Transition-based dependency parsingtransition-based systems use supervised machine learning methods to train classifiers that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators(actions). Bottom up Like shift-reduce parsing, but the ‘reduce’ actions are specialized to create dependencies with head on left or right. configuration：consists of a stack, an input buffer of words or tokens, and a set of relations/arcs, a set of actions. How to choose the next action: each action is predicted by a discriminative classifier(often SVM, could be maxent) over each legal move. features: a sequence of the correct (configuration, action) pairs f(c ; x). Evaluation: accuracy (# correct dependencies with or ignore label)). Dependency tree Dependencies from a CFG tree using heads, must be projective: There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words. But dependency theory normally does allow non-projective structures to account for displaced constituents. Bounded and unbounded dependenciesUnbounded dependency could be considered as long distance dependency Long-distance dependencies: contained in wh-non-subject-question, “What flights do you have from Burbank to Tacoma Washington?”, the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the VP. AmbiguityStructural ambiguityOccurs when the grammar can assign more than one parse to a sentence. Attachment ambiguityA sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. Coordination ambiguitydifferent sets of phrases can be conjoined by a conjunction like and. E.g green egg and bread. Coordination: The major phrase types discussed here can be conjoined with conjunctions like and, or, and but to form larger constructions of the same type. Global and local ambiguity global ambiguity: multiple analyses for a full sentence, like I saw the man with the telescope local ambiguity: multiple analyses for parts of sentence. the dog bit the child: first three words could be NP (but aren’t). Building useless partial structures wastes time. Morphology 构词学（英语言学分科学名：morphology，“组织与形态”；morphology (/mɔːrˈfɒlədʒi/[1]) is the study of words, how they are formed, and their relationship to other words in the same language.），又称形态学，是语言学的一个分支，研究单词（word）的内部结构和其形成方式。如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher。构词学正是研究这种单字间组成的关系，并试着整理出其组成的规则。 Challenge of rich MorphologyFor a morphologically rich language, many issues would arise because of the morphological complexity. These productive word-formation processes result in a large vocabulary for these languages Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages For POS, augmentations become necessary when dealing with highly inflected or agglutinative languages with rich morphology like Czech, Hungarian and Turkish., part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than asingle primitive tag. Dependency grammar is better than constituency in dealing with languages that are morphologically rich。 morphemesThe way words are built up from smaller meaning-bearing units. Lemma Lexeme, refers to the set of all the forms that have the same meaning, lemma: refers to the particular form that is chosen by convention to represent the lexeme. E.g: run, runs, ran, running are forms of the same lexeme, with run as the lemma. AffixesAdding additional meanings of various kinds. “+ed, un+” suffix : follow the stem Plural of nouns ‘cat+s’ Comparative and superlative of adjectives ‘small+er’ Formation of adverbs ‘great+ly’ Verb tenses ‘walk+ed’ All inflectional morphology in English uses suffixes Prefix: precede the stem In English: these typically change the meaning Adjectives ‘un+friendly’, ‘dis+interested’ Verbs ‘re+consider’ Some language use prefixing much more widely Infix: inserted inside the stem Circumfix: do both(follow, precede) Morphological parsingMethod: Finite-state transducers Finite-state transducersFST, a transducer maps between one representation and another; It is a kind of FSA which maps between two sets of symbols. Root Root, stem and base are all terms used in the literature to designate that part of a word that remains when all affixes have been removed. The root word is the primary lexical unit of a word, and of a word family (this root is then called the base word), which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents. E.g: In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’. Stem Stem is of concern only when dealing with inflectional morphology Stemming: reduce terms to their stems in info retrieval, E.g: In the form ‘untouchables’ the stem is ‘untouchable’, ‘touched’ -&gt; ‘touch’; ‘wheelchairs’ -&gt; ‘wheelchair’. Inflectional vs. Derivational MorphologyInflectional· nouns for count (plural: +s) and for possessive case (+’s)· verbs for tense (+ed, +ing) and a special 3rd person singular present form (+s)· adjectives in comparative (+er) and superlative (+est) forms. Derivational· Changing the part of speech, e.g. noun to verb: ‘word → wordify’· Changing the verb back to a noun· Nominalization: formation of new nouns, often verbs or adjectives Inflectional Derivational does not change basic meaning or part of speech may change the part of speech or meaning of a word expresses grammatical features or relations between words not driven by syntactic relations outside the word applies to all words of the same part of speech, inflection occurs at word edges: govern+ment+s, centr+al+ize+d applies closer to the stem Open-class Closed-classClosed classes are those with relatively fixed membership prepositions: on, under, over, near, by, at, from, to, with determiners: a, an, the pronouns: she, who, I, others conjunctions: and, but, or, as, if, when auxiliary verbs: can, may, should, are particles: up, down, on, off, in, out, at, by numerals: one, two, three, first, second, third Open-class Nouns, verbs, adjectives, adverbs Word senseA discrete representation of an aspect of a word’s meaning.How: Distributional semantic models Word sense disambiguationWSD, The task of selecting the correct sense for a word, formulated as a classification task. Chose features: Directly neighboring words, content words, syntactically related words, topic of the text, part-of-speech tag, surrounding part-of-speech tags, etc … CollocationA sequence of words or terms that co-occur more often than would be expected by chance. Lexical semantic relationshipsRelations between word senses synonym代名词, When two senses of two different words (lemmas) are identical, or nearly identical, the two senses are synonyms. E.g. couch/sofa vomit/throw up filbert/hazelnut car/automobile hyponym下义词, One sense is a hyponym of another sense if the first sense is more specific, denoting a subclass of the other. E.g. car is a hyponym of vehicle; dog is a hyponym of animal, and mango is a hyponym of fruit. hypernymSuperordinate, 上位词, vehicle is a hypernym of car, and animal is a hypernym of dog. similarityOr distance, a looser metric than synonymy.Two ways to measure similarity: Thesaurus词库-based: are words nearby in hypernym hierarchy? Do words have similar definitions? Distributional: do words have similar distributional contexts Distributional semantic modelsVector semantics(embeddings): The meaning of a word is represented as a vector. Two words are similar if they have similar word contexts vector. Term-context matrix(Co-occurrence Matrices): a word/term is defined by a vector over counts of context words. The row represent words, columns contexts. Problem: simple frequency isn’t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. “the” and “of” are very frequent, but maybe not the most discriminative. Sulution: use Pointwise mutual information. Then the Co-occurrence Matrices is filled with PPMI, instead of raw counts. Measuring vectors similarity based on PPMI: Dot product(inner product): More frequent words will have higher dot products, which cause similarity sensitive to word frequency. Cosine: normalized dot product , Raw frequency or PPMI is non-negative, so cosine range [0,1]. Evaluation of similarity Intrinsic: correlation between algorithm and human word similarity ratings. Check if there is correlation between similarity measures and word frequency. Application: sentiment analysis, see lab8 Pointwise mutual informationPMI: do events x and y co-occur more than if they were independent? PMI between two words: Compute PMI on a term-context matrix(using counts):12345PMI(x, y) = log2( N·C(x, y)/C(x)C(y) )p(w=information,c=data) = 6/19p(w=information) = 11/19p(c=data) = 7/19PMI(information,data) = log2(6\*19/(11\*7)) PMI is biased towards infrequent events, solution: Add-one smoothingPPMIPositive PMI, could better handle low frequenciesPPMI = max(PMI,0) t-testThe t-test statistic, like PMI, can be used to measure how muchmore frequent the association is than chance. The t-test statistic computes the difference between observed and expected means, normalized by the variance. The higher the value of t, the greater the likelihood that we can reject the null hypothesis. Null hypothesis: the two words are independent, and hence P(a,b) = P(a)P(b) correctly models the relationship between the two words. Minimum Edit Distancethe minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.Algorithm: searching the shortest path, use Dynamic programming to avoid repeating, (use BFS to search the shortest path?) WordNetA hierarchically organizesd lexical database, resource for English sense relations Synset: The set of near-synonyms for a WordNet sense (for synonym set) Topic modellingInstead of using supervised topic classification – rather not fix topics in advance nor do manual annotation, Use clustering to teases out the topics. Only the number of topics is specified in advance. Latent Dirichlet allocation(LDA): each document may be viewed as a mixture of various topics where each document is generated by LDA. A topic is a distribution over words generate document: Randomly choose a distribution over topics For each word in the document randomly choose a topic from the distribution over topics randomly choose a word from the corresponding topic (distribution over the vocabulary) training: repeat until converge assign each word in each document to one of T topics. For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t) Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d) Meaning representation languageThe symbols in our meaning representations correspond to objects, properties, and relations in the world. Qualifications of MRL: Canonical form: sentences with the same (literal) meaning should have the same MR. Compositional: The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined. Verifiable: Can use the MR of a sentence to determine whether the sentence is true with respect to some given model of the world. Unambiguous: an MR should have exactly one interpretation. Inference: we should be able to verify sentences not only directly, but also by drawing conclusions based on the input MR and facts in the knowledge base. Expressivity: the MRL should allow us to handle a wide range of meanings and express appropriate relationships between the words in a sentence. Good MRL: First-order Logic First-order LogicFOL, Predicate logic, meets all of the MRL qualifications except compositionality. Expressions are constructed from terms: constant and variable symbols that represent entities function symbols that allow us to indirectly specify entities predicate symbols that represent properties of entities and relations between entities Terms can be combined into predicate-argument structures Logical connectives: ∨ - or, ∧ - and, ¬, ⇒ Quantifiers: ∀ (universal quantifier, i.e., “for all”), ∃ (existentialquantifier, i.e. “exists”) Predicates in FOL Predicates with multiple arguments represent relations between entities: member-of(UK, EU) “/N” to indicate that a predicate takes N arguments: member-of/2 Variables in FOL An expression consisting only of a predicate with a variable among its arguments is interpreted as a set: likes(x, Gim) is the set of entities that like Gim. A predicate with a variable among its arguments only has a truth value if it is bound by a quantifier: ∀x.likes(x, Gim) has an interpretation as either true or false. Universal Quantifier (∀): Cats are mammals has MR ∀x.cat(x) ⇒ mammal(x) Existential Quantifier (∃): Used to express that a property/relation is true of some entity, without specifying which one: Marie owns a cat has MR ∃x.cat(x) ∧ owns(Marie,x) Lambda λ ExpressionExtend FOL, to work with ‘partially constructed’ formula, Compositionality. E.g.： λx.sleep(x) is the function that takes an entity x to the FOL expression sleep(x). λx.sleep(x)(Marie) -&gt; sleep(Marie) Verbal (event) MRs： λz. λy. λx. Giving1(x,y,z) (book)(Mary)(John) -&gt; Giving1(John, Mary, book) -&gt; John gave Mary a book Problem: fixed arguments Requires separate Giving predicate for each syntactic subcategorisation frame(number/type/position of arguments). Separate predicates have no logical relation: if Giving3(a, b, c, d, e) is true, what about Giving2(a, b, c, d) and Giving1(a, b, c). Solution: Reification of events 事件具象化 Reification of eventsJohn gave Mary a book -&gt; ∃e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Reify: to “make real” or concrete, i.e., give events the same status asentities. In practice, introduce variables for events, which we can quantify over Entailment relations: automatically gives us logical entailment relations between events1234[John gave Mary a book on Tuesday] -&gt; [John gave Mary a book]∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) ∧ Time(e, Tuesday)-&gt;∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Semantic ParsingAka semantic analysis. Systems for mapping from a text string to any logical form. Motivation: deriving a meaning representation from a sentence. Application: question answering Method: Syntax driven semantic analysis with semantic attachments Syntax Driven Semantic Analysis Principle of compositionality: the construction of constituent meaning is derived from/composed of the meaning of the constituents/words within that constituent, guided by word order and syntactic relations. Build up the MR by augmenting CFG rules with semantic composition rules. Add semantic attachments to CFG rules. Problem: encounter invalide FOL for some (base-form) MR, need type-raise. Training Semantic attachmentsE.g123456VP → Verb NP : &#123;Verb.sem(NP.sem)&#125;Verb.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y)NP.sem = Meat-&gt;VP.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) (Meat)= λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) The MR for VP, is computed by applying the MR function to VP’s children. Complete the rule:123456S → NP VP : &#123;VP.sem(NP.sem)&#125;VP.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat)NP.sem = AyCaramba-&gt;S.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) (AyCa.)= ∃e. Serving(e) ∧ Server(e, AyCaramba) ∧ Served(e, Meat) Lexical semanticsthe meaning of individual words. EVALUATION CONCEPTS AND METHODSInstrinsic vs. extrinsic evaluationExtrinsicUse something external to measure the model. End-to-end evaluation, the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Put each model in a task: spelling corrector, speech recognizer, MT system Run the task, get an accuracy for A and for B How many misspelled words corrected properly How many words translated correctly Compare accuracy for A and B Unfortunately, running big NLP systems end-to-end is often very expensive. IntrinsicMeasures independenly to any application. Train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. Which means whichever model assigns a higher probability to the test set Perplexity It is intrinsic. Intuition based on Shannon game:The best language model is one that best predicts an unseen test set(e.g. next word), gives the highest P(sentence) to the word that actually occurs. Definition： Perplexity is the inverse probability of the test set, normalized by the number of words(lie between 0-1). So minimizing perplexity is the same as maximizing probability Cannot divide 0, so use smoothing. Bad approximation: unless the test data looks just like the training data, so generally only useful in pilot experiments. Human evaluationE.g to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying togold labels match. We will refer to these human labels as the gold labels. Precision, Recall, F-measure To deal with unbalanced lables Application: text classification, parsing. Evaluation in text classification: the 2 by 2 contingency table, golden lable is true or false, the classifier output is positive or negative. Precision% of positive items that are golden correct, from the view of classifier Recall% of golden correct items that are positive, from the view of test set. F-measure Motivation: there is tradeoff between precision and recall, so we need a combined meeasure that assesses the P/R tradeoff. The b parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of b &gt; 1 favor recall, while values of b &lt; 1 favor precision. Balanced F1 measure with beta =1, F = 2PR/(P+R) Confusion matrixRecalled that confusion matrix’s row represent golden label, column represent the classifier’s output, to anwser the quesion：for any pair of classes(c1,c2), how many test sample from c1 were incorrectly assigned to c2&gt; Recall: Fraction of samples in c1 classified correctly, CM(c1,c1)/sum(CM(c1,:)) Precision: fraction of samples assigned c1 that are actually c1, CM(c1,c1)/sum(CM(:,c1)) Accuracy: sum of diagnal / all CorrelationWhen two sets of data are strongly linked together we say they have a High Correlation.Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases. Pearson correlation: covariance of the two variables divided by the product of their standard deviations. Spearman correlation: the Pearson correlation between the rank values of the two variables]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
        <tag>UoE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 11 测试 Testing - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-11-testing%2F</url>
    <content type="text"><![CDATA[测试如何知道自己的程序是否真的在工作？在现实世界中，程序员相信他们的代码，因为代码通过了他们自己编写的测试。常用的测试有 Ad Hoc Testing， Unit test 和 Integration Testing。 Ad Hoc Testing，是指没有计划和记录的软件测试，除非发现缺陷，不然一般只运行一次。 Unit test程序可分解为单元（或程序中可测试的最小部分），Unit test 严格测试代码的每个单元，最终确保项目正确运行。Unit test 好处： Unit test 保证良好的代码结构（每个 method “只打一份工”），帮助我们较好地解析任务， 允许我们考虑每个方法的所有边界情况，并单独测试它们。 让我们每次只专注于一个单元，进行测试，debug，对准确度有信心后，再进行下一个单元的开发。相比于一次性写完所有代码，再测试debug，Unit test 减少了 debugging 时间。 坏处： 测试也要花时间 测试本身也是有可能出错的，测试可能不全面，不规范，或者有bug 有些单元是依赖于其他单元的 Unit testing 无法保证各个模块的交互，无法保证整个系统作为一个整体是否正常工作。 JUnitJUnit是一个给Java做测试的框架，由Erich Gamma（Design Patterns）和Kent Beck（eXtreme Programming）编写。JUnit使用Java的 reflection 功能（Java程序可以检查自己的代码）和注释。JUnit允许我们： 定义并执行测试和测试套件 使用测试作为规范的有效手段 使用测试来支持重构 将修改的代码集成到构建中JUnit可用于多个IDE，例如BlueJ，JBuilder和Eclipse在一定程度上具有JUnit集成。 1234567import org.junit.Test;import static org.junit.Assert.*;@Testpublic void testMethod() &#123; assertEquals(&lt;expected&gt;, &lt;actual&gt;);&#125; assertEquals测试一个变量的实际值是否等于它的期望值。JUnit test 各个测试方法，必须是非静态的（JUnit的设计人员设计规定的）。 JUnit的术语 Test runner：测试运行器， 运行测试和报告结果的软件。实现方式：集成到IDE中，独立GUI，命令行等 Test suite：测试套件是一组测试用例。 Test case：测试用例用于测试单个方法对特定输入集的响应。 Unit test：单元测试的单元，是代码中我们能够相对合理地测试的最小的元素，通常是单个类。 常用的JUnit接口和方法@Before: Creates a test fixture by creating and initialising objects and values. @After: Releases any system resources used by the test fixture. Java usually does this for free, but files, network connections etc. might not get tidied up automatically. @Test：tests cases. static void assertTrue(boolean test), static void assertTrue(String message, boolean test), static void assertFalse(boolean test), static void assertFalse(String message, boolean test) Integration Testing鉴于 Unit testing 无法保证，有交互的多个模块，作为一个整体是否正常工作。我们可能需要 integration testing，把各个模块合并，作为一个组合，进行测试（也可以把 Unit test 组合起来变成 integration testing）。 Integration testing 一般都比较麻烦，也不容易自动化，而且一般是在比较高的抽象层进行测试，可能会漏掉微小的错误。 当把所有模块都作为一个整体，也就是整个系统作为测试对象时，就是 system testing。 Test driven developmentTDD开发步骤： 明确一项新功能需求。 为该功能编写 Unit test。 运行测试，按理应该无法通过测试（因为还没写功能程序）。 编写通过实现该功能的代码，通过测试。 可选：重构代码，使其更快，更整洁等等。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 10 LinkedList 还是 ArrayList - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-10-java-which-list%2F</url>
    <content type="text"><![CDATA[Java 提供了 ArrayList, ArrayDeque 和 LinkedList 几个API. 队列 queue, 通俗的含义, 就是不能插队, 只能在末尾插入. 双端队列 Double Ended Queue (Deque) 是具有动态大小的序列容器，可以在两端（前端或后端）扩展或收缩–http://www.cplusplus.com/reference/deque/deque/ CS61b的project 1a需要实现两种双端队列（array based 和 linkedklist based）. 不同的API, 在考虑什么时候应该用哪个时, 我们需要考虑它们的性能差异: 搜索/定位：与LinkedList相比，ArrayList搜索更快。 ArrayList的get(int index)性能是O(1)的，而LinkedList的性能是O(n)。因为ArrayList基于array数据结构，可以直接用 array index 定位元素。 删除/插入：LinkedList 操作性能是O(1)，而ArrayList的性能从O(n)（删除/插入第一个元素）到O(n)（最后一个元素）都有可能。因为LinkedList的每个元素都包含两个指向其相邻前后元素的指针（地址），因此仅需要改变，被删节点的prev和next指针位置。而在ArrayList中，需要移动剩余元素，来重新填充array空间。 内存开销：LinkedList的每个元素都有更多的内存开销(额外的指针), 而ArrayLists没有这个开销。但是，ArrayLists需要占用初始容量。一般ArrayList的默认初始容量非常小（Java 1.4 - 1.8使用10）。但是，往ArrayLists添加元素时， 它可能会适当地增大容量，所以如果添加了很多元素，则必须不断调整数组的大小，那样也可能会导致元素频繁挪动位置。 综上所述： 如果在应用中需要频繁插入和删除，那么选择LinkedList。 假如一开始，就知道后面要添加大量元素，那就使用较高的初始容量来构造ArrayList。 大部分用例中, 相比LinkedList, 人们更偏爱ArrayList以及ArrayDeque。如果你不确定应该选哪个, 那么就直接考虑ArrayList吧(参考 https://stackoverflow.com/questions/322715/when-to-use-linkedlist-over-arraylist).]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 09 双向链表 Doubly Linked List - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-09-java-doubly-linked-list%2F</url>
    <content type="text"><![CDATA[双向链表Doubly Linked List 前面介绍过的单向链表有几个缺点. 第一个就是它的addLast操作非常慢。单向链表只有一个变量保存列表头的地址, 以及每个节点对后面节点的单向引用(链接). 对于很长的列表，addLast方法必须遍历整个列表, 直到找到列表末尾才能执行插入操作.最直观的优化方案就是加个’车尾’ 这样我们就可以直接通过last.next引用末尾位置. 不过另一个问题并没有解决, 就是删除列表最后一项removeLast这个操作还是很慢。因为在目前的结构设计下, 我们需要先找到倒数第二项，然后将其下一个指针设置为null。而要找到倒数第二节点, 我们就得先找到倒数第三个节点…… 以此类推。也就是说，对于删除末尾的操作，还是要几乎遍历整个列表。 反方向的链接基于前面单向链表构建双向链表, 一个比较有效的方法是额外为每个节点添加一个指向前面节点的链接 - 指针.12345public class OneNode &#123; public OneNode prev; //指向前 public int item; public OneNode next; //指向后&#125; 增加这些额外的指针会导致额外的代码复杂度, 以及额外的内存开销, 这就是追求时间效率的代价. Sentinel 与尾节点双向链表的一个设计初衷，就是为了解决单向链表针对列表末尾位置的操作效率不高的问题，除了sentinel和反方向的链接还不够，我们还需要一个节点（指针）能够直接帮我们定位到列表末端。可以考虑添加一个的尾节点last 这样的列表就可以支持O(1)复杂度的addLast,getLast 和 removeLast操作了。 循环双端队列Circular double ended queue 上面的尾节点设计虽然没什么错误，但有点瑕疵：最后一个尾节点指针有时指向前哨节点，有时指向一个真正的节点。更好的方法是使双向链表首尾相连, 构成一个循环，即前后节点共享唯一的一个前哨节点。 这样的设计相对更整洁，更美观(主观上的), sentinel的prev就指向列表最后一个节点, sentinel的next指向列表第一个节点.12345678910111213public class LinkedListDeque&lt;GType&gt; &#123; private class OneNode &#123; public OneNode prev; public GType item; public OneNode next; public OneNode(OneNode p, GType i, OneNode n) &#123; prev = p; item = i; next = n; &#125; &#125;&#125; Sentinel’s forward link always points to the last element.Sentinel’s backward link always points to the first element. 然后修改构造函数:123456789101112131415/** Creates an empty deque. */public LinkedListDeque()&#123; sentinel = new OneNode(null,null, null); sentinel.prev = sentinel; sentinel.next = sentinel; size = 0;&#125;/** Creates a deque with x */public LinkedListDeque(GType x)&#123; sentinel = new OneNode(null, null, null); sentinel.next = new OneNode(sentinel, x, sentinel); sentinel.prev = sentinel.next; size = 1;&#125; 如果初始化的是空列表, 其实就是一个自己指向自己的sentinel节点. 如果是非空列表, 那么sentinel节点和真实的节点就构成了一个最简单的二元循环体. 针对列表末尾位置的操作双端链表结构优雅，虽然某些操作如addFirst等编码复杂度会提高, 但不影响速度. 更重要的是, 相比单向链表, 它反而使得addLast, moveLast等方法的代码实现变得简单了, 而且还进一步提升了运行速度(从O(n)到O(c)).12345678910111213141516171819202122/** Adds an item to the back of the Deque - O(c) */public void addLast(GType x)&#123; OneNode oldBackNode = sentinel.prev; OneNode newNode = new OneNode(oldBackNode, x, sentinel); sentinel.prev = newNode; oldBackNode.next = newNode; size += 1;&#125;/** Removes and returns the item at the front of the Deque. * If no such item exists, returns null.O(c). */public GType removeFirst()&#123; if (isEmpty())&#123; return null; &#125; OneNode oldFrontNode = sentinel.next; sentinel.next = oldFrontNode.next; oldFrontNode.next.prev = sentinel; size -= 1; return oldFrontNode.item;&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 08 单向链表 Singly Linked List - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-08-java-singly-linked-list%2F</url>
    <content type="text"><![CDATA[链表Linked List 前面有介绍以array为基础搭建的列表，支持自动扩容, 各种插入，删除速度都很快. 这里再介绍另一种方案, 链表, 也可以实现列表自动扩容. 带链接的节点链表的核心组成是带链接的节点, 每个节点就像火车车厢, 有钩子连接下一节车厢.以int节点为例:123456789public class IntNode &#123; public int item; public IntNode next; public IntNode(int i, IntNode n) &#123; item = i; next = n; &#125;&#125; next就是这个链接, 每一个节点就是其上一个节点的next. 嵌套类Nested static class 这个节点作为一个相对独立的数据结构, 我们更希望让他单独作为一个类来维护. 再另外创建一个名为LinkedList的class与用户进行交互. 这样还有另一个好处就是提供一个命名为LinkedList的类给用户交互，用户更直观地知道自己是在调用链表。如果直接与node类交互，用户可能会困扰. 但同时考虑到这个node类只有LinkedList会调用，所以我们可以把node类嵌套进LinkedList中，也就是嵌套类，在类中定义类。1234567891011121314151617181920public class LinkedList&lt;XXX&gt; &#123; private class OneNode &#123; public XXX item; public OneNode next; public OneNode(XXX i, OneNode n) &#123; item = i; next = n; &#125; &#125; private OneNode first; private int size; public LinkedList(XXX x) &#123; first = new OneNode(x, null); size = 1; &#125; //下面是各种方法...&#125; 以上定义使用了泛型。声明OneNode实例first为私有变量, 是为了防止用户错误地摆弄链接指向，private和public的使用参考. 静态与非静态嵌套类123456789class OuterClass &#123; ... static class StaticNestedClass &#123; ... &#125; class InnerClass &#123; ... &#125;&#125; 如果嵌套类不需要使用外部类的任何实例方法或变量，那可以声明嵌套类为static。像静态类方法一样， 静态嵌套类不能直接引用其外部类中定义的实例变量或方法。外部类不能直接访问静态嵌套类的成员变量，要通过静态嵌套类来访问。 非静态嵌套类一般叫做内部类 inner class，可以直接访问外部类的方法和变量。一个内部类的实例作为成员存在于其外部类的实例中。因为内部类与一个实例相关联，所以它不能自己定义任何静态成员。1234567891011public class Outer &#123; public int outVar; public class Inner &#123; public int inVar; &#125;&#125;public static void main(String[] args) &#123; Outer O = new Outer(); Outer.Inner I = O.new Inner();&#125; 作为OuterClass的成员，嵌套类可以声明为private，public，protected或package private。外部类只能声明为public或package private。更多详情参考官网. 补充必要的实例方法插入的操作核心是改变链接指向， 比如原来是A-&gt;B-&gt;D, 要插入C, 则把C.next指向D,然后把B.next改为指向C, 变为A-&gt;B-&gt;C-&gt;D1234567891011121314151617181920212223242526272829303132333435363738394041public class LinkedList&lt;XXX&gt; &#123; private class OneNode &#123; ... &#125; private OneNode first; private int size; public LinkedList(XXX x) &#123; ... &#125; /** 在列表开头插入 x. */ public void addFirst(XXX x) &#123; first = new OneNode(x, first); size += 1; &#125; /** 返回列表第一个元素. */ public XXX getFirst() &#123; return first.item; &#125; /** 在列表末尾插入 x. */ public void addLast(XXX x) &#123; size += 1; OneNode p = first; /* 把 p 当做指针顺藤摸瓜一直挪到列表末尾. */ while (p.next != null) &#123; p = p.next; &#125; p.next = new OneNode(x, null); &#125; /** 删除列表末尾的元素. */ public void removeLast()&#123; //自行补充... &#125; public int size() &#123; return size; &#125;&#125; 可以看到，如果用户不小心把某节点x指回自己x.next=x,那就会进入死循环，所以我们需要把OnoNode实例first声明为私有变量已提供必要的保护。 超载Overloading 如果想初始化一个空列表, 可以:12345/** 构造一个空列表. */public LinkedList() &#123; fist = null; size = 0;&#125; 即使原来已经有一个带参数x的构造器了, 这里再加一个同名构造器也没问题. 因为Java允许有不同参数的方法重名, 即超载 overloading. 程序不变条件Invariants 上面超载了一个初始化空列表的构造器, 加入初始化一个空列表，然后直接调用addLast，程序会报错, 因为null没有next. 有几种修改方法, 比如用if else这种加特例的方法. 这个方案虽然可以能解决问题，但是应尽量避免加入特例代码。毕竟有特例就意味着增加了复杂度和额外的代码特例记忆需求, 而人记忆是有限的. 一个更简洁（尽管不太显而易见）的解决方案是修改数据结构本身，让所有LinkedList，维护起来都没有差别，即使是空的。如果把列表比做拉货的火车，那么货物就是列表承载的数据。一列火车如果只有车厢而没有车头（或者车尾）的话是没有意义的，因为没有动力。所以不管火车有没有拉货，有车厢还是没车厢，要称之为火车我们至少需要一个火车头，通过创建一个特殊节点 - 前哨节点 sentinel。前哨节点将保存一个值，具体数值我们不关心，它只是作为火车头，不装货。所以我们要修改LinkedList为：12345678910111213141516171819202122/* 第一个元素 （假如有的话）就是 sentinel.next. */public class LinkedList&lt;XXX&gt; &#123; private class OneNode &#123; //... &#125; private OneNode sentinel; private int size; /** 构造一个空列表. */ public LinkedList() &#123; sentinel = new OneNode(null, null); size = 0; &#125; /** 构造一个初始元素为x的列表. */ public LinkedList(XXX x) &#123; sentinel = new OneNode(null, null); sentinel.next = new OneNode(x, null); size = 1; &#125;&#125; 对于像LinkedList这样简单的数据结构来说，特例不多，我们也许可以hold住, 一旦后续遇到像树tree等更复杂的数据结构，控制特例数量就显得极为重要了。所以现在就要培养自己的这方面的习惯，保持程序不变条件成立。所谓 invariants 就是指数据结构任何情况下都是不会出错（除非程序有bug）. 具有前哨节点的LinkedList至少具有以下 invariants： 列表默认存在前哨节点。 列表第一个元素（如果非空的话）总是在sentinel.next.item。 size变量始终是已添加的元素总数。 不变条件使得代码的推敲变得更加容易，同时给程序员提供了能够确保代码正常工作的具体目标。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Bash 直接启动 sublime 或 atom 等编辑器以打开或新建文件]]></title>
    <url>%2FLaunch-editor-in-Gitbash%2F</url>
    <content type="text"><![CDATA[程序员或者其他需要码字多的人，经常要使用编辑器如sublime、atom 和 Typora等。如果每次都要用鼠标点击才能用sublime打开文件，或者在编辑器中新建文件，那么就会有点麻烦！但你可以用一句命令解决！ 配置在Git Bash中用各种文本编辑器打开文件或者直接新建文件。这里以atom为例。 常规步骤 打开Git Bash并cd到你的目标文件夹, 或者直接在目标文件中右键打开Git Bash. atom xxx.md 就会在弹出的atom窗口中打开名为xxx.md的markdown文件, 如果没有这个文件, 会自动创建一个. 适用于其他类型文件, 如.java等. 如果想用sublime, 可以用subl xxx.java, 同理notepad++ 可以用 notepad++ xxx.java等。 (若出现错误,看下面) 若系统无法识别命令一般使用sublime或者notepad++的用户, 可能会出现error: 系统无法识别命令...之类的, 可以这么解决: 方法1新建一个文件命名为subl（注意不能有后缀名），内容：12#!/bin/sh&quot;D:\Sublime Text 3\sublime_text.exe&quot; $1 &amp; 第一行指明这是个 shell 脚本.第二行的字符串是sublime的安装目录, 示例只是我电脑的目录, 注意这里要改为你自己的目录,第二行的$1 是取的命令之后输入的参数第二行的&amp;是此命令在后台打开，这样sublime打开之后，就不会阻塞你的git bash 文件保存到 C:\Program Files (x86)\Git\mingW32\bin 目录下(你的git目录可能与我的不一样，注意改成你自己的) 同理适用于其他编辑器，比如用chrome打开.html文件等。如果不想每次都新建一个文件，可以用下面的方法2。 方法2 找到 C:\Users\你的计算机名目录，如果你的计算机名是Administrator，那么你就要去C:\Users\Administrator目录下, 这里一般存放着windows系统的我的文档, 桌面等文件夹. 在该目录下用Git Bash输入notepad .bashrc, 这会用windows记事本新建并打开一个文件.bashrc，这个文件没有名称只有后缀名。.bashrc里面可以给Git Bash设置命令的别名, 设置路径等。 在.bashrc文件加入下面一行文本alias notepad++=&quot;/D/Notepad++/notepad++.exe&quot;, 这里你需要修改为你电脑的安装路径。alias就是别名的意思，当我们执行notepad++的时候，实际执行的是=后面的语句. 重新打开Git Bash, 设置才能生效，如果不想关掉在打开的话，可以直接在bash下输入source ~/.bashrc就可以立刻加载修改后的设置，设置立即生效。现在在bash下输入notepad++ test.py, 就直接打开了notepad++并创建了这个叫test的Python文件。这里的别名不一定非要取notepad++，随你想叫什么都行。 同理也可以扩展到别的文本编辑器，alias atom=&quot;atom的路径&quot;, alias sublime=&quot;sublime的路径&quot;等. 最后还要注意一点，上面所说的路径最好不要有空格，括号等，否则会造成命令无效. .bashrc还有很多有用的配置,可以根据需要进行扩展. 比如很多程序猿会选择修改删除命令rm(此命令不加任何参数的话，会直接删除文件, 可能会造成误删的后果)。这个时候可以给rm加个参数-i，意为在删除的时候给出提示。在文件.bashrc里添加这行代码alias rm=&quot;rm -i&quot;。但这里不建议这么做，因为rm=&quot;rm -i&quot;是一个定时炸弹，在使用它之后，习惯了之后, 你会本能地期望rm在删除文件之前会提示你。但是，总有一天你可能会用一个没有rm alias 别名的系统, 这时若你也直接随手一甩rm, 本以为会有提示, 结果发现数据真的被删除了。 在任何情况下，预防文件丢失或损坏的好方法就是进行备份。 所以如果你想个性化删除命令, 最好不要动rm，而是创建属于你的命令，比如trash, myrm, delete等, 用alias trash=&#39;/bin/rm -irv&#39;会创建一条把文件放入垃圾回收站的命令.]]></content>
      <categories>
        <category>提高效率</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>Git Bash</tag>
        <tag>Sublime</tag>
        <tag>Atom</tag>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 07 用数组构建数据列表 list - CS61B Berkeley - Josh Hug - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-07-java-array-based-list%2F</url>
    <content type="text"><![CDATA[列表（List）前面说到Java的数组无法更改长度，那么也就无法实现插入或者删除数组成员。Java提供了功能更丰富的数据结构 - 列表（list）。所谓列表，即有序的集合（序列），用户可以精确地控制每个元素插入到列表中的哪个位置。用户可以通过整数索引（列表中的位置）来访问元素，并搜索列表中的元素（详细可进一步参考oracle官网）。这里我们尝试以java的array为基础实现一个列表，目标是实现自动扩容 (Java中的ArrayList不仅仅有自动扩容, 也继承了[List]的其他功能)。在探索的过程中, 可以顺带学习很多相关的内容.使用自上而下的设计思想搭建一个框架:先写出最基础的部分, 也就是一个构造器，前面学过了整数数组，我们直接拿来用123456789101112131415161718/** Array based list. */// index 0 1 2 3 4 5 6 7// items: [6 9 -1 2 0 0 0 0 ...]// size: 5public class AList &#123; private int[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() &#123; items = new int[100]; size = 0; &#125; /** 下面添加其他方法 */&#125; 然后思考我们需要什么功能，把功能需求转化为实例方法instance method的形式，先把方法的外壳描绘出来，注释上该方法的功能（目的），输入值，返回值是什么之类的。具体的功能实现可以先空着，之后一步步丰富。 公共与私有Public vs. Private 在上面的代码块中，可以看到 items 和 size 都被声明为 private 私有变量, 这样就只能被所在的java文件内调用. 私有变量和方法的设计初衷是服务于程序的内部功能实现, 而不是用来和外部程序(用户)进行交互的. 设置成私有, 可以避免这些变量和方法被外部程序直接调用, 避免用户通过不恰当/容易出错的方式修改某些变量. 在程序说明文档中, 一般也会明确说明程序提供什么公共变量和方法给用户调用. 因此我们这里也提供几个 public 方法让用户调用, 这样用户就能按照我们设计的方式来访问数据。分别是getLast() - 访问列表最后一个元素，get(int i)访问第i个元素, 和size()访问列表的大小.12345678910111213141516/** 程序内的方法可以访问 private 变量 *//** 返回列表末尾的值. */public int getLast() &#123; return items[size - 1];&#125;/** 返回第 i 个值 (0 是第一个). */public int get(int i) &#123; return items[i];&#125;/** 返回列表元素长度. */public int size() &#123; return size;&#125; 泛型数组我们不仅希望我们的列表可以存整数，也可以存其他类型的数据，可以通过泛型解决，泛型的介绍参考这篇文章. 泛型数组跟前面介绍的泛型示例有一个重要的语法差异：Java不允许我们创建一个通用对象的数组，原因这里不细展开。 假如我们用Item来标识泛型, 那么在上面的列表类中构建泛型数组时, 我们不能用items = new Item[8];, 而要用items = (Item []) new Object[8];, 即使这样也会产生一个编译警告，但先忍着, 后面会更详细地讨论这个问题。12345678910public class AList&lt;Item&gt; &#123; private Item[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() &#123; items = (Item[]) new Object[100]; //会有编译警告, 暂时不管 size = 0; &#125;&#125; 数组扩容Resize 一个列表应该支持基本的插入和删除数据的操作，但是因为数组本身无法更改长度，所以我们就需要一个方法，在给数组插入新数据时，先检查长度容量是否足够，如果不够，那么就要增加长度。我们考虑简单的情况, 即需要在数组末尾插入或者删除数据怎么办。 插入元素：123456789101112/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; /** 检查长度容量是否足够，如果不够，那么就要增加长度 */ if (size == items.length) &#123; Item[] temp = (Item[]) new Object[size + 1]; System.arraycopy(items, 0, temp, 0, size); items = temp; &#125; items[size] = x; size = size + 1;&#125; 创建新array并把旧数据复制过去的过程通常称为“resizing”。其实用词不当，因为数组实际上并没有改变大小，只是把小数组上的数据复制到大数组上而已。 为了让代码更易于维护，可以把上面的代码中负责大小调整的部分包装在一个独立的method中12345678910111213141516/** 改变列表容量, capacity为改变后的容量. */private void resize(int capacity) &#123; Item[] temp = (Item[]) new Object[capacity]; System.arraycopy(items, 0, temp, 0, size); items = temp;&#125;/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; if (size == items.length) &#123; resize(size + 1); &#125; items[size] = x; size = size + 1;&#125; 删除元素：1234567/** 删去列表最后一个值，并返回该值 */public int removeLast() &#123; Item x = getLast(); items[size - 1] = null; // 曾经引用“删除”的元素的内存地址被清空 size = size - 1; return x;&#125; 事实上即使没有items[size - 1] = null;,也可以达到删除元素的目的.删除对存储对象的引用, 是为了避免“loitering”。所谓 loitering，可以理解为占着茅坑不拉屎的对象，它们已经没啥用了，却还是占用着内存。如果这个对象是些几十兆的高清图片，那么就会很消耗内存。这也是为什么安卓手机越用越慢的一个原因。 当引用/内存地址丢失时，Java会销毁对象。如果我们不清空引用，那么Java将不会垃圾回收这些本来预计要删除的对象, 因为它们实际还被列表引用着。 扩容效率分析我们直觉也会感觉到，如果按照现在的设计，即每插入一个新元素，就重新复制一遍数组，这样随着数组越来越大，效率肯定会越来越差。事实上也是这样，如果数组目前长度是100个内存块，那么插入1000次，需要创建并填充大约50万个内存块（等差数列求和N(N+1)/2，101+102+…+1000 ≈ 500000）。但假如我们第一次就扩容到1000，那么就省却了很多运算消耗。可惜我们不知道用户需要插入多少数据，所以要采取其他方法-几何调整。也就是与其按照size + FACTOR这样的速率增加容量, 不如按照size * RFACTOR成倍扩容, 前者的增加速率为1, 后者为 RFACTOR, 只要设置 RFACTOR 大于1, 就能减少扩容的次数.123456789/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; if (size == items.length) &#123; resize(size * RFACTOR); //用 RFACTOR 作为因子扩容数组, &#125; items[size] = x; size = size + 1;&#125; 目前我们解决了时间效率问题, 但代价是需要更大的内存空间, 也就是空间效率下降了. 假设我们插入了十亿个item，然后再删去九亿九千万个项目。在这种情况下，我们将只使用10,000,000个内存块，剩下99％完全没有使用到。 为了解决这个问题，我们可以在数组容量利用率比较低时把容量降下来. 定义利用率 R 为列表的大小除以items数组的长度。一般当R下降到小于0.25时，我们将数组的大小减半。 其他功能比如排序等, 在后面介绍链表的文章中再讨论.]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 06 array 数组 - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-06-java-array%2F</url>
    <content type="text"><![CDATA[数组（Array）数组是一种特殊的对象，有一个固定的数组长度参数N，由一连串（N个）连续的带编号的内存块组成，每个都是相同的类型(不像Python可以包含不同类型)，索引从0到N-1编号。A[i]获得数组A的第i个元素。这与普通的类实例不同，类实例有具体变量名命名的内存块。 数组实例化，包含对象的数组Array Instantiation, Arrays of Objects 要创建最简单的整数数组, 有三种方式:123x = new int [3]; //创建一个指定长度的数组，并用默认值（0）填充每个内存块。y = new int [] &#123;1，2，3，4，5&#125;; //创建一个合适大小的数组，以容纳指定的初始值int [] z = &#123;9，10，11，12，13&#125;; //省略了new，只能结合变量声明使用。 创建一组实例化对象:12345678910public class DogArrayDemo &#123; public static void main(String[] args) &#123; /* Create an array of two dogs. */ Dog[] dogs = new Dog[2]; dogs[0] = new Dog(8); dogs[1] = new Dog(20); /* Yipping will result, since dogs[0] has weight 8. */ dogs[0].makeNoise(); &#125;&#125; 注意到new有两种不同的使用方式：一种是创建一个可以容纳两个Dog对象的数组，另外两个创建各个实际的Dog实例。 数组复制123x = new int[]&#123;-1, 2, 5, 4, 99&#125;;int[] b = &#123;9, 10, 11&#125;;System.arraycopy(b, 0, x, 3, 2); //效果类似于Python的`x[3:5] = b[0:2]` System.arraycopy的五个参数分别代表： 待复制的数组(源) 源数组复制起点 目标数组 目标数组粘贴起点 有多少项要复制 2D数组Java的二维数组实质上是一数组的数组, 即每一个数组元素里面也是一个数组。1234567891011121314151617int[][] matrix; //声明一个引用数组的数组matrix = new int[4][]; //创建四个内存块, 用默认null值填充, 之后用于储存对整数数组的引用, 即地址,int[] rowZero = matrix[0];/** 实例化整数数组, 把其地址/引用分别赋值给/储存到 matrix 的第N个内存块*/matrix[0] = new int[]&#123;1&#125;;matrix[1] = new int[]&#123;1, 1&#125;;matrix[2] = new int[]&#123;1, 2, 1&#125;;matrix[3] = new int[]&#123;1, 3, 3, 1&#125;;int[] rowTwo = matrix[2];rowTwo[1] = -5;/** 创建四个内存块, 其中每个被引用的整数数组长度为4,每个元素都是0.*/matrix = new int[4][4];int[][] matrixAgain = new int[][]&#123;&#123;1&#125;, &#123;1, 1&#125;,&#123;1, 2, 1&#125;, &#123;1, 3, 3, 1&#125;&#125;;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 05 数据类型 - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-05-java-variable-types%2F</url>
    <content type="text"><![CDATA[数据类型数据类型是程序设计语言描述事物、对象的方法。Java数据类型分为基本类型（内置类型）和引用类型(扩展类型）两大类。基本类型就是Java语言本身提供的基本数据类型，比如，整型数，浮点数，字符，布尔值等等。而引用类型则是Java语言根据基本类型扩展出的其他类型，Java要求所有的引用扩展类型都必须包括在类定义里面，这就是Java为什么是面向对象编程语言的原因…上面的定义有点抽象，要理解数据类型，需要先理解一个问题: 神秘的海象问题 尝试预测下面的代码运行时会发生什么。b的变化是否会影响a？提示：类似Python。123456Walrus a = new Walrus(1000, 8.3);Walrus b;b = a;b.weight = 5;System.out.println(a);System.out.println(b); 同样尝试预测下面的代码运行时会发生什么。x的改变是否影响y？123456int x = 5;int y;y = x;x = 2;System.out.println("x is: " + x);System.out.println("y is: " + y); 答案是b的变化会影响a, 但x的改变不影响y，具体见可视化过程.这里的差别虽然微妙, 但其背后的原理对于数据结构的效率来说是非常重要的，对这个问题的深入理解也将引导我们写出更安全，更可靠的代码。 基本类型Primative Types 计算机中的所有信息都以一系列1和0的形式存储在内存中，这些二进制的0和1就是比特位（bits）。比如72和“H”在内存一般以01001000的形式存储，对他们的形式是一样的。一个引申问题就是：Java代码如何解释01001000，怎么知道应该解释为72还是“H”？ 通过类型types，预先定义好类型即可, 以下代码1234char x = 'H';int y = x;System.out.println(x);System.out.println(y); 会分别得到“H”和72. 在这种情况下，x和y变量都包含几乎相同的bits，但是Java解释器在输出时对它们进行了不同的处理。 Java有8种基本类型：byte，short，int，long，float，double，boolean和char。 变量声明Declaring Variables 计算机的内存可以视为包含大量用于存储信息的内存比特位，每个位都有一个唯一的地址。现代计算机可以使用许多这样的位。 当你声明一个特定类型的变量时，Java会用一串连续的内存位存储它。例如，如果你声明一个int，你会得到一个长度32的内存list，里面有32bits。Java中的每个数据类型都有不同的比特数。 除了留出内存空间外，Java解释器还会在一个内部表中创建一个条目，将每个变量名称映射到内存块中第一个位置（表头list head）。 例如，如果声明了int x和double y，那么Java可能会决定使用计算机内存的352到384位来存储x，而20800到20864位则用来存储y。然后解释器将记录int x从352开始，y从20800开始。 在Java语言里无法知道变量的具体内存位置，例如你不能以某种方式发现x在位置352。不像C++这样的语言，可以获取一段数据的确切地址。Java的这个特性是一个折衷！隐藏内存位置自然意味着程序猿的控制权更少，就无法做某些类型的优化。但是，它也避免了一大类非常棘手的编程错误。在现在计算成本如此低廉的时代，不成熟的优化还不如少点bug。 当声明一个变量时，Java不会在预留的内存位置中写入任何内容, 也即没有默认值。因此，如果没有赋值, Java编译器会阻止你使用变量。 以上只是内存分配的简要说明, 堆和栈的介绍可以参考 CS106B 笔记。 引用类型Reference Types 所有基本数据类型之外的类型都是引用类型。引用类型顾名思义，就是对对象的引用。在java中内存位置是不开放给程序员的, 但我们可以通过引用类型访问内存中某处对象。所有引用类型都是 java.lang.Object 类型的子类。 对象实例化Object Instantiation 对象实例化：当我们使用new（例 new Dog）实例化对象时，Java首先为类的每个实例变量分配一串长度合适的bits位，并用缺省值填充它们。然后，构造函数通常（但不总是）用其他值填充每个位置.123456789public static class Walrus &#123; public int weight; public double tuskSize; public Walrus(int w, double ts) &#123; weight = w; tuskSize = ts; &#125;&#125; 用new Walrus(1000, 8.3)创建一个Walrus实例后, 我们得到分别由一个32位(int weight = 1000)和一个64位(double tuskSize = 8.3)的内存块组成的实例：通过程序可视化过程)来更好地理解. 当然在Java编程语言的实际实现中，实例化对象时都有一些额外的内存开销, 这里不展开. 通过 new 实例化对象，new 会返回该对象的内存地址给我们，但假如我们没有用一个变量去接收这个地址，那么我们就无法访问这个对象。之后该对象会被作为垃圾回收。 引用变量声明Reference Variable Declaration 前面有提到，我们需要声明变量来接受实例化的对象在内存中的地址。当声明任何引用类型的变量（比如array, 前面的Dog类等）时，Java都会分配一串64位的内存位置. 这个64位的内存块仅用于记录变量的内存地址, 所谓内存地址, 可以理解为内存(房子)的编号(地址), 一般是内存块的表头位置的64位表达式1234Walrus someWalrus; // 创建一个64位的内存位置someWalrus = new Walrus(1000, 8.3); //创建一个新的实例/** 内存地址由 new 返回, 并被复制/赋值给 someWalrus 对应的内存位置*/ 比如, 假设weight是从内存位5051956592385990207开始存储的，后面连续跟着其他实例变量，那么就可以把5051956592385990207存储在someWalrus变量中。5051956592385990207由64位的二进制0100011000011100001001111100000100011101110111000001111000111111表达，这样someWalrus的内存就可以抽象的理解为一个表someWalrus: 0100011000011100001001111100000100011101110111000001111000111111 -&gt; 具体存放实例的内存(Walrus: weight=1000, tuskSize=8.3)‘-&gt;’可以理解为指针. 前面有提到，如果丢失了引用变量存储的内存地址，那么该地址对应的对象就找不回来了。例如，如果一个特定的 Walrus 地址的唯一副本存储在x中，那么x = null这行代码将删去地址，我们则丢失了这个 Walrus 对象。这也不一定是坏事，很多时候在完成了一个对象后就不在需要了，只需简单地丢弃这个参考地址就可以了。 等值规则Java Rule of Equals 对于y = x，Java解释器会将x的位拷贝到y中,这个规则适用于java中任何使用=赋值的语法, 是理解开头的”神秘的海象”问题的关键. 基本类型变量的位, 存储赋值的值（基本类型）在内存中值(具体位数取决于具体的类型) 1234int x = 5; // 此时是把内存中的某一个地址 p 复制给 xint y;y = x; // y 也指向 px = 2; // 把一个新的内存地址 new p 复制给x, 但y还是指向原来的p x的位存储的是基本类型int 5(32 bits), x = 2是把新的基本类型int 2复制给x, 但y还是指向原来的int 5， 所以y没变化。 引用类型 reference type 变量的位, 存储赋值的值（引用类型）在内存中的地址(固定的64 bits) 1234Dog a = new Dog(5); // 创建一个64位的内存位, 并赋值一个新的实例 pDog b; // 仅创建一个64位的内存位, 没有引用内存地址(null)b = a; // 把a的位（是实例 p 的内存地址）复制给b, 这样 b 也是指向实例 pb.weight = 21; // 此时修改b, 会改写b指向的内存实例 p a和b只存储地址, 而它们的地址都指向相同的实例； 如果对 b 的修改本质是对 p的修改, 那么输出a.weight的时候, 就会变成21. 参数传递Parameter Passing 给函数传递参数，本质上也是赋值操作，参考上面的等值规则，也即复制这些参数的bits给函数，也称之为pass by value。Java的参数传递都是pass by value。至于传递过去的参数会不会因为函数内部的操作而更改，其判断原理在上面的等值规则已经阐明。 通用数据类型Generic 在定义类的时候，有时候我们可能希望这个类能够接受任何类型的数据，而不仅仅是限定了基本类型中的任何一种。比如我们想实现一个类似excel表格的类，自然需要这个表格类能够接收各种类型的字符，数字，并呈现出来。这个时候就需要使用泛型 Generic, 也即通用数据类型。 在2004年，Java的设计者在语言中加入了泛型，使​​我们能够创建包含任何引用类型的数据结构。方法就是在类声明的类名后面，使用一个任意的占位符，并用尖括号括住&lt;随便什么字符&gt;。然后，在任何你想使用泛型的地方，改用占位符。比如1234567public class table &#123; public class table &#123; public int item; ... &#125; ...&#125; 改为1234567public class table&lt;xxx&gt; &#123; public class table &#123; public xxx item; ... &#125; ...&#125; &lt;xxx&gt;里面的名称并不重要, 改成其他也行, 只是一个标识符, 用来接受参数, 当用户实例化这个类时, 必须使用特殊的语法table&lt;String&gt; d = new table&lt;&gt;(&quot;hello&quot;); 由于泛型仅适用于引用类型，因此我们不能将基本类型int等放在尖括号内。相反，我们使用基本类型的引用版本，比如对于int, 用 Integer，table&lt;Integer&gt; d = new table&lt;&gt;(&quot;10&quot;); 总结使用方法: 在一个实现某数据结构的.java文件中，在类名后面, 只指定泛型类型一次。 在其他使用该数据结构的java文件中，声明实例变量时要指定所需的类型。 如果您需要在基本类型上实例化泛型，请使用Integer, Double, Character, Boolean, Long, Short, Byte, Float，而不是其基本类型。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 04 类 class 02 类与实例 - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-04-java-class-02-class-instance%2F</url>
    <content type="text"><![CDATA[Class前面提到，类的方法和变量细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。 类方法与实例方法Class Methods vs. Instance Methods 参考上一篇文章的例子，类方法由类调用Dog.makeNoise();. 实例方法只能由实例调用bigDog.makeNoise();. 同理可推, 类方法无法调用实例变量.可以看到实例方法更具体, 更贴近实体世界, 那我们仍需要类方法, 因为: 有些类不需要实例化, 毕竟我们也经常需要处理抽象的概念, 这些抽象概念在人类认知范畴内是统一的, 比如数学计算, 我们需要计算某个数值的平方根, x = Math.sqrt(100);, 拿来就用, 不需要先实例化. 这点在Python中体现得很好. 有些类有静态方法, 是有实际作用的 - 每个实例都通用的方法。例如，若想比较一个类里面的不同实例, 比如两只狗的重量。比较简单的方法就是使用一个比较狗的重量的类方法: 123456789public static Dog maxDog(Dog d1, Dog d2) &#123; if (d1.weight &gt; d2.weight) &#123; return d1; &#125; return d2;&#125;Dog d = new Dog(15);Dog d2 = new Dog(100);Dog.maxDog(d, d2); 这个时候, 若使用实例方法也可以, 但没那么直观：12345678910/** 我们使用关键字this来引用当前对象d。*/public Dog maxDog(Dog d2) &#123; if (this.weight &gt; d2.weight) &#123; return this; &#125; return d2;&#125;Dog d = new Dog(15);Dog d2 = new Dog(100);d.maxDog(d, d2); 类变量与实例变量Class Variables vs. Instance Variables 静态变量的也是有用处的。这些变量一般是类本身固有的属性。例如，我们可能需要用狗类的另一种生物学的统称“犬科”来作为类的说明， 这个时候可以用public static String binomen = &quot;犬科&quot;;，这个变量理论上是由类来访问的。 虽然Java在技术上允许使用实例名称来访问静态变量，但是这有时候可能会令人困惑， 所以还是少用为好。 构造器（Constructors）与上面的DogLauncher实例化对象的方式相比, 我们更希望实例化可以带参数的，那样可以为我们节省手动给实例变量赋值的麻烦。为了启用这样的语法，我们只需把如下与类名同名的构造函数直接添加进Dog类中：12345/**注意：构造函数与class类同名 */public Dog(int w) &#123; weight = w;&#125; 然后在DogLauncher里实例化一只狗时, 直接Dog d = new Dog(20);即可. 在以上代码的基础上, 后续当我们想使用new和参数创建一只狗时，可以随时调用public Dog(int w)构造函数。对于熟悉Python的人来说，你可以理解java的构造函数为Python的__init__。 一些术语: 声明(declaration): Dog smalldog;声明一个类作为一个变量在内存中占位 实例化: new Dog(20), 如果没有把它作为值赋给一个类声明变量,那么这个实例化的值会被垃圾回收. 声明, 实例化并赋值: Dog smalldog = new Dog(5) 构造器并不总是必须的。编译器会自动为没有构造器的类提供一个无参数的默认构造函数。这个默认构造函数将调用其超类的（可调用的）无参构造函数。 如果其超类没有无参数构造函数，编译会出错。 如果没有显式的超类，那么就调用隐式的超类Object的无参构造函数。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 04 类 class 01 变量和方法 - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-04-java-class-01-intro%2F</url>
    <content type="text"><![CDATA[ClassJava的语法是为了更容易地模拟真实世界而设计的. 比如用程序实现一只狗, 可以用定义一个类class来描述它. 类class里面包括变量Variable，方法method（可以理解为Python的函数function）。变量可以储存数据，方法可以处理数据。变量必须在类中声明(即不能离开类独立存在)，不像Python或Matlab这样的语言可以在运行时添加新的变量。类的方法和变量又细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。实例instance的概念后面会解释。 类（静态）变量与方法Class(Static) Variables and Methods 静态变量和方法的特征就是有static字符在前面.以下代码定义了一个类来模拟狗，包含一个类变量作为这个类的说明，一个类方法用于发出叫声：12345678public class Dog &#123; public static String instruction = "狗类实例"; //类变量, 说明 public static void makeNoise() &#123; System.out.println("汪!"); &#125;&#125; 这里没有定义main(), 在这种情况下如何直接运行这个类(java Dog), 程序是会报错的123错误: 在类 Dog 中找不到 main 方法, 请将 main 方法定义为: public static void main(String[] args)否则 JavaFX 应用程序类必须扩展javafx.application.Application. 你可以选择在里面添加一个main()方法. 但这次我们选择不定义具体的main(). 具体要如何运行, 我们可以另写一个类定义一个main()方法来调用这个类.12345public class DogLauncher &#123; public static void main(String[] args) &#123; Dog.makeNoise(); &#125;&#125; 这两种方式(在类A内部定义好main()vs. 在其他类B定义main()来调用A)没有优劣之分, 二者有不同的适用情况. 随着不断深入学习，二者的区分将变得更清晰。 注意到, 类变量和方法是有局限性的。现实世界中, 并不是所有的狗都是一样的特征，仅仅靠类这个概念是无法区分不同个体的狗, 除非你为不同的狗定义不同的类（以及里面的变量和方法）, 那么就会很繁琐痛苦. 也就是说，用类来模拟个体是低效的，我们要使用实例. 实例变量与对象实例化Instance Variables and Object Instantiation. Java的类定义就像定义一张蓝图, 我们可以在这个蓝图的基础上, 生成不同的实例instance. 实例是概念性的说法，本质上在Java里就是对象object。这样的特性提供了一个很自然而然地在java中模拟生成实体世界的方法：定义一个狗的类，在这个类的基础上，通过不同的特征参数实例化不同特征的狗（instances），并使类方法的输出取决于特定实例的狗的属性。1234567891011121314/** 一只狗的类:*/public class Dog &#123; public int weight; public void makeNoise() &#123; if (weight &lt; 10) &#123; System.out.println("嘤嘤嘤!"); &#125; else if (weight &lt; 30) &#123; System.out.println("汪汪汪"); &#125; else &#123; System.out.println("嗷呜!"); &#125; &#125;&#125; 这里的方法和变量没有static, 所以是实例（非静态）方法和变量. 如果直接用 Dog 类来调用这些方法, 会报错:123456public class DogLauncher &#123; public static void main(String[] args) &#123; Dog.weight = 21; Dog.makeNoise(); &#125;&#125; 123456DogLauncher.java:3: 错误: 无法从静态上下文中引用非静态 变量 weight Dog.weight = 21; ^DogLauncher.java:4: 错误: 无法从静态上下文中引用非静态 方法 makeNoise() Dog.makeNoise(); ^ 这个时候, 你需要实例化一只狗, 让这个实例来调用非静态变量和方法:1234567public class DogLauncher &#123; public static void main(String[] args) &#123; Dog bigDog = new Dog(); bigDog.weight = 5; bigDog.makeNoise(); &#125;&#125; 运行时，这个程序将会创建一个重量为5的狗，这个狗就会“嗷呜”叫。 总的来说，之所以需要实例方法和变量，是因为我们需要模拟个体，一只具体的狗，并让它发出声音。这个weight和makeNoise()只能由具体的狗调用。狗类不能调用，也没有调用的意义, 毕竟每只狗的重量和声音都不同的. 在设计程序时, 如果其中一个方法我们只打算让特定的实例来调用它(而不让类去调用它), 那么这个方法应该设计成实例方法。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 03 代码风格 注释 Javadoc - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-03-java-code-style-comments%2F</url>
    <content type="text"><![CDATA[代码风格与注释Code style and comments 在学习和实践过程中，我们应该努力保持代码可读性。良好的编码风格的一些最重要的特点是： 一致的风格（间距，变量命名，缩进风格等） 大小（线不太宽，源文件不要太长） 描述性命名（变量，函数，类），例如变量或函数名称为年份或getUserName而不是x或f。让代码本身提供可解读性。 避免重复的代码：若有两个重要的代码块及其相似，应该想办法合并。 适当的评论, 使其他读者也能轻松理解你的代码 行注释: //分隔符开头行被当做注释。 Block（又名多行注释）注释: /*, */, 但我们更推荐javadoc形式的注释。 JavadocJavadoc: / **，*/, 可以（但不总是）包含描述性标签。 借助javadoc工具可以生成HTML格式的API文档。第一段是方法的描述。描述下面是不同的描述性标签, 比如参数 @param， 返回值 @return， 可能抛出的任何异常 @throws123456789/** * @author 名字，邮箱&lt;address @ example.com&gt; * @version 1.6 版本 * @param * @return */public class Test &#123; // class body&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 02 语法基础 - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-02-java-basic-syntax%2F</url>
    <content type="text"><![CDATA[Java基本语法12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello world!"); &#125;&#125; 上面的程序由一个类声明组成，该声明使用关键字public class声明。 Java所有的代码都应该包含在class里面。 真正负责运行的代码，是一个名为main的method，它声明为public static void main(String[] args)。 public：公共的，大部分方法都是以这个关键字开始的，后面会进一步解释。 static：这是一个静态方法，不与任何特定的实例关联，后面会解释。 void：它没有返回类型。 main：这是方法的名称。 String [] args：这是传递给main方法的参数。 使用大括号{ }来表示一段代码的开始和结束。 声明必须以分号结尾 静态分类Static Typing 程序语言静态与动态的分类，可以参考oracle的说明文件，它解释了动态和静态类型之间的区别, 帮助你理解由程序的错误提示信息。两个主要区别:1. 动态类型语言在运行时执行类型检查，而静态类型语言在编译时执行类型检查。这意味如果以静态类型语言（如Java）编写的脚本包含错误，则在编译错误之前将无法编译. 而用动态类型语言编写的脚本可以编译，即使它们包含会阻止脚本正常运行（如果有的话）的错误。2. 静态类型语言要求你在使用它们之前声明变量的数据类型，而动态类型语言则不需要。考虑以下两个代码示例：123// Javaint num;num = 5; 12# Pythonnum = 5 这两段代码都创建一个名为num的变量并赋值为5. 不同之处在于Java需要将num的数据类型明确定义为int。因为Java是静态类型的，因此它期望变量在被赋值之前被声明。 Python是动态类型的，不需要定义类型, Python根据变量的值确定其数据类型。动态类型语言更加灵活，在编写脚本时可以节省时间和空间。但是，这可能会导致运行时出现问题。例如：123# pythonnumber = 5numbr = (number + 15) / 2 #注意错字 上面的代码本应创建一个值为5的可变数字，然后将其加上15并除以2以得到10. 但是，number在第二行的开头拼写错误。由于Python不需要声明变量，因此会不由分说直接创建一个名为numbr的新变量，并把本应分配给number的值分配给它。这段代码会很顺利编译，但是如果程序试图用number来做某事，程序员假设它的值是10，那么后续就无法产生期望的结果,而且还很难注意到问题。 Java的compiler其中一个关键作用是进行静态类型检查（static type check）。若前面定义了 int x = 0;, 那么后面若给x赋值其他的类型值x = &#39;horse&#39;;, compiler就会报错. 这样就保证了程序不会出现类型错误. 除了错误检查外, static types 也可以让程序媛/猿知道自己处的是什么对象. 总而言之，静态类型具有以下优点： 编译器确保所有类型都是兼容的，这使得程序员更容易调试他们的代码。 由于代码保证没有类型错误，所以编译后程序的用户将永远不会遇到类型错误。例如，Android应用程序是用Java编写的，通常仅以.class文件的形式分发，即以编译的格式。因此，这样的应用程序不应该由于类型错误而崩溃。 每个变量，参数和函数都有一个声明的类型，使程序员更容易理解和推理代码。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 01 安装 - CS61B Berkeley - Josh Hug]]></title>
    <url>%2FNOTE-CS61B-data-structures-01-java-install%2F</url>
    <content type="text"><![CDATA[Hello World本系列是伯克利 Josh Hug 的 cs61b spring 2017 和 cs61b spring 2018 的学习笔记. Lab, homework 和 project 代码实现参考 https://github.com/ShootingSpace/cs61b-data-structures. Java安装与配置安装Java，前往Oracle下载java sdk，我用的是Java SE 8u151/ 8u152 版本。安装sdk时会同时安装sdr。 Windows系统配置: 推荐安装git bash, 一切按照默认安装就好. 更新系统环境变量: 直接在运行中搜索Environment Variables, 选择编辑系统环境变量, 在弹出的框中选择高级-&gt;环境变量, 在弹出的框中系统变量里面 新建变量: 变量名 = JAVA_HOME, 变量值 = 你的jdk路径,如C:\Program Files\Java\jdk1.8.0_151 编辑Path: 在前面加入%JAVA_HOME%\bin;%PYTHON_HOME%;(请注意，不能有空格.) OS X系统配置: 安装Homebrew，一个非常好用的包管理工具。要安装，请在terminal终端输入ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;(注意：在此过程中，可能会提示输入密码。当输入密码时，终端上不会显示任何内容，但计算机还是会记录你的密码的。这是一个安全措施, 让其他人在屏幕上看不到你的密码。只需输入您的密码，然后按回车。) 然后，通过输入以下命令来检查brew系统是否正常工作brew doctor. 如果遇到警告，要求下载命令行工具，则需要执行此操作。请参考这个StackOverflow。 安装git：输入brew install git 安装并配置好java后，测试是否成功:随便在你喜欢的文件夹里新建一个java文件HelloWorld.java12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello world!"); &#125;&#125; 你可以选择用sublime来快速新建文件, 直接在你选择的文件里右键 git bash, 在git bash 里面键入subl HelloWorld.java, 还自动启动sublime并新建一个空白的HelloWorld.java文件, 把上面的代码复制进去并保存即可. (若出现类似提示: 找不到subl command, 解决办法请参考博文在Gitbash中直接启动sublime或atom等编辑器以打开或新建文件 )开始真正的测试。直接在之前打开的git bash中输入: ls, 会看到HelloWorld.java这个文件, ls会列出这个目录中的文件/文件夹 javac HelloWorld.java, 理论上这一步不会有任何输出，有的话可能是设置有问题。现在，如果你继续ls，会看到多了一个HelloWorld.class文件， 这是javac创建的。 java HelloWorld (注意没有.java), 会看到输出Hello World, 表明你的Java设置没有问题]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Note - cs229 - Stanford]]></title>
    <url>%2Fmachine-learning%2F</url>
    <content type="text"><![CDATA[参考CS229: Machine Learning, Stanford 什么是机器学习？目前有两个定义。 亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。 汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：E：经验，即历史的数据集。T：某类任务。P：任务的绩效衡量。若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习“如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。 Supervised LearningLinear Regression Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y Intercept term: to simplify notation, introduce the convention of letting x0 = 1 Cost function J(θ): a function that measures, for each value of the θ’s, how close the h(x(i))’s are to the corresponding y(i)’s Purpose: to choose θ so as to minimize J(θ). Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ). LMS(least mean squares) algorithm: gradient descent learning rate error term batch gradient descent：looks at every example in the entire training set on every step stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according tothe gradient of the error with respect to that single training example only. particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent. The normal equationsperforming the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θj’s, and setting them to zero.To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices Matrix derivatives: the gradient ∇Af(A) is itself an m-by-n matrix, whose (i, j)-element is ∂f/∂Aij Least squares revisited: Given a training set, define the design matrix X to be the m-by-n matrix (actually m-by-n + 1, if we include the intercept term) that contains the training examples’ input values in its rows, let y be the m-dimensional vector containing all the target values from the training set, used the fact that the trace of a real number is just the real number( trace operator, written “tr.” For an n-by-n matrix A, the trace of A is defined to be the sum of its diagonal entries: trA = ΣAii To minimize J, find its derivatives with respect to θ: ∇θJ(θ) = XTXθ − XTy To minimize J, we set its derivatives to zero, and obtain the normal equations: XTXθ = XTy Thus the value of θ that minimizes J(θ) is given in closed form by the equation: θ = (XTX)-1XTy Probabilistic interpretationwhy the least-squares cost function J is a reasonable choice? With a set a probabilistic assumptions, under which least-squares regression is derived as a very natural algorithm. Locally weighted linear regression (LWR) algorithmassuming there is sufficient training data, makes the choice of features less critical. In the original linear regression algorithm, to make a prediction at a query point x (i.e., to evaluate h(x)), we would: Fit θ to minimize Σi(y(i) − θTx(i))2. Output θTx. The locally weighted linear regression algorithm does the following: Fit θ to minimize Σiw(i)(y(i) − θTx(i))2. Output θTx. Here, the w(i)’s are non-negative valued weights Intuitively, if w(i) is large for a particular value of i, then in picking θ, we’ll try hard to make (y(i) − θTx(i))2 small. If w(i) is small, then the error term will be pretty much ignored in the fit. A fairly standard choice for the weights is w(i) = exp(-(x(i)-x)2 / 2τ2 ) if |x(i)-x| is small, then w(i) ≈ 1; if large, then w(i) is small. Hence, θ is chosen giving a much higher “weight” to the (errors on) training examples close to the query point x. The parameter τ controls how quickly the weight of a training example falls off with distance of its x(i), from the query point x; τ is called the bandwidth parameter Classification and logistic regressionLogistic regression logistic function or the sigmoid function: g(z) = (1 + e−z)-1. g(z) tends towards 1 as z → ∞, and g(z) tends towards 0 as z → −∞. derivative of the sigmoid function: g(z)‘ = g(z)(1 - g(z)) endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likelihood: Similar to our derivation in the case of linear regression, we can use gradient ascent to maximize the likelihood. updates will therefore be given by θ := θ + α∇θℓ(θ). (Note the positive rather than negative sign in the update formula, since we’re maximizing,rather than minimizing, a function now.) This therefore gives us the stochastic gradient ascent rule: θj := θj + α(y(i)− hθ(x(i)))x(i)j If we compare this to the LMS update rule, we see that it looks identical; but this is not the same algorithm, because hθ(x(i)) is now defined as a non-linear function of θTx(i). Nonetheless, it’s a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. Is this coincidence, or is there a deeper reason behind this? Check GLM models. Generalized Linear ModelsThe exponential family Bernoulli distributions Gaussianexponential distributions multinomial Poisson (for modelling count-data) beta and the Dirichlet (for distributions over probabilities) Constructing GLMs Ordinary Least Squares Logistic Regression Softmax Regression Softmax RegressionConsider a classification problem in which the response variable y can take on any one of k values, so y ∈ {1, 2, . . . , k}. We will thus model it as distributed according to a multinomial distribution. parameterize the multinomial with only k − 1 parameters, φ1, . . . , φk−1, where φi = p(y = i; φ), and p(y = k; φ) = 1 − Σki=1φi. To express the multinomial as an exponential family distribution, we will definee T(y) ∈ Rk-1： η = [log(φ1/φk),…,log(φk-1/φk)], the ηi’s are linearly related to the x’s. softmax function: a mapping from the η’s to the φ’s: φi = eηi / Σkj=1eηi softmax regression: the model, which applies to classification problems where y ∈ {1, . . . , k}: p(y = i|x; θ) = φi = eθTi x / Σkj=1eθTi x This hypothesis will output the estimated probability that p(y = i|x; θ), for every value of i = 1, . . . , k. parameter fitting: obtain the maximum likelihood estimate of the parameters by maximizing ℓ(θ) in terms of θ, using a method such as gradient ascent or Newton’s method. Naive Bayes classification 朴素贝叶斯以二元分类为例: 根据A和B各自的先验概率和条件概率, 算出针对某一特征事件的后验概率, 然后正则化(正则化后两个后验概率之和为1, 但不影响对事件的触发对象是A或B的判断) Why naïve: 忽略了事件发生的顺序, 故称之为”朴素” Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 详见加速自然语言处理-朴素贝叶斯 Problem: how to deal with continuous values features? Use Gaussian Naive Bayes. Gaussian Naive BayesWith real-valued inputs, we can calculate the mean and standard deviation of input values (x) for each class to summarize the distribution. This means that in addition to the probabilities for each class, we also store the mean μ and standard deviations σ of each feature for each class. The class conditional probability P(x|c) is estimated by probability density of the normal distribution : Algorithm – continuous Xi (but still discrete Y) Train Naïve Bayes (examples) 1234for each class value yk: estimate P(Yk) for each attribute Xi: estimate class conditional mean, variance Classify(xnew): Ynew &lt;- argmax(k) ∏P(xi|Yk)P(Yk) Short: classes with the same distribution Missing data instances in NB Ignore attribute in instance where its value is missing compute likelihood based on observed attribtues no need to “fill in” or explicitly model missing values based on conditional independence between attributes Generative and Discriminative Algorithm: Generative classifiers learn a model of the joint probability, p(x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y|x), and then picking the most likely label y. Discriminative classifiers model the posterior p(y|x) directly, or learn a direct map(hypothesis/functions) from inputs x to the class labels. Generative models advantage: Can be good with missing data, naive Bayes handles missing data good for detecting outliers to generate likely input (x,y). Decision trees 决策树 Algorithm: ID3 algorithm Decision trees with continuous attributes: Create split based on threshold ID3 algorithmRecursive Split( node, {examples} ): 1. A &lt;- the best attribute for splitting the {examples} 2. For each value of A, create new child node 3. Split training {examples} to child nodes 4. For each child node, subset: * If subset is pure - stop * Else: split(child_node, {subset} ) How to decide which attribute is the best to split on: Entropy Entropy Use log2 here is to represent concepts of information - on average how many bits needed to tell X split purity To represent two classes, need one bit “0, 1”, to represent 4 classes, need 2 bits “00, 01, 10, 11” If x is pure(one class only), entropy is 0. Information Gain: Expected drop in entropy after split, Gain( P, C) = Entropy(parent) - Σw*Entropy(children), w is weighted average matrix., A is the split attribute Problems: tend to pick attributes with lots of values, could not generalize well on new data. use GainRation: for attribute A with many different values V, the SplitEntropy will be large, Overfitting in Decision Treesthe tree split too deep to try to classify almost every single sample. As a result the model could not predict new data well. Sub-tree replacement pruning For each node: Pretend remove node + all children from the tree Measure performance on validation set Remove node that results in greatest improvement Repeat until further pruning is harmful Decision boundaryLogistic Regression and trees differ in the way that they generate decision boundaries Decision Trees bisect the space into smaller and smaller regions, Logistic Regression fits a single line/hyperplane to divide the space exactly into two. Random Decision forest Grow K different decision trees: pick a random subset Sr of training examples grow a full ID3 tree (no prunning): When splitting: pick from d&lt;&lt;D random attributes Computing gain based on Sr instead of full set repeat for r =1…K Given a new data point X: classify X using each of the trees T1 …. Tk use majority vote: class predicted most often SVM Intuition: Suppose there is a good hyperplane to seperate data set, h(x)=g(wTx+b), (relation with fully connected layer and activation funciton in DNN). Want functional margin of hyperplane to be large: for dataset (xi,yi), functional margin γi = yi(wTxi+b), if yi=1, need wTxi+b&gt;&gt;0, if yi=-1, need wTxi+b&lt;&lt;0. Thus γi&gt;0 means the classification is correct. Geometric margins: Define the hyperplane as wTx+b=0, the normal of the hyperplane is w/||w||, thus a point A(xi)’s, which represents the input x(i) of some training example with label y(i) = 1, projection on the hyperplane is point B = xi - γi·w/||w||, where γi is xi’s distance to the decision boundary. Thus wT(xi - γi·w/||w||) + b=0 =&gt; γi = (w/||w||)Txi+ b/||w||. More generally, the geometric margin of (w, b) with respect to a training example (xi, yi) is γi = yi· (w/||w||)Txi+ b/||w|| If ||w|| = 1, then the functional margin equals the geometric margin The optimal margin classifier: Given a training set, a natural desideratum is to try to find a decision boundary that maximizes the minimum (geometric) margin, i.e want min(γi) as large as possible. Via some transformation, the object turns to minimize ||w||2, subject to y(i)·(wTxi+b) ≥ 1, Lagrange duality: solving constrained optimization problems. w = Σαiyixi, αi is Lagrange multipliers. Support vector: The points with the smallest margins. The number of support vectors can be much smaller than the size the training set Training: fit our model’s parameters to a training set, and now wish to make a prediction at a new point input x. We would then calculate wTx + b, and predict y = 1 if and only if this quantity is bigger than zero. . In order to make a prediction, we have to calculate it which depends only on the inner product between x and the points in the training set. Moreover, αi’s will all be zero except for the support vectors. Thus, many of the terms in the sum above will be zero, and we need to find only the inner products between x and the support vectors (of which there is often only a small number) in order to make our prediction. The inner product &lt;xi,x&gt; could be replaced by kernel k(xi,x) Kernels Define the “original” input value x as the input attributes of a problem. When that is mapped to some new set of quantities that are then passed to the learning algorithm, we’ll call those new quantities the input features. φ denote the feature mapping, which maps from the attributes to the features. E.g. φ(x) = [x, x^2, x^3] given a feature mapping φ, we define the corresponding Kernel to be K(x, z) = φ(x)Tφ(z) Often, φ(x) itself may be very expensive to calculate (perhaps because it is an extremely high dimensional vector, require memory), K(x, z) may be very inexpensive to calculate. We can get SVMs to learn in the high dimensional feature space given by φ, but without ever having to explicitly find or represent vectors φ(x). E.g. Based on Mercer’s Theorem, you can either explicitly map the data with a φ and take the dot product, or you can take any kernel and use it right away, without knowing nor caring what φ looks like Keep in mind however that the idea of kernels has significantly broader applicability than SVMs. Specifically, if you have any learning algorithm that you can write in terms of only inner products &lt;x, z&gt; between input attribute vectors, then by replacing this with K(x, z) where K is a kernel, you can allow your algorithm to work efficiently in the high dimensional feature space corresponding to K. SVM vs. Logistic regression Logistic regression focuses on maximizing the probability of the data. The further the data lies from the separating hyperplane (on the correct side), the happier LR is. An SVM don’t care about getting the right probability, i.e the right P(y=1|x), but only care about P(y=1|x)/P(y=0|x)≥ c. It tries to find the separating hyperplane that maximizes the distance of the closest points to the margin (the support vectors). If a point is not a support vector, it doesn’t really matter. P(y=1|x)/P(y=0|x) &gt; c, if c=1, that means P(y=1|x) &gt; P(y=0|x), thus y=1, take log of both side, and plug in P(y=1|x) = sigmoid(wTx + b), P(y=0|x)=1-P(y=1|x), recall the sigmoid, we get wTx + b &gt; 0 Underlying basic idea of linear prediction is the same, but error functions differ, the r = P(y=1|x)/P(y=0|x) = exp(wTx + b), different classifiers assigns different cost to r If cost(r)=log(1 + 1/r), this is logistic regression If cost(r)=max(0, 1-log(r))=max(0, 1-(wTx + b)), then SVM Logistic regression (non-sparse) vs SVM (hinge loss, sparse solution) Linear regression (squared error) vs SVM (ϵ insensitive error) K Nearest NeighbourIntuition: predict based on nearby/similar training data. Algorithm: for a test data compute its distance to every training example xi select k closest training instances prediction: For Classification: predict as the most frequent label among the k instances. For regression: predict as the mean of label among the k instances. Choose k large k: everything classified as the most probable class small k: highly variable, unstable decision boundaries affects “smoothness” of the boundary Use train-validation to choose k Distance meansures: Euclidian: symmetric, spherical, treats all dimensions equally, but sensitive to extreme differences in single attribtue Hamming: number of attribtues that differ Resolve ties: random prior: pick class with greater prior nearest: use 1-NN classifier to decide Missing values: have to fill in the missing values, otherwise cannot compute distance. Pro and cons: Almost no assumptions about data easy to update in online setting: just add new item to training set Need to handle missing data: fill-in or create a special distance Sensitive to outliers Sensitve to lots of irrelevant aeributes (affect distance) Computationally expensive: need to compute distance to all examples O(nd) - Vectorization Faster knn: K-D Trees, Inverted lists, Locality-sensitive hashing K-D Treeslow-dimensional, real-valued data A kd-tree is a binary tree data structure for storing a fi nite set of points from a k-dimensional space. Build the tree: Pick random dimension, Find median, Split data Nearest neighbor search: Traverse the whole tree, BUT make two modifications to prune to search space: Keep variable of closest point C found so far. Prune subtrees once their bounding boxes say that they can’t contain any point closer than C Search the subtrees in order that maximizes the chance for pruning Inverted listshigh-dimensional, discrete data, sparse Application: text classification, most attribute values are zero (sparseness), training: list all training examples that contain particular attribute Testing: merge inverted list for attribtues presented in the test set, and choose those instances in the new inverted list as the neighbours Locality-sensitive hashinghigh-d, discrete or real-valued Unsupervised learning 无监督学习ClusteringK-meanssplit data into a specified number of populations Input: K (number of clusters in the data) Training set {x1, x2, x3 …, xn) Algorithm: Randomly initialize K cluster centroids as {μ1, μ2, μ3 … μK}, now centroid could represent cluster. Repeat until converge: Inner loop 1: repeatedly sets the c(i) variable to be the index of the closes variable of cluster centroid closes to xi, i.e. take ith example, measure squared distance to each cluster centroid, assign c(i)to the closest cluster(centroid) Inner loop 2: For each cluster j, new centroid c(j) = average mean of all the points assigned to the cluster j in previous step. Target (Distortion) function: J(c,μ)=Σ|| xi-μi ||^2, coordinate ascent, decrease monotonically, thus guarantee to converge. What if there’s a centroid with no data: Remove that centroid, so end up with K-1 classes, Or, randomly reinitialize it, not sure when though… How to choose cluster numbers: scree plot to find the best k. Hierarchical K-means A Top-down approach run k-means algorithm on the original dataset for each of the resulting clusters, recursively run k-means Pro cons: Fast nearby points may end up in different clusters Agglomerative ClusteringA bottom up algorithm:123451. starts with a collections of singleton clusters2. repeat until only one cluster is left: 1. Find a pair of clusters that is closest 2. Merge the pair of clusters into one new cluster 3. Remove the old pair of clusters Need to define a distance metric over clusters Produce a dendrogram: Hierarchical tree of clusters slow Gaussian MixturesFor non-Gaussian distribution data, assume it is a mixture of several(k) Gaussians. Algorithm: EM EM algorithmstrategy will be to repeatedly construct a lower-bound on ℓ(E-step) based on Jensen’s inequality, and then optimize that lower-bound(M-step). E step: For each i, let Qi be some distribution over the z’s (ΣzQi(z) = 1, Qi(z) ≥ 0). z(i) indicating which of the k Gaussians each x(i) had come from, get P(Z)=φ, then compute the conditional probability wj as P(x|Z) via Gaussian Naive Bayes: M step: maximize, with respect to our parameters φ, µ, Σ, the quantity, by updating parameter(φ, µ, σ) 举例：start with two randomly placed Gaussians (μa, σa), (μb, σb), assume a uniform prior (P(a)=P(b)=0.5), iterate until convergence: E-step: for each point: P(b|xi), P(a|xi)=1-P(b|xi) , does it look like it came from b or a? M-step: adjust (μa, σa) and (μb, σb) to fit points soft assigned to them, The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments c, we instead have the “soft” assignments w. Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea. How to pick k: cannot discover K, likelihood keeps growing with K K-means vs. EM Dimensionality Reduction Pros: reflects human intuitions about the data allows estimating probabilities in highadimensional data: no need to assume independence etc. dramatic reduction in size of data: faster processing (as long as reduction is fast), smaller storage Cons too expensive for many applications (Twitter, web) disastrous for tasks with fine-grained classes understand assumptions behind the methods (linearity etc.): there may be better ways to deal with sparseness Factor analysisIf the features n ≫ m, or n≈m, in such a problem, it might be difficult to model the data even with a single Gaussian, 更别提高斯混合了. Because the variance matrix Σ becomes singular - non invertable. Principal Components AnalysisPCA, automatically detect and reduce data to lower dimension k, k &lt;&lt; n, preserve dimenson that affects class separability most. Algorithm: Pre-process: data normalization to 0 mean and unit variance,Steps (3-4) may be omitted if we had apriori knowledge that the different attributes are all on the same scale to project data into a k-dimensional subspace (k &lt; n), we should choose e1,… ek to be the top k eigenvectors of Σ. The e’s now form a new, orthogonal basis for the data. To represent a training data point x with d dimension into this basis (k dimension), e1Tx,…ekTx The vectors u1,…, uk are called the first k principal components of the data. Eigenvalue λi = variance along ei. Pick ei that explain the most variance by sorting eigenvectors s.t. λ1 ≥ λ2 ≥…≥ λn pick first k eigenvectors which explain 90% or 95% of the total variance Σλ(i). Maximize the variance of projection of x onto a unit vector u, Application: eigenfaces Linear Discriminant AnalysisLDA Idea: pick a new dimension that gives maximum separation between means of projected classes minimum variance within each projected class How: eigenvectors based on between-class and within-class covariance matrices LDA not guaranteed to be better for Classification assumes classes are unimodal Gaussians fails when discriminatory information is not in the mean, but in the variance of the data Singular Value DecompositionGeneralization and evaluationReceiver Operating CharacteristicROC, plot TPR(Sensitivity) vs. FPR(Specificity) as t varies from ∞ to -∞, shows performance of system across all possible thresholds A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner. Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test AUC: area under ROC curve, popular alternative to Accuracy Confidence intervaltell us how closed our estimation E = probability that misclassify a random instance: Take a random set of n instances, how many misclassified? Equal to Binomial distribution with mean = nE, variance = nE(1-E) Efuture: the next instance’s probability of misclassified = average #misclassifed = variance / n = mean E= E(1-E)/n, small variance means big confidence interval, a Gaussian distribution with one variance distance extend from mean will cover 2/3 future test sets p% Confidence interval for future error, 95% confidence interval needs about 2 variance extends from mean. .]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning with Scikit-learn (Sklearn) 机器学习实践]]></title>
    <url>%2Fmachine-learning-with-sklearn%2F</url>
    <content type="text"><![CDATA[Scikit-learn 提供一套实用的工具，用于解决机器学习中的实际问题，并配合适当的方法来制定解决方案。 涉及数据和模型简介，决策树，误差的作用，最小化误差，回归拟合，逻辑回归，神经网络，感知器，支持向量机，朴素贝叶斯，降维，K均值，简单高斯混合模型，分层聚类，模型评估。 实验和代码在GitHub;练习作业答案可以参考GitHub]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS106b Programming Abstractions - C++ 算法与数据结构 Stanford cs106b]]></title>
    <url>%2FNOTE-CS106B-Programming-Abstractions%2F</url>
    <content type="text"><![CDATA[Stanford CS106B Programming Abstractions 和 CS106A 的学习笔记. 我的课程作业(cs106b spring 2017)实现代码见 https://github.com/ShootingSpace/cs106b-programming-abstraction Topics:Recursion, algorithms analysis (sort/search/hash), dynamic data structures (lists, trees, heaps), data abstraction (stacks, queues, maps), implementation strategies/tradeoffs Purposes: become acquainted with the C++ programming language learn more advanced programming techniques explore classic data structures and algorithms and apply these tools to solving complex problemsReference Text Book: Data Structures &amp; Algorithm Analysis in C++, 4th ed, by Mark A. Weiss Text Book: Programming Abstractions in C++ 1st Edition by Eric Roberts Text Book: Algorithms, 4th Edition Blog: Red Blob Games, Amit’s A* Pages Coding style Works correctly in all situations: Using a listing of specific test cases to exercise the program on. The overall approach is straight-forward, data structure is cleanly organized, tasks are nicely decomposed, algorithms are clear and easy to follow, comments are helpful, layout is consistent.CommentingExamples of information you might include in comments: General overview. What are the goals and requirements of this program? this function? The overview comment should also contain author and version information: who worked on this file and when. Data structures. How is the data stored? How is it ordered, searched, accessed? Design decisions. Why was a particular data structure or algorithm chosen? What other strategies were tried and rejected? Error handling. How are error conditions handled? What assumptions are made? What happens if those assumptions are violated? Nitty-gritty code details. Comments are invaluable for explaining the inner workings of particularly complicated (often labeled “clever”) paths of the code. Planning for the future. How might one make modifications or extensions later? And more… (This list is by no means exhaustive) ADTDefinitionAn abstract data type is a set of objects together with a set of operations. Abstract data types are mathematical abstractions; nowhere in an ADT’s definition is there any mention of how the set of operations is implemented.Objects such as lists, sets, and graphs, along with their operations, can be viewed as ADTs.Also there are search tree, set, hash table, priority queue. Client uses class as abstraction Invokes public operations only Internal implementation not relevant! Client can’t and shouldn’t muck with internals Class data should private Imagine a “wall” between client and implementor Wall prevents either from getting involved in other’s business Interface is the “chink” in the wall Conduit allows controlled access between the two Consider Lexicon Abstraction is a word list, operations to verify word/prefix How does it store list? using array? vector? set? does it matter to client? Why ADTs? Abstraction: Client insulated from details, works at higher-level Encapsulation: Internals private to ADT, not accessible by client Independence: Separate tasks for each side (once agreed on interface) Flexibility: ADT implementation can be changed without affecting client Vector and list in the STLThe C++ language includes, in its library, an implementation of common data structures.This part of the language is popularly known as the Standard Template Library (STL). In general, these data structures are called collections or containers. IteratorsIn the STL, a position is represented by a nested type, iterator. Getting an Iterator iterator begin( ) returns an appropriate iterator representing the first item in thecontainer. iterator end( ) returns an appropriate iterator representing the endmarker in thecontainer (i.e., the position after the last item in the container). Iterator Methods itr++ and ++itr advances the iterator itr to the next location. Both the prefix and postfix forms are allowable. itr returns a reference to the object stored at iterator itr’s location. The reference returned may or may not be modifiable (we discuss these details shortly). itr1==itr2 returns true if iterators itr1 and itr2 refer to the same location and false otherwise. itr1!=itr2 returns true if iterators itr1 and itr2 refer to a different location and false otherwise. Container Operations that require IteratorsThe three most popular methods that require iterators are those that add or remove from the list (either a vector or list) at a specified position: iterator insert( iterator pos, const Object &amp; x ): adds x into the list, prior to theposition given by the iterator pos. This is a constant-time operation for list, but not forvector. The return value is an iterator representing the position of the inserted item. iterator erase( iterator pos ): removes the object at the position given by the iterator. This is a constant-time operation for list, but not for vector. The return value is the position of the element that followed pos prior to the call. This operation invalidates pos, which is now stale, since the container item it was viewing has been removed. iterator erase( iterator start, iterator end ): removes all items beginning at position start, up to, but not including end. Observe that the entire list can be erased by the call c.erase( c.begin( ), c.end( ) ) Range for loopC++11 also allows the use of the reserved word auto to signify that the compiler will automatically infer the appropriate type, for simple data type: 12for( auto x : squares ) cout&lt;&lt; x; for complicate data type like map: Each element of the container is a map&lt;K, V&gt;::value_type, which is a typedef for std::pair&lt;const K, V&gt;. Consequently, you’d write this as 123for (auto&amp; kv : myMap) &#123; std::cout &lt;&lt; kv.first &lt;&lt; " has value " &lt;&lt; kv.second &lt;&lt; std::endl;&#125; RecursionHelper Function No clear definition of helper function How to utilize helper function to help constructing recursion algarithm: construct a same-name recursive function with extra parameters to pass in. In some other cases, decomposition with several step into a function is itself a helper function, which help to make the main function simple and clean. Exhaustive recursionPermutations/subsets are about choice Both have deep/wide tree of recursive calls Depth represents total number of decisions made Width of branching represents number of available options per decision Explores every possible option at every decision point, typically very expensive, N! permutations, 2N subsets Recursive BacktrackingPartial exploration of exhaustive space. In the case that if we are interested in finding any solution, whichever one that works out first is fine. If we eventually reach our goal from here, we have no need to consider the paths not taken. However, if this choice didn’t work out and eventually leads to nothing but dead ends; when we backtrack to this decision point, we try one of the other alternatives. The back track based on the stacks of recursion, if a stack return false (or fail result), we back to previous stack and try another way(un-making choice). Need something return(normally bool) to step out of the entire recursion once any one solution found. One great tip for writing a backtracking function is to abstract away the details of managing the configuration (what choices are available, making a choice, checking for success, etc.) into other helper functions so that the body of the recursion itself is as clean as can be. This helps to make sure you have the heart of the algorithm correct and allows the other pieces to be developed, test, and debugged independently. PointerlvalueIn C++, any expression that refers to an internal memory location capable of storing data is called an lvalue (pronounced “ell-value”).x = 1.0; Declaring pointer variables123456789101112131415161718192021222324252627282930313233int main() &#123; -------------------------------------------------- // Declaration, in the stack // Not yet initialized! int num; int *p, *q; // If cout &lt;&lt; num &lt;&lt; p &lt;&lt; q &lt;&lt; endl; // There will be junk number, junk address. // If now *p=10, it may blow up, because what *p point to is an address points to somewhere around that could be invalid. --------------------------------------------------- // new operator allocate memory from the heap, returns address p = new int; // P -----&gt; [ int ] （heep 1000） *p = 10; // P -----&gt; [ 10 ] （heep 1000） q = new int; // P -----&gt; [ int ] （heep 1004） *q = *p; // q -----&gt; [ 10 ] （heep 1004） q = p; // q -----&gt; [ 10 ] （heep 1000） // [ 10 ] （heep 1004） became orphan, and could not be reclaim back --------------------------------------------------- delete p; // [ 10 ] （heep 1000）memory was reclaimed and free, // and available for others as [ ]（heep 1000）, // but p still hold the address delete q; // bad idea, [ 10 ]（heep 1000） already been reclaimed! q = NULL; // NULL is zero pointer, means the pointer does not hold any address, // used as sentinel value, sometimes better than delete. // Accessing "deleted" memory has unpredictable consequences --------------------------------------------------- // int *p declaration reserves only a single word, which is large enough to hold a machine address. // ≠ // int *p = NULL declare pointer p as nullptr --------------------------------------------------- (*newOne).name = name // "." &gt; "*" newOne-&gt;name = name Use of pointerBig program that contains a certain amout of classes and objects that are share some relationship. Instead of copying data from each other, using pointer to point to specific data is better: Saves space by not repeating the same information. If some objects gets new information to update, change in one place only! Dynamic allocation Request memoryTo acquire new memory when you need it and to free it explicitly when it is no longer needed. Acquiring new storage when the program is running. While the program is running, you can reserve part of the unallocated memory, leaving the rest for subsequent allocations.The pool of unallocated memory available to a program is called the heap.int *p = new int; //new operator to allocate memory from the heapIn its simplest form, the new operator takes a type and allocates space for a variable of that type located in the heap.The call to new operator will return the address of a storage location in the heap that has been set aside to hold an integer. Free occupied memoryDelete which takes a pointer previously allocated by new and returns the memory associated with that pointer to the heap. TreeTree terminology Node, tree, subtree, parent, child, root, edge, leaf For any node ni, the depth of ni is the length of the unique path from the root to ni. The height of ni is the length of the longest path from ni to a leaf Rules for all trees Recursive branching structure Single root node Every node reachable from root by unique path Binary treeEach node has at most 2 children. Binary search tree All nodes in left subtree are less than root, all nodes in right subtree are greater. Arranged for efficient search/insert. It is the basis for the implementation of two library collections classes, set and map. Most operations’ average running time is O(log N). Operating on trees Many tree algorithms are recursive Handle current node, recur on subtrees Base case is empty tree (NULL) Tree traversals to visit all nodes, order of traversal: Pre: cur, left, right In: left, cur, right Post: left, right, cur Others: level-by-level, reverse orders, etc Balanced Search TreesBinary search tree have poor worst-case performance.To make costs are guaranteed to be logarithmic, no matter what sequence of keys is used to construct them, the ideal is to keep binary search trees perfectly balanced. Unfortunately, maintaining perfect balance for dynamic insertions is too expensive. So consider data structure that slightly relaxes the perfect balance requirement to provide guaranteed logarithmic performance not just for the insert and search operations, but also for all of the ordered operations (except range search). AVL treeAdelson-Velskii and Landis tree is a binary search tree with a balance condition. Track balance factor for each node: Height of right subtree - height of left subtree information is kept for each node (in the node structure) For every node in the tree, the height of the left and right subtrees can differ by at most 1 (Balance factor = 0 or 1). When balance factor hits 2, restructure Rotation moves nodes from heavy to light side Local rearrangement around specific node When finished, node has 0 balance factor Single rotation: one time rotation between new insert node and its parent node Double rotation: two single rotation of the new insert node 2-3 treesAllow the nodes in the tree to hold more than one key: 3-nodes, which hold three links and two keys. Definition: A 2-3 search tree is a tree that is either empty or A 2-node, with one key (and associated value) and two links, a left link to a 2-3 search tree with smaller keys, and a right link to a 2-3 search tree with larger keys A 3-node, with two keys (and associated values) and three links, a left link to a 2-3 search tree with smaller keys, a middle link to a 2-3 search tree with keys between the node’s keys, and a right link to a 2-3 search tree with larger keys A perfectly balanced 2-3 search tree is one whose null links are all the same distance from the root. The concept guarantee that search and insert operations in a 2-3 tree with N keys are to visit at most lg N nodes. But its dicrect implementation is inconvenient: Not only is there a substantial amount of code involved, but the overhead incurred could make the algorithms slower than standard BST search and insert. Consider a simple representation known as a red-black BST that leads to a natural implementation. Priority QueuesA priority queue is a data structure that allows at least the following two operations: insert, and deleteMin, which finds, returns, and removes the minimum element in the priority queue. Binary HeapA heap is a binary tree that is completely filled, with the possible exception of the bottom level, which is filled from left to right. Such a tree is known as a complete binary tree. Structure A heap data structure consist of an array (of Comparable objects) and an integer representing the current heap size. For any element in array position i, the left child is in position 2i, the right child is in the cell after the left child [2i + 1], and the parent is in position [i/2]. Heap-Order Property For every node X, the key in the parent of X is smaller than (or equal to) the key in X. So to make find minimum operation quick. Basic Heap Operation insert: To insert an element X into the heap, create a hole in the next available location. Then Percolate up - swap X with its parent index (i/2) so long as X has a higher priority than its parent. Continue this process until X has no more lower priority parent. 1234567//Percolate upint hole = ++size; binaryQueue[0]=std::move(*newOne); for (;(priority&lt;binaryQueue[hole/2].priority || (priority==binaryQueue[hole/2].priority &amp;&amp; name&lt;binaryQueue[hole/2].name) );hole/=2) &#123; binaryQueue[hole] = std::move(binaryQueue[hole/2]); &#125; binaryQueue[hole] = std::move(binaryQueue[0]); deleteMin: When the minimum is removed, a hole is created at the root. Move the last element X in the heap to place in the root hole. Then Percolate down - swapp X with its more urgent-priority child [index (i2 or i2+1)] so long as it has a lower priority than its child. Repeat this step until X has no more higher priority child. 1234567891011//Percolate downint child; for (; hole*2&lt;=size;hole=child) &#123; child = hole*2; if ( child!=size &amp;&amp; (binaryQueue[child+1].priority&lt;binaryQueue[child].priority || (binaryQueue[child+1].priority==binaryQueue[child].priority &amp;&amp; binaryQueue[child+1].name&lt;binaryQueue[child].name)) ) ++child; if ( binaryQueue[child].priority&lt;priority_tobePerD || (binaryQueue[child].priority==priority_tobePerD &amp;&amp; binaryQueue[child].name&lt;name_tobePerD) ) &#123; binaryQueue[hole] = std::move(binaryQueue[child]); &#125; else break; &#125; Use integer division to avoid even odd index. Algorithm AnalysisSpace/time, big-O, scalability Big-O Computational complexity: The relationship between N and the performance of an algorithm as N becomes large Big-O notation: to denote the computational complexity of algorithms. Standard simplifications of big-O Eliminate any term whose contribution to the total ceases to be significant as N becomes large. Eliminate any constant factors. Worst-case versus average-case complexityAverage-case performance often reflects typical behavior, while worst-case performance represents a guarantee for performance on any possible input. Predicting computational complexity from code structure Constant time: Code whose execution time does not depend on the problem size is said to run in constant time, which is expressed in big-O notation as O(1). Linear time: function that are executed exactly n times, once for each cycle of the for loop, O(N) Quadratic time: Algorithms like selection sort that exhibit O(N2) performance are said to run in quadratic tim For many programs, you can determine the computational complexity simply by finding the piece of the code that is executed most often and determining how many times it runs as a function of N Space/time In general, the most important measure of performance is execution time. It also possible to apply complexity analysis to the amount of memory space required. Nowadays the memory is cheap, but it still matters when designing extreamly big programs, or APPs on small memory device, such as phones and wearable devices. SortingThere are lots of different sorting algoritms, from the simple to very complex. Some optimized for certain situations (lots of duplicates, almost sorted, etc.). So why do we need multiple algorithms? Selection sort Select smallest and swap to front/backend 12345678910void SelectionSort(Vector&lt;int&gt; &amp;arr)&#123; for (int i = 0; i &lt; arr.size()-1; i++) &#123; int minIndex = i; for (int j = i+1; j &lt; arr.size(); j++) &#123; if (arr[j] &lt; arr[minIndex]) minIndex = j; &#125; Swap(arr[i], arr[minIndex]); &#125; Selection sort analysisCount work inside loops: First iteration does N-1 compares, second does N-2, and so on. One swap per iteration O(N2) Insertion sort As sorting hand of just-dealt cards, each subsequent element inserted into proper place Start with first element (already sorted) Insert next element relative to first Repeat for third, fourth, etc. Slide elements over to make space during insert123456789void InsertionSort(Vector&lt;int&gt; &amp;v)&#123; for (int i = 1; i &lt; v.size(); i++) &#123; int cur = v[i]; // slide cur down into position to left for (int j=i-1; j &gt;= 0 &amp;&amp; v[j] &gt; cur; j--) v[j+1] = v[j]; v[j+1] = cur; &#125;&#125; Insertion sort analysisBecause of the nested loops, each of which can take N iterations, insertion sort is O(N2). HeapsortPriority queues can be used to sort in O(N log N) time. The algorithm based on this idea is known as heapsort. Heapsort analysisThe building of the heap, uses less than 2N comparisons. In the second phase, the ith deleteMax uses at most less than 2*log (N − i + 1) comparisons, for a total of at most 2N log N − O(N) comparisons (assuming N ≥ 2). Consequently, in the worst case, at most 2N log N − O(N) comparisons are used by heapsort. Merge sort Inspiration: Algorithm like selection sort is quadratic growth (O(N2)). Double input -&gt; 4X time, halve input -&gt; 1/4 time.Can recursion save the day? If there are two sorted halves, how to produce sorted full result? Divide and conquer algorithm Divide input in half Recursively sort each half Merge two halves together “Easy-split hard-join” No complex decision about which goes where, just divide in middle Merge step preserves ordering from each half Merge depends on the fact that the first element in the complete ordering must be either the first element in v1 or the first element in v2, whichever is smaller. 12345678910111213141516171819202122232425void MergeSort(Vector&lt;int&gt; &amp;v)&#123; if (v.size() &gt; 1) &#123; int n1 = v.size()/2; int n2 = v.size() - n1; Vector&lt;int&gt; left = Copy(v, 0, n1); Vector&lt;int&gt; right = Copy(v, n1, n2); MergeSort(left); MergeSort(right); v.clear(); Merge(v, left, right); &#125;&#125;void Merge(Vector&lt;int&gt; &amp;v,Vector&lt;int&gt; &amp;left,Vector&lt;int&gt; &amp;right) &#123; int l=0, r=0; while(l&lt;left.size() &amp;&amp; r&lt;right.size()) &#123; if (left[l]&lt;right[r]) v.add(left[l++]); else v.add(right[r++]); &#125; while(l&lt;left.size()) v.add(left[l++]); while(r&lt;right.size()) v.add(right[r++]);&#125; Mergesort analysisThe time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge, which is linear. T(N) = N + 2T(N/2). log N levels * N per level= O(NlogN). Mergesort uses the lowest number of comparisons of all the popular sorting algorithms.Theoretical result show that no general sort algorithm could be better than NlogN.But there is still better in practice: The running time of mergesort, when compared with other O(N log N) alternatives, depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array). These costs are language dependent. In Java, when performing a generic sort (using a Comparator), an element comparison can be expensive, but moving elements is cheap (because they are reference assignments, rather than copies of large objects). In C++, in a generic sort, copying objects can be expensive if the objects are large, while comparing objects often is relatively cheap because of the ability of the compiler to aggressively perform inline optimization. QuicksortMost sorting programs in use today are based on an algorithm called Quicksort, which employs a Divide and conquer strategy as merge sort, but instead take a different approach to divide up input vector into low half and high half. Quicksort uses a few more comparisons, in exchange for significantly fewer data movements. The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efficiently. “Hard-split easy-join”, Each element examined and placed in correct half, so that join step become trivial. Choose an element (pivot) to serve as the boundary between the small and large elements. Partitioning: Rearrange the elements in the vector so that all elements to the left of the boundary are less than the pivot and all elements to the right are greater than or possibly equal to the pivot. Sort the elements in each of the partial vectors.12345678void Quicksort(Vector&lt;int&gt; &amp;v, int start, int stop)&#123; if (stop &gt; start) &#123; int pivot = Partition(v, start, stop); Quicksort(v, start, pivot-1); Quicksort(v, pivot+1, stop); &#125;&#125; Quicksort performance analysisThe running time of quicksort is equal to the running time of the two recursive calls plus the linear time spent in the partition (the pivot selection takes only constant time). T(N) = T(i) + T(N − i − 1) + cN, where i = |S1| is the number of elements in S1.There are thre cases Ideal 50/50 split: The pivot is in the middle, T(N) = cN + 2T(N/2) =&gt; O(NlogN) Average bad 90/10 split: N per level, but more levels, solve N*(9/10)k = 1, still k = O(NlogN) Worst N-1/1 split: The pivot is the smallest element, all the time. Then i = 0, T(N) = T(N − 1) + cN, N &gt; 1. With N levels! O(N2) In a vector with randomly chosen elements, Quicksort tends to perform well, with an average-case complexity of O(N log N). In the worst case — which paradoxically consists of a vector that is already sorted — the performance degenerates to O(N2). Despite this inferior behavior in the worst case, Quicksort is so much faster in practice than most other algorithms that it has become the standard. Design StrategyWhen an algorithm is given, the actual data structures need not be specified. It is up to the programmer to choose the appropriate data structure in order to make the running time as small as possible. There are many to be considered: algorithms, data structure, space-time tradeoff, code complexity. Dynamic ProgrammingTo solve optimization problems in which we make a set of choices in order to arrive at an optimal solution. As we make each choice, subproblems of the same form often arise. Dynamic programming is effective when a given subproblem may arise from more than one partial set of choices; the key technique is to store the solution to each such subproblem in case it should reappear. Unlike divide-and-conquer algorithms which partition the problem into disjoint subproblems, dynamic programming applies when the subproblems overlap. “Programming” in this context refers to a tabular method. When should look for a dynamic-programming solution to a problem? Optimal substructure: a problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. Overlapping subproblems: When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problemhas overlapping subproblems. In contrast, a problem for which a divide-andconquer approach is suitable usually generates brand-new problems at each step of the recursion. General setps of Dynamic Programming Characterize the structure of an optimal solution. Recursively define the value of an optimal solution. Compute the value of an optimal solution, typically in a bottom-up fashion. Construct an optimal solution from computed information. Greedy AlgorithmsGreedy algorithms work in phases. In each phase, a decision is made in a locally optimal manner, without regard for future consequences. When the algorithm terminates, we hope that the local optimum is equal to the global optimum. If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution. Huffman Codes A Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations. We merely select the two smallest trees. If we maintain the trees in a priority queue, ordered by weight, then the running time is O(C logC), since there will be one buildHeap, 2C − 2 deleteMins, and C − 2 inserts. A simple implementation of the priority queue, using a list, would give an O(C2) algorithm. The choice of priority queue implementation depends on how large C is. In the typical case of an ASCII character set, C is small enough that the quadratic running time is acceptable. Divide and ConquerTraditionally, routines in which the text contains at least two recursive calls and subproblems be disjoint (that is, essentially nonoverlapping) are called divide-and-conquer algorithms. Divide: Smaller problems are solved recursively (except, of course, base cases). Conquer: The solution to the original problem is then formed from the solutions to the subproblems.We have already seen several divide-and-conquer algorithms: mergesort and quicksort, which have O(N log N) worst-case and averagecase bounds, respectively. Backtracking AlgorithmsSee Recursive BacktrackingIn some cases, the savings over a brute-force exhaustive search can be significant.The elimination of a large group of possibilities in one step is known as pruning. How to evaluate/compare alternatives Often interested in execution performance: Time spent and memory used Should also consider ease of developing, verifying, maintaining codeQuicksort strategy Picking the pivotPicking a good pivot improves performance, but also costs some time. If the algorithm spends more time choosing the pivot than it gets back from making a good choice, you will end up slowing down the implementation rather than speeding it up. The popular, uninformed choice is to use the first element as the pivot. This is acceptable if the input is random, but if the input is presorted or in reverse order, then the pivot provides a poor partition. A safe approach is to choose the pivot element randomly. On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all. A good estimate can be obtained by picking three elements randomly and using the median of these three as pivot. The randomness turns out not to help much, so the common course is to use as pivot the median of the left, right, and center elements. Quicksort partitioning strategyA known method that is very easy to do it wrong or inefficiently. General process: The first step is to get the pivot element out of the way by swapping it with the last element. Two pointers, i point to the first element and j to the next-to-last element. What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part. “Small” and “large” are relative to the pivot. While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot. We move j left, skipping over elements that are larger than the pivot. When i and j have stopped, i is pointing at a large element and j is pointing at a small element. If i is to the left of j (not yet cross), those elements are swapped. Repeat the process until i and j cross The final is to swap the pivot element with present i element One important detail we must consider is how to handle elements that are equal to the pivot? Suppose there are 10,000,000 elements, of which 500,000 are identical (or, more likely, complex elements whose sort keys are identical). To get an idea of what might be good, we consider the case where all the elements in the array are identical. If neither i nor j stops, and code is present to prevent them from running off the end of the array, no swaps will be performed. Although this seems good, a correct implementation would then swap the pivot into the last spot that i touched, which would be the next-to last position (or last, depending on the exact implementation). This would create very uneven subarrays. If all the elements are identical, the running time is O(N2). If both i and j stop, there will be many swaps between identical elements. The partition creates two nearly equal subarrays. The total running time would then be O(N log N). Thus it is better to do the unnecessary swaps and create even subarrays than to risk wildly uneven subarrays. Small arrays For very small arrays (N ≤ 20), quicksort does not perform as well as insertion sort. Furthermore, because quicksort is recursive, these cases will occur frequently. A common solution is not to use quicksort recursively for small arrays, but instead use a sorting algorithm that is efficient for small arrays, such as insertion sort. A good cutoff range is N = 10, although any cutoff between 5 and 20 is likely to produce similar results. This also saves nasty degenerate cases, such as taking the median of three elements when there are only one or two. Text editor case study Buffer requirements Sequence of characters + cursor position Operations to match commands above What to consider? Implementation choices performance implications Buffer class interface 1234567891011121314class Buffer &#123; public: Buffer(); ~Buffer(); void moveCursorForward(); void moveCursorBackward(); void moveCursorToStart(); void moveCursorToEnd(); void insertCharacter(char ch); void deleteCharacter(); void display(); private: // TBD!&#125;; Buffer layered on Vector Need character data + cursor Chars in Vector&lt;char&gt; Represent cursor as integer index Minor detail – is index before/after cursor? Buffer contains: AB|CDE 1234// for Buffer classprivate: Vector&lt;char&gt; chars;int cursor; Performance insertCharacter() and deleteCharacter() is linear, other operation is just O(1) Space used ~1 byte per char Buffer layered on Stack Inspiration: add/remove at end of vector is fast If chars next to cursor were at end… Build on top of stack? Another layered abstraction! How is cursor represented? Buffer contains:AB|CDEThere is no explicit cursor representation, instead using two stack to represent a whole data structure being seperated by the implicit cursor. 123// for Buffer classprivate: Stack&lt;char&gt; before, after; Performance moveCursorToStart(), moveCursorToEnd() operation is linear, other operation is just O(1) Space used ~2 byte per char Buffer as double linked list Inspiration: contiguous memory is constraining Connect chars without locality Add tail pointer to get direct access to last cell Add prev link to speed up moving backwards Buffer contains:AB|CDE 1234567// for Buffer classprivate: struct cellT &#123; char ch; cellT *prev, *next; &#125;; cellT *head, *tail, *cursor; Cursor design To cell before or after? 5 letters, 6 cursor positions… Add “dummy cell” to front of list Performance destruction is linear, other operation is just O(1) Space used ~9 byte per char Compare implementations table th:nth-of-type(1) { width: 200px; } table th:nth-of-type(2) { width: 80px; } table th:nth-of-type(3) { width: 80px; } Operation Vector Stack Single linked list Double linked list Buffer() O(1) O(1) O(1) O(1) ~Buffer() O(1) O(1) O(N) O(N) moveCursorForward() O(1) O(1) O(1) O(1) moveCursorBackward() O(1) O(1) O(N) O(1) moveCursorToStart() O(1) O(N) O(1) O(1) moveCursorToEnd() O(1) O(N) O(N) O(1) insertCharacter() O(N) O(1) O(1) O(1) deleteCharacter() O(N) O(1) O(1) O(1) Space used 1N 2N 5N 9N Space-time tradeoff Doubly-linked list is O(1) on all six operations But, each char uses 1 byte + 8 bytes of pointers =&gt; 89% overhead! Compromise: chunklist Array and linked list hybrid Shares overhead cost among several chars Chunksize can be tuned as appropriate Cost shows up in code complexity Cursor must traverse both within and across chunks Splitting/merging chunks on insert/deletes Implementing MapMap is super-useful, support any kind of dictionary, lookup table, index, database, etc.Map stores key-value pairs, support fast access via key, operations to optimize: add, getValueHow to make it work efficiently? Implement Map as Vector Layer on Vector, provides convenience with low overhead Define pair struct, to olds key and value together, Vector&lt;pair&gt; Vector sorted or unsorted? If sorted, sorted by what? Sorting: Provides fast lookup, but still slow to insert (because of shuffling) How to implement getValue, add? Does a linked list help? Easy to insert, once at a position But hard to find position to insert… Implementing Map as tree Implementatation Each Map entry adds node to tree, node contains: string key, client-type value, pointers to left/right subtrees Tree organized for binary search, Key is used as search field getValue: Searches tree, comparing keys, find existing match or error add: Searches tree, comparing keys, overwrites existing or adds new node Private members for Map 1234567891011121314151617template &lt;typename ValType&gt; class Map &#123; public: // as before private: struct node &#123; string key; ValType value; node *left, *right; &#125;; node *root; node *treeSearch(node * t, string key); void treeEnter(node *&amp;t, string key, ValType val); DISALLOW_COPYING(Map)&#125;; Evaluate Map as tree Space used: Overhead of two pointers per entry (typically 8 bytes total) Runtime performance: Add/getValue take time proportional to tree height(expected to be O(logN)) Degenerate trees The insert order is “sorted”: 2 8 14 15 18 20 21, totally unbalanced with height = 7 The insert order is “alternately sorted”: 21 2 20 8 14 15 18 or 2 8 21 20 18 14 15 Association: What is the relationship between worst-case inputs for tree insertion and Quicksort? What to do about it: AVL tree Compare Map implementations Operation Vector BST Sorted Vector getValue O(N) O(lgN) O(lgN) add O(N) O(lgN) O(N) Space used N 9N N Hashing Hash table ADT Hash table data structure: A list of keys and TableSize Hash function: A mapping that map each key into some number in the range 0 to TableSize-1 and distributes the keys evenly among the appropriate cell HashingThe major problems are choosing a function, deciding what to do when two keys hash to the same value (this is known as acollision), and deciding on the table size RehashingIf the table gets too full, the running time for the operations will start taking too long, and insertions might fail for open addressing hashing with quadratic resolution. A solution is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table. The Big-FiveIn C++11, classes come with five special functions that are already written for you. These are the destructor, copy constructor, move constructor, copy assignment operator, and move assignment operator. Collectively these are the big-five. DestructorThe destructor is called whenever an object goes out of scope or is subjected to a delete. Typically, the only responsibility of the destructor is to free up any resources that were acquired during the use of the object. This includes calling delete for any corresponding news, closing any files that were opened, and so on. The default simply applies the destructor on each data member. ConstructorA constructor is a method that describes how an instance of the class is constructed. If no constructor is explicitly defined, one that initializes the data members using language defaults is automatically generated. Copy Constructor and Move Constructor Copy Assignment and Move Assignment (operator=)By Defaults, if a class consists of data members that are exclusively primitive types and objects for which the defaults make sense, the class defaults will usually make sense.The main problem occurs in a class that contains a data member that is a pointer. The default destructor does nothing to data members that are pointers (for good reason—recall that we must delete ourselves). Furthermore, the copy constructor and copy assignment operator both copy the value of the pointer rather than the objects being pointed at. Thus, we will have two class instances that contain pointers that point to the same object. This is a so-called shallow copy (contrast to deep copy). To avoid shallow copy, ban the copy funtionality by calling DISALLOW_COPYING(ClassType). As a result, when a class contains pointers as data members, and deep semantics are important, we typically must implement the destructor, copy assignment, and copy constructors ourselves. Explicit constructor:All one-parameter constructors should be made explicit to avoid behind-the-scenes type conversions. Otherwise, there are somewhat lenient rules that will allow type conversions without explicit casting operations. Usually, this is unwanted behavior that destroys strong typing and can lead to hard-to-find bugs.The use of explicit means that a one-parameter constructor cannot be used to generate an implicit temporary 1234567891011class IntCell &#123;public: explicit IntCell( int initialValue = 0 ) : storedValue&#123; initialValue &#125; &#123; &#125; int read( ) const &#123; return storedValue; &#125;private: int storedValue; &#125;;IntCell obj; // obj is an IntCellobj = 37; // Should not compile: type mismatch Since IntCell constructor is declared explicit, the compiler will correctly complain that there is a type mismatch TemplateType-independentWhen we write C++ code for a type-independent algorithm or data structure, we would prefer to write the code once rather than recode it for each different type Function template A function template is not an actual function, but instead is a pattern for what could become a function. An expansion for each new type generates additional code; this is known as code bloat when it occurs in large projects.Class template12345678template &lt;typename Object&gt;class MemoryCell &#123; public: explicit MemoryCell( const Object &amp; initialValue = Object&#123; &#125; ) : storedValue&#123; initialValue &#125; &#123; &#125; private: Object storedValue;&#125;; MemoryCell is not a class, it is only a class template. It will be a class if specify the Object type. MemoryCell&lt;int&gt; and MemoryCell&lt;string&gt; are the actual classes. Graph AlgorithmsDefinitions: vertices, edges, arcs, directed arcs = digraphs, weight/cost, path, length, acyclic(no cycles) Topological Sort A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from vi to vj, then vj appears after vi in the ordering. A topological ordering is not possible if the graph has a cycle To find a topological ordering, define the indegree of a vertex v as the number of edges (u, v), then use a queue or stack to keep the present 0 indegree vertexes. At each stage, as long as the queue is not empty, dequeue a 0 indegree vertexes in the queue, enqueue each new generated 0 indegree vertexes into the queue. Sortest-Path Algorithms Breadth-first search Explores equally in all directions To find unweighted shortest paths Operates by processing vertices in layers: The vertices closest to the start are evaluated first, and the most distant vertices are evaluated last. Dijkstra’s Algorithm Also called Uniform Cost Search, cost matters Instead of exploring all possible paths equally, it favors lower cost paths. Dijkstra’s algorithm proceeds in stages. At each stage, while there are still vertices waiting to be known: Selects a vertex v, which has the smallest dv among all the unknown vertices, and declares v as known stage. For each of v’s neighbors, w, if the new path’s cost from v to w is better than previous dw, dw will be updated. But w will not be marked as known, unless at next while-loop stage, dw happens to be the smalles. The above steps could be implemented via a priority queue. A proof by contradiction will show that this algorithm always works as long as no edge has a negative cost. If the graph is sparse, with |E| =θ(|V|), this algorithm is too slow. In this case, the distances would need to be kept in a priority queue. Selection of the vertex v is a deleteMin operation. The update of w’s distance can be implemented two ways. One way treats the update as a decreaseKey operation. An alternate method is to insert w and the new value dw into the priority queue every time w’s distance changes. Greedy Best First Search(Heuristic search) With Breadth First Search and Dijkstra’s Algorithm, the frontier expands in all directions. This is a reasonable choice if you’re trying to find a path to all locations or to many locations. However, a common case is to find a path to only one location. A modification of Dijkstra’s Algorithm, optimized for a single destination. It prioritizes paths that seem to be leading closer to the goal. To make the frontier expand towards the goal more than it expands in other directions. First, define a heuristic function that tells us how close we are to the goal, design a heuristic for each type of graph 123def heuristic(a, b): # Manhattan distance on a square grid return abs(a.x - b.x) + abs(a.y - b.y) Use the estimated distance to the goal for the priority queue ordering. The location closest to the goal will be explored first. This algorithm runs faster when there aren’t a lot of obstacles, but the paths aren’t as good(not always the shortest). A* Algorithm Dijkstra’s Algorithm works well to find the shortest path, but it wastes time exploring in directions that aren’t promising. Greedy Best First Search explores in promising directions but it may not find the shortest path. The A* algorithm uses both the actual distance from the start and the estimated distance to the goal. Compare the algorithms: Dijkstra’s Algorithm calculates the distance from the start point. Greedy Best-First Search estimates the distance to the goal point. A* is using the sum of those two distances. So A* is the best of both worlds. As long as the heuristic does not overestimate distances, A* does not use the heuristic to come up with an approximate answer. It finds an optimal path, like Dijkstra’s Algorithm does. A* uses the heuristic to reorder the nodes so that it’s more likely that the goal node will be encountered sooner. Conclusion: Which algorithm should you use for finding paths on a map? If you want to find paths from or to all all locations, use Breadth First Search or Dijkstra’s Algorithm. Use Breadth First Search if movement costs are all the same; use Dijkstra’s Algorithm if movement costs vary. If you want to find paths to one location, use Greedy Best First Search or A*. Prefer A in most cases. When you’re tempted to use Greedy Best First Search, consider using A with an “inadmissible” heuristic. If you want the optimal paths, Breadth First Search and Dijkstra’s Algorithm are guaranteed to find the shortest path given the input graph. Greedy Best First Search is not. A* is guaranteed to find the shortest path if the heuristic is never larger than the true distance. (As the heuristic becomes smaller, A turns into Dijkstra’s Algorithm. As the heuristic becomes larger, A turns into Greedy Best First Search.) Advanced Data StructuresRed-Black TreesRed-black tree leads to a natural implementation of the insertion algorithm for 2-3 trees RBT definition Red-black tree means encoding 2-3 trees in this way: red links, which bind together two 2-nodes to represent 3-nodes, and black links, which bind together the 2-3 tree. An equivalent definition is to define red-black BSTs as BSTs having red and black links and satisfying the following three restrictions: Red links lean left. No node has two red links connected to it. The tree has perfect black balance : every path from the root to a null link has the same number of black links. A 1-1 correspondence: If we draw the red links horizontally in a red-black BST, all of the null links are the same distance from the root, and if we then collapse together the nodes connected by red links, the result is a 2-3 tree. RBT implementaion Color representation: Each node is pointed to by precisely one link from its parent, Encode the color of links in nodes, by adding a boolean instance variable color to our Node data type, which is true if the link from the parent is red and false if it is black. By convention, null links are black. For clarity, define constants RED and BLACK for use in setting and testing this variable. Rotation To correct right-leaning red links or two red links in a row conditions. takes a link to a red-black BST as argument and, assuming that link to be to a Node h whose right link is red, makes the necessary adjustments and returns a link to a node that is the root of a red-black BST for the same set of keys whose left link is red. Actually it is switching from having the smaller of the two keys at the root to having the larger of the two keys at the root. Flipping colors to split a 4-node In addition to flipping the colors of the children from red to black, we also flip the color of the parent from black to red. Keeping the root black. Insertion Maintain the 1-1 correspondence between 2-3 trees and red-black BSTs during insertion by judicious use of three simple operations: left rotate, right rotate, and color flip. If the right child is red and the left child is black, rotate left. If both the left child and its left child are red, rotate right. If both children are red, flip colors. Deletion Assignments Name Hash Game of Life Serafini Recursion Boggle! Patient Queue Huffman Encoding Trailblazer]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs106b</tag>
        <tag>C++</tag>
      </tags>
  </entry>
</search>
