<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[算法与数据结构 13 - Java | 实现继承 Implementation Inheritance - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-13-Implementation%20Inheritance%2F</url>
    <content type="text"><![CDATA[Implementation Inheritance实现继承是subclass从superclass继承实现的关系。 default method 缺省方法（从 Java 8开始）除了signature之外，Java也允许subclass继承implementation。我们可以在List中列出已实现的method。这些方法是 default method，定义了List hypernyms的一些默认行为：default public void method() { ... }. 我们可以自由调用interface中定义的方法，而不用操心具体的实现。default method 应该适用于实现接口的任何类型的对象！子类不必在任何地方重新实现 default method， 可以直接调用。1234// Listdefault public void print() &#123; ...&#125; 不过，我们仍然可以重写 default method，在子类中重新定义该方法。这样，只要我们在LinkedLList上调用print()，它就会调用这个方法，而不是List的。12345// LinkedList@Overridepublic void print() &#123; ...&#125; Java是通过一个叫“dynamic method selection”的特性，来确定要调用 default method 还是已经被子类重写的method。这个实例声明List&lt;String&gt; l = new LinkedList&lt;String&gt;();,指明l的类型是 List, 是 static type。由 new 生成的 object 本身是LinkedList类型，也从属于 List 类型。但是，因为这个对象本身是使用 LinkedList 构造函数实例化的，所以我们称之为 dynamic type。 Dynamic type 的名称起源于: 当`l`被重新分配指向另一种类型的对象时，比如说一个 ArrayList 对象，`l`的动态类型现在就变为 ArrayList. 因为它根据当前引用的对象的类型而改变, 所以是动态的。 Static vs. Dynamic Type: Java 每个变量都有一个static type （compile-time type），这是变量声明时指定的类型，在编译时会检查。 每个变量也有一个 Dynamic Type（run-time type），此类型在变量实例化（new）时指定，并在运行时检查。等同于地址指向的对象的类型。 当Java运行一个被overriden的方法时，它会在它的dynamic type 中搜索合适的 method signature 并运行。 注意，如果是overload:123456public static void peek(List&lt;String&gt; list) &#123; ...&#125;public static void peek(LinkedList&lt;String&gt; list) &#123; ...&#125; 对于上面的实例化的l, 当Java检查要调用哪个方法时，它会检查 static type (此时是List)并使用相同类型的参数调用该方法(也就是使用List作为签名的那个方法)。 总结：区别 Interface Inheritance vs Implementation InheritanceInterface Inheritance 接口继承（what）：指定 subclass 应该实现的功能，即只提供 method signature。 Implementation Inheritance 实现继承（how）：提供功能的实现方案，即提供 method implementation。允许代码再利用，也给subclass设计者提供了更多的自由度，由他们自行决定是否重写 default method。 Implementation inheritance 也有一些缺点： 人会犯错。我们有可能忘了自己曾经重写过一个方法。 如果两个接口给出冲突的 default method，则可能很难解决冲突。 无形中鼓励代码复杂化。 最后，注意从属和拥有的区别：subclass 和 superclass 是上下级从属分类，而不是拥有与被拥有的关系，不要跟 nested class 混淆。 [Oracle](https://docs.oracle.com/javase/tutorial/java/IandI/index.html：Interface MethodsDefault methods and abstract methods in interfaces are inherited like instance methods. However, when the supertypes of a class or interface provide multiple default methods with the same signature, the Java compiler follows inheritance rules to resolve the name conflict. These rules are driven by the following two principles:]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 12 - Java | 接口继承 Interface Inheritance - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-12-Interface%20Inheritance%2F</url>
    <content type="text"><![CDATA[Interfaces InheritanceMotivation:扩展代码适用性我们前面创建的 LinkedList and ArrayList 其实很相似 - 所有的method都一样. 如果我们需要写一个需要用到 list 的类比如WordUtils class, 我们如何让它既可以使用LinkedList又可以用ArrayList？ 简单的方法及时写两个同名不同参数的methods。即所谓method overloading。public static String longest(LinkedList&lt;String&gt; list)public static String longest(ArrayList&lt;String&gt; list) 但 overload 有几个缺点: 超级重复，写两个几乎相同的代码块。 产生更多需要维护的代码，那意味着如果你想对的方法做一个小优化或debug，你需要在对应每种list的方法中改变它。 如果我们想要适配更多的列表类型，不得不复制每个新列表类的方法。 另一种方法是使用 interface 接口。 Hypernyms, Hyponyms, and Interface Inheritance 上位词，下义词和接口继承首先要理解，上位词和下位词是语言学的定义，直接沿用到编程语言中。就像狗是哈士奇的上位词，哈士奇是狗的下义词，在Java把这种关系形式化：如果LinkedList是List的Hyponyms，那么LinkedList类是List的subclass，而List类是LinkedList类的superclass(超类/父类)。 在Java中，为了表达这种层次结构，我们需要： 为 hypernym - 通用列表 List 定义类型。 指定LinkedList和ArrayList是该类型的hyponyms。 1234public interface List&lt;Item&gt; &#123; public void addFirst(Item x); ...&#125; 这里的 List 是Java中的 interface 接口。本质上是一个指定list必须能够做什么的合约，具体如何做并不是它关心的。 123456public class ArrayList&lt;Item&gt; implements List&lt;Item&gt;&#123; // 具体的执行 public void addFirst(Item x) &#123; insert(x, 0); &#125;&#125; 指定ArrayList是List的hyponyms. implements List&lt;Item&gt;类似一种承诺，保证将拥有并定义在List interface 中制定的所有属性（变量）和行为（方法）。 List中指定的方法的具体实现过程就是在这种hyponyms中实现的。 这样就可以同时适配多种list：123456789101112131415public class WordUtils &#123; /** Returns the length of the longest word. */ public static String longest(List&lt;String&gt; list) &#123; ... return list.get(maxDex); &#125; public static void main(String[] args) &#123; ArrayList&lt;String&gt; someList = new ArrayList&lt;&gt;(); //or LinkedList&lt;String&gt; someList = new LinkedList&lt;&gt;(); ... System.out.println(longest(someList)); &#125;&#125; Overriding 重写如果subclass和superclass有signature一样的method, 那么subclass就是在 override 重写这个方法。 Override 要与 overloaded 区别开，重载的方法虽同名，却不同signature。 在子类中实现合约指定的功能时，需要在method的signature顶部包含@Override标签。1234@Overridepublic void addFirst(Item x) &#123; ...&#125; 值得注意的是，即使不包含这个@Override，仍然重写了这个方法。所以技术上讲，它不是必须的。但是，它可以作为一个保障, 提醒编译器我们打算重写此方法，就好像有一个校对员, 如果过程中出现问题, 编译器可以提醒。假设当我们想 override addLast，却不小心写成addLsat。此时如果不包含@Override，那么可能无法发现错误。如果有了@Override，编译器就会提示我们修复错误。 总结：Interface Inheritance接口继承是指subclass继承superclass的所有方法/行为的关系： 子类继承父类 Interfaces 接口列出所有方法的签名，就像‘合约’，但没有具体的实现 根据‘合约’，由子类来实现且必须实现（override 重写）每一个method，否则无法通过编译 继承关系可以延续多代。例如，B可以继承A，C可以继承B. GRoE根据Java的Golden Rule of Equals，每一个赋值a = b，本质上是把b中的bits拷贝到a中，着要求b和a的类型相同。 同理, 假设public static String longest(List&lt;String&gt; list)既接受List, 也接受ArrayList和LinkedList，但是由于ArrayList和List是不同的类，那怎么遵守GRoE呢？ 因为ArrayList与List有着上下位包含的关系，这意味着ArrayList应该能够赋值给List的内存位中. 1234public static void main(String[] args) &#123; List&lt;String&gt; someList = new SLList&lt;String&gt;(); someList.addFirst("elk");&#125; 这段代码运行时，会创建SLList并将其地址存储在someList变量中。然后将字符串“elk”插入到由addFirst引用的SLList中。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理（快速入门） - Accelerated Natural Language Processing - University of Edinburgh]]></title>
    <url>%2FAccelerated-Natural-Language-Processing-UoE%2F</url>
    <content type="text"><![CDATA[自然语言处理入门，概念汇总。 References:Accelerated natural language processing 爱丁堡大学ANLP revision guideJM NLP, Stanford 概率模型 Probability model概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率 估算概率 Probability estimation相关频率/最大似然估计Relative frequency / maximum likelihood estimation p(X) = Count(x)/N 平滑 Smoothing一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。 Add alpha smoothing Assign equal probability to all unseen events. Applied in text classification, or domains where zeros probability is not common. Backoff smoothing Use information from lower order N-grams (shorter histories) Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram. Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. Interpolation smoothing Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts Simple interpolation: P(w3|w1,w2)=1P(w3|w1,w2)+λ2P(w3|w2)+λ3P(w3), Σλ=1. λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data Kneser-Ney smoothing Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams. Use a better estimate for probabilities of lower-order unigrams, the continuation probability, P_continuatin(w) is how likely is w to appear as a novel continutaion. For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w. Every bigram type was a novel continuation the first time it was seen. normalized by the total number of word bigram types. To lower the probability of some fix bigram like “San Franscio” For bigram, Pkn(wi|wi-1)=max(count(wi-1,wi)-d, 0)/c(wi-1) +λ(wi-1)P_continuatin(wi), λ(wi-1) = d{w:count(wi-1,w)&gt;0}/c(wi-1), where {w:count(wi-1,w)&gt;0} is the number of word types that can follow wi-1, also is the # of word types we discounted, also is the # of times we applied normalized discount. For general N-gram, Probabilistic Language modeling What: To compute the probability of sentence /sequence of words P(w1, w2, w3…), or to predict upcomming words P(w|w1, w2, w3…)… a language model is also a probability model. Why: the motivation is that probability is essential in identifying information in noisy, ambiguous inputs: speech recognition, machine translation, spelling correction… How: rely on chain rule of probability, the products of a sequence of conditional probability. Simplified by Markov Assumption: approximate the conditional probability by only accounting several prefixes,P(the| water is so transparent that) ≈ P(the| that) Evaluation: how good is the model GENERATIVE PROBABILISTIC MODELSGenerative(joint) models palce probabilities P(c,d) over both observed data d and the hidden variables c (generate the obersved data from hidden stuff). N-Gram Language Model Unigram P(w1,w2,w3..) ≈ P(w1)*P(w2)*P(w3) Bigram P(wn| w1,w2,w3..) ≈ P(wn| wn-1) Estimate probability by counting:P(wi| prefixes) = count(prefixes, wi)/count(prefixes) In practice, use log space to avoid underflow, and adding is faster than multiplying. Insufficient: long-distance dependencies N-grams only work well for word prediction if the test corpus looks like the training corpus. To deal with 0 probability, commonly use Kneser-Ney smoothing, for very large N-grams like web, use stupid backoff. Naive Bayes classifier Application: Text classification, to classify a text, we calculate each class probability given the test sequence, and choose the biggest one. Evaluation: precision, recall, F-measure Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 Text classificationOr text categorization, method is not limited to NB, see lab7.Spam email, gender/authorship/language identification, sentiments analysis,(opinion extraction, subjectivity analysis)… Sentiments analysis For sentiment(or other text classification), word occurrence may matter more than word frequency. Thus it often improves performance to clip the word counts in each document at 1. This variant binary NB is called binary multinominal naive Bayes or binary NB. Remove duplicates in each data sample - bag of words representation, boolean features. Binarized seems to work better than full word counts. Deal with negation: like, not like, A very simple baseline that is commonly used in sentiment to deal with negation is during text normalization to prepend the prefix NOT_ to every word after a token of logical negation Sentiment lexicons: lists of words that are preannotated with positive or negative sentiment. To deal with insufficient labeled training data. A common way to use lexicons in the classifier is to use as one feature the totalcount of occurrences of any words in the positive lexicon, and as a second feature the total count of occurrences of words in the negative lexicon. Using just two features results in classifiers that are much less sparse to small amounts of training data, and may generalize better. See lab8. Naive Bayes Assumptions Bags of words: a set of unordered words/features with its frequency in the documents, their order was ignored. Conditional independence: the probabilities P(w|C) are independence given the class, thus a sequence of words(w1,w2,w3…) probability coculd be estimate via prducts of each P(wi|C) by walking through every pisition of the sequence, noted that the orders in the sequnce does not matter. NB Training Each classes’ prior probability P(C) is the percentage of the classes in the training set. For the test set, its probability as a class j, is the products of its sequence probability P(w1, w2, w3…|Cj) and P(Cj), normalized by the sequence probability P(w1, w2, w3…), which could be calculated by summing all P(w1, w2, w3…|Cj)*P(Cj). The joint features probability P(w1, w2, w3…|C) of each class is calculated by naively multiplying each word’s MLE given that class. In practice, to deal with 0 probability, we dun use MLE, instead we use add alpha smoothing. Why 0 probability matters? Because it makes the whole sequence probability P(w1, w2, w3…|C) 0, then all the other features as evidence for the class are eliminated too. How: first extract all the vocabulary V in the training set. Then, for each feature/word k, its add alpha smoothing probability estimation within a class j is (Njk + alpha)/(Nj+V*alpha). This is not the actual probability, but just the numerator. Naive bayes relationship to language modelling When using all of the words as features for naive bayes, then each class in naive bayes is a unigram languange model. For each word, assign probability P(word|C), For each sentence, assign probability P(S|C) = P(w1,w2,w3…|C) Running multiple languange models(classes) to assign probabilities, and pick out the highest language model. Hidden Markov Model What: The HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence. parameter λ: A Transition probability matrix, B Emission probability Application: part-of-speech tagging, name entity recognition(NEr), parse tree, speech recognition Hidden: these tags, trees or words is not observed(hidden) The three fundamental problems of HMM: decoding: discover the best hidden state sequnce via Viterbi algorithm Probability of the observation: Given an HMM with know parameters λ and an observation sequence O, determine the likelihood P(O| λ) (a language model regardless of tags) via Forward algorithm Learning: Given only the observed sequence, learn the best(MLE) HMM parameters λ via forward-backward algorithm, thus training a HMM is an unsupervised learning task. Parts-of-speech tagging Parts-of-speech(POS), word classes, or syntactic categories, a description of eight parts-of-speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Motivation: Use model to find the best tag sequnce T for an untagged senetnce S: argmax P(T|S) -&gt; argmax P(S|T)*P(T), where P(T) is the transition (prior) probabilities, P(S|T) is the emission (likelihood) probabilities. Parts-of-speech can be divided into two broad supercategories: closed class types and open class types Search for the best tag sequnce: Viterbi algorithm evaluation: tag accuracy Transition probability matrix Tags or states Each (i,j) represent the probability of moving from state i to j When estimated from sequnces, should include beginning and end markers. Tag transition probability matrix: the probability of tag i followed by j Emission probability Also called observation likelihoods, each expressing the probability of an observation j being generated from a states i. Word/symbol Penn Treebank Viterbi algorithm Decoding task: the task of determining which sequence of variables is the underlying source of some sequence of observations. Intuition: The probability of words w1 followed by w2 with tag/state i and j (i,j is index of all Tags), is the chain rule of the probability of i followed by j and the probability of i output wi P(w1|i) and P(w2 |j), then choose the maximum from all the possible i j. Then using chain rule to multiply the whole sequence of words. The value of each cell Vt(j) is computed by recursively taking the most probable path that could lead us to this cell from left columns to right. See exampls in tutorial 2 Since HMM based on Markov Assumptions, so the present column Vt is only related with the nearby left column Vt-1. Forward algorithm Compute the likelihood of a particular observation sequence. Implementation is almost the same as Viterbi. Yet Viterbi takes the max over the previous path probabilities whereas the forward algorithm takes the sum. HMM Traininglearning the parameters of an HMM Forward-backward algorithm inputs: just the observed sequence output: the converged λ(A,B). For each interation k until λ converged: Compute expected counts using λ(k-1) Set λ(k) using MLE on the expected counts. Context-free grammarCFG(phrase-structure grammar) consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered toLexicon gether, and a lexicon of words and symbols. Probabilistic Context-Free GrammarPCFG(Stochastic Context-Free Grammar SCFG (SCFG)), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability. G = (T,N,S,R,P) T, N: Terminal and Non-terminal S: starts symbol R: Derive rule/grammar, N -&gt; N/C P: a probability function, for a given N, ΣP(N-&gt;Ni/Ci)=1. Normally P(S-&gt;NP VP)=1, because this is the only rule for S. PCFG could generates a sentence/tree, thus it is a language model, assigns a probability to the string of words constituting a sentence The probability of a tree t is the product of the probabilities of the rules used to generate it. The probability of the string s is the sum of the probabilities of the trees/parses which have that string as their yield. The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence. Application: Probabilistic parsing Shortage: lack the lexicalization of a trigram model, i.e only a small fraction of the rules contains information about words. To solve this problem, use lexicalized PCFGs Lexicalization of PCFGs The head word of phrase gives a good representation of the phrase’s structure and meaning Puts the properties of words back into a PCFG Word to word affinities are useful for certain ambiguities, because we know the probability of rule with words and words now, e.g. PP attachment ambiguity Recursive Descent Parsing It is a top-down, depth-first parser: Blindly expand nonterminals until reaching a terminal (word). If multiple options available, choose one but store current stateas a backtrack point (in a stack to ensure depth-first.) If terminal matches next input word, continue; else, backtrack can be massively inefficient (exponential in sentence length) if faced with local ambiguity infinite loop CKY parsingDynamic programmingWell-formed substring tableFor parsing, subproblems are analyses of substrings, memoized in well-formed substring table(WFST, chart). Chart entries are indexed by start and end positions in the sentence, and correspond to: either a complete constituent (sub-tree) spanning those positions (if working bottom-up), or a prediction about what complete constituent might be found (if working top-down). The chart is a matrix where cell [i, j] holds information about the word span from position i to position j: The root node of any constituent(s) spanning those words Pointers to its sub-constituents (Depending on parsing method,) predictions about whatconstituents might follow the substring. Probability CKY parsing Noisy channel model: The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. a probability model using Bayesian inference, input -&gt; noisy/errorful encoding -&gt; output, see an observation x (a misspelled word) and our job is to find the word w that generated this misspelled word. P(w|x) = P(x|w)\*P(w)/P(x) Noisy channel model of spelling using naive bayes The noisy channel model is to maximize the product of likelihood(probability estimation) P(s|w) and the prior probability of correct words P(w). Intuitively it is modleing the noisy channel that turn a correct word ‘w’ to the misspelling. The likelihood(probability estimation) P(s|w) is called the the channel/error model, telling if it was the word ‘w’, how likely it was to generate this exact error. The P(w) is called the language model DISCRIMINATIVE PROBABILISTIC MODELSDiscriminative(conditional) models take the data as given, and put a probability over hidden structure given the data, P(c|d). Exponential (Log-linear, MaxEnt, Logistic) modelsMake probability model from the linear combination of weights λ and features f as votes, normalized by the total votes. It is a probabilistic distribution: it estimates a probability for each class/label, aka Softmax. It is a classifier, choose the highest probability label. Application: dependency parsing actions prediction, text classification, Word sense disambiguation Topics categorizationTraining discriminative model Features in NLP are more general, they specify indicator function(a yes/no[0,1] boolean matching function) of properties of the input and each class. Weights: low possibility features will associate with low/negative weight, vise versa. Define features: Pick sets of data points d which are distinctive enough to deserve model parameters: related words, words contians #, words end with ing, etc. Regularization in discriminative modelThe issue of scale: Lots of features sparsity: easily overfitting: need smoothing Many features seen in training never occur again in test Optimization problem: feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. See tutorial 4. Solution: Early stopping Smooth the parameter via L2 regularization. Smooth the data, like the add alpha smoothing, but hard to know what artificial data to create Generative vs. Discriminative Models Navie bayes models multi-count correlated evidence: each feature is multipled in, even when you have multiple features telling the same informaiton. Maxent: solve this issue by weighting features so that model expectations match the observed(empirical) expectations. LINGUISTIC AND REPRESENTATIONAL CONCEPTSRegular Expressionsa language for specifying text search strings. Parsing Parsing is a combination of recognizing an input string and assigning a correct linguistic structure/tree to it based on a grammar. The Syntactic, Statistical parsing are constituent-based representations(context-free grammars). The Dependency Parsing are based on dependency structure(dependency grammars). Syntactic ParsingSyntactic parsing, is the task of recognizing a sentence and assigning a correct syntactic structure to it. Syntactic parsing can be viewed as a search search space: all possible trees generated by the grammar search guided by the structure of the space and the input. search direction top-down: start with root category (S), choose expansions, build down to words. bottom-up: build subtrees over words, build up to S. Search algorithm/strategy: DFS, BFS, Recursive descent parsing, CKY Parsing Challenge: Structual Ambiguity Statistical ParsingOr probabilistic parsing, Build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers. motivation: to solve the problem of disambiguation algorithm: probability CKY parsing evaluation: Compare the output constituency parser with golden standard tree, a constituent(part of the output parser) marked as correct if it spans the same sentence positions with the corresponding constituent in golder standard tree. Then we get the precision, recall and F1 measure. constituency: S-(0:10), NP-(0:2), VP-(0:9)… Precission = (# correct constituents)/(# in parser output), recall = (# correct constituents)/(# in gold standard) Not a good evaluation, because it higher order constituent is marked wrong simply it contains a lower level wrong constituent. Dependency ParsingConstituencyPhrase structure, organizes words into nested constituents. Groups of words behaving as a single units, or constituents. Noun phrase(NP), a sequence of words surrounding at least one noun. While the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase Preposed or Postposed constructions. While the entire phrase can be placed differently, the individual words making up the phrase cannot be. Fallback: In languages with free word order, phrase structure(constituency) grammars don’t make as much sense. Headed phrase structure: many phrase has head, VP-&gt;VB, NP-&gt;NN, the other symbols excepct the head is modifyer. Dependency Dependency structure shows which words depend on (modify or are arguments of) which toehr words. Motivation: In languages with free word order, phrase structure(constituency) grammars don’t make as much sense. E.g. we may need both S → NP VP and S → VP NP, but could not tell too much information simply looking at the rule. Dependencies: Identifies syntactic relations directly. The syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words. Relation between phrase structure and dependency structure Convert phrase structure annotations to dependencies via head rules. (Convenient if we already have a phrase structure treebank.): For a given lexicalized constituency parse(CFG tree), remove the phrasal categories, remove the (duplicated) terminals, and collapse chains of duplicates. The closure of dependencies give constituency from a dependency tree Dependency parsing Motivation: find a final configuration where all the words have been accounted for and an appropriate dependency tree has been synthesized Approach: Transition-based dependency parsing Transition-based dependency parsingtransition-based systems use supervised machine learning methods to train classifiers that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators(actions). Bottom up Like shift-reduce parsing, but the ‘reduce’ actions are specialized to create dependencies with head on left or right. configuration：consists of a stack, an input buffer of words or tokens, and a set of relations/arcs, a set of actions. How to choose the next action: each action is predicted by a discriminative classifier(often SVM, could be maxent) over each legal move. features: a sequence of the correct (configuration, action) pairs f(c ; x). Evaluation: accuracy (# correct dependencies with or ignore label)). Dependency tree Dependencies from a CFG tree using heads, must be projective: There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words. But dependency theory normally does allow non-projective structures to account for displaced constituents. Bounded and unbounded dependenciesUnbounded dependency could be considered as long distance dependency Long-distance dependencies: contained in wh-non-subject-question, “What flights do you have from Burbank to Tacoma Washington?”, the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the VP. AmbiguityStructural ambiguityOccurs when the grammar can assign more than one parse to a sentence. Attachment ambiguityA sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. Coordination ambiguitydifferent sets of phrases can be conjoined by a conjunction like and. E.g green egg and bread. Coordination: The major phrase types discussed here can be conjoined with conjunctions like and, or, and but to form larger constructions of the same type. Global and local ambiguity global ambiguity: multiple analyses for a full sentence, like I saw the man with the telescope local ambiguity: multiple analyses for parts of sentence. the dog bit the child: first three words could be NP (but aren’t). Building useless partial structures wastes time. MorphologyThe study of wordforms and word formation. Challenge of rich MorphologyFor a morphologically rich language, many issues would arise because of the morphological complexity. These productive word-formation processes result in a large vocabulary for these languages Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages For POS, augmentations become necessary when dealing with highly inflected or agglutinative languages with rich morphology like Czech, Hungarian and Turkish., part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than asingle primitive tag. Dependency grammar is better than constituency in dealing with languages that are morphologically rich。 morphemesThe way words are built up from smaller meaning-bearing units. Lemma Lexeme, refers to the set of all the forms that have the same meaning, lemma refers to the particular form that is chosen by convention to represent the lexeme. E.g: run, runs, ran, running are forms of the same lexeme, with run as the lemma. AffixesAdding additional meanings of various kinds. “+ed, un+” suffix : follow the stem Plural of nouns ‘cat+s’ Comparative and superlative of adjectives ‘small+er’ Formation of adverbs ‘great+ly’ Verb tenses ‘walk+ed’ All inflectional morphology in English uses suffixes Prefix: precede the stem In English: these typically change the meaning Adjectives ‘un+friendly’, ‘dis+interested’ Verbs ‘re+consider’ Some language use prefixing much more widely Infix: inserted inside the stem Circumfix: do both(follow, precede) Morphological parsingMethod: Finite-state transducers Finite-state transducersFST, a transducer maps between one representation and another; It is a kind of FSA which maps between two sets of symbols. Root Root, stem and base are all terms used in the literature to designate that part of a word that remains when all affixes have been removed. The root word is the primary lexical unit of a word, and of a word family (this root is then called the base word), which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents. E.g: In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’. Stem Stem is of concern only when dealing with inflectional morphology Stemming: reduce terms to their stems in info retrieval, E.g: In the form ‘untouchables’ the stem is ‘untouchable’, ‘touched’ -&gt; ‘touch’; ‘wheelchairs’ -&gt; ‘wheelchair’. Inflectional nouns for count (plural: +s) and for possessive case (+’s) verbs for tense (+ed, +ing) and a special 3rd person singular present form (+s) adjectives in comparative (+er) and superlative (+est) forms. Derivational Changing the part of speech, e.g. noun to verb: ‘word → wordify’ Changing the verb back to a noun Nominalization: formation of new nouns, often verbs or adjectives Inflectional Morphology vs. Derivational MorphologyInflectional: does not change basic meaning or part of speech expresses grammatical features or relations between words applies to all words of the same part of speech Derivational: may change the part of speech or meaning of a word is not driven by syntactic relations outside the word applies closer to the stem; whereas inflection occurs at word edges: govern+ment+s, centr+al+ize+d Open-class Closed-classClosedClosed classes are those with relatively fixed membership prepositions: on, under, over, near, by, at, from, to, with determiners: a, an, the pronouns: she, who, I, others conjunctions: and, but, or, as, if, when auxiliary verbs: can, may, should, are particles: up, down, on, off, in, out, at, by numerals: one, two, three, first, second, third Open Nouns, verbs, adjectives, adverbs Word senseA discrete representation of an aspect of a word’s meaning.How: Distributional semantic models Word sense disambiguationWSD, The task of selecting the correct sense for a word, formulated as a classification task. Chose features: Directly neighboring words, content words, syntactically related words, topic of the text, part-of-speech tag, surrounding part-of-speech tags, etc … CollocationA sequence of words or terms that co-occur more often than would be expected by chance. Lexical semantic relationshipsRelations between word senses synonym代名词, When two senses of two different words (lemmas) are identical, or nearly identical, the two senses are synonyms. E.g. couch/sofa vomit/throw up filbert/hazelnut car/automobile hyponym下义词, One sense is a hyponym of another sense if the first sense is more specific, denoting a subclass of the other. E.g. car is a hyponym of vehicle; dog is a hyponym of animal, and mango is a hyponym of fruit. hypernymSuperordinate, 上位词, vehicle is a hypernym of car, and animal is a hypernym of dog. similarityOr distance, a looser metric than synonymy.Two ways to measure similarity: Thesaurus词库-based: are words nearby in hypernym hierarchy? Do words have similar definitions? Distributional: do words have similar distributional contexts Distributional semantic modelsVector semantics(embeddings): The meaning of a word is represented as a vector. Two words are similar if they have similar word contexts vector. Term-context matrix(Co-occurrence Matrices): a word/term is defined by a vector over counts of context words. The row represent words, columns contexts. Problem: simple frequency isn’t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. “the” and “of” are very frequent, but maybe not the most discriminative. Sulution: use Pointwise mutual information. Then the Co-occurrence Matrices is filled with PPMI, instead of raw counts. Measuring vectors similarity based on PPMI: Dot product(inner product): More frequent words will have higher dot products, which cause similarity sensitive to word frequency. Cosine: normalized dot product , Raw frequency or PPMI is non-negative, so cosine range [0,1]. Evaluation of similarity Intrinsic: correlation between algorithm and human word similarity ratings. Check if there is correlation between similarity measures and word frequency. Application: sentiment analysis, see lab8 Pointwise mutual informationPMI: do events x and y co-occur more than if they were independent? PMI between two words: Compute PMI on a term-context matrix(using counts):12345PMI(x, y) = log2( N·C(x, y)/C(x)C(y) )p(w=information,c=data) = 6/19p(w=information) = 11/19p(c=data) = 7/19PMI(information,data) = log2(6\*19/(11\*7)) PMI is biased towards infrequent events, solution: Add-one smoothingPPMIPositive PMI, could better handle low frequenciesPPMI = max(PMI,0) t-testThe t-test statistic, like PMI, can be used to measure how muchmore frequent the association is than chance. The t-test statistic computes the difference between observed and expected means, normalized by the variance. The higher the value of t, the greater the likelihood that we can reject the null hypothesis. Null hypothesis: the two words are independent, and hence P(a,b) = P(a)P(b) correctly models the relationship between the two words. Minimum Edit Distancethe minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.Algorithm: searching the shortest path, use Dynamic programming to avoid repeating, (use BFS to search the shortest path?) WordNetA hierarchically organizesd lexical database, resource for English sense relations Synset: The set of near-synonyms for a WordNet sense (for synonym set) Topic modellingInstead of using supervised topic classification – rather not fix topics in advance nor do manual annotation, Use clustering to teases out the topics. Only the number of topics is specified in advance. Latent Dirichlet allocation(LDA): each document may be viewed as a mixture of various topics where each document is generated by LDA. A topic is a distribution over words generate document: Randomly choose a distribution over topics For each word in the document randomly choose a topic from the distribution over topics randomly choose a word from the corresponding topic (distribution over the vocabulary) training: repeat until converge assign each word in each document to one of T topics. For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t) Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d) Meaning representation languageThe symbols in our meaning representations correspond to objects, properties, and relations in the world. Qualifications of MRL: Canonical form: sentences with the same (literal) meaning should have the same MR. Compositional: The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined. Verifiable: Can use the MR of a sentence to determine whether the sentence is true with respect to some given model of the world. Unambiguous: an MR should have exactly one interpretation. Inference: we should be able to verify sentences not only directly, but also by drawing conclusions based on the input MR and facts in the knowledge base. Expressivity: the MRL should allow us to handle a wide range of meanings and express appropriate relationships between the words in a sentence. Good MRL: First-order Logic First-order LogicFOL, Predicate logic, meets all of the MRL qualifications except compositionality. Expressions are constructed from terms: constant and variable symbols that represent entities function symbols that allow us to indirectly specify entities predicate symbols that represent properties of entities and relations between entities Terms can be combined into predicate-argument structures Logical connectives: ∨ - or, ∧ - and, ¬, ⇒ Quantifiers: ∀ (universal quantifier, i.e., “for all”), ∃ (existentialquantifier, i.e. “exists”) Predicates in FOL Predicates with multiple arguments represent relations between entities: member-of(UK, EU) “/N” to indicate that a predicate takes N arguments: member-of/2 Variables in FOL An expression consisting only of a predicate with a variable among its arguments is interpreted as a set: likes(x, Gim) is the set of entities that like Gim. A predicate with a variable among its arguments only has a truth value if it is bound by a quantifier: ∀x.likes(x, Gim) has an interpretation as either true or false. Universal Quantifier (∀): Cats are mammals has MR ∀x.cat(x) ⇒ mammal(x) Existential Quantifier (∃): Used to express that a property/relation is true of some entity, without specifying which one: Marie owns a cat has MR ∃x.cat(x) ∧ owns(Marie,x) Lambda λ ExpressionExtend FOL, to work with ‘partially constructed’ formula, Compositionality. E.g.： λx.sleep(x) is the function that takes an entity x to the FOL expression sleep(x). λx.sleep(x)(Marie) -&gt; sleep(Marie) Verbal (event) MRs： λz. λy. λx. Giving1(x,y,z) (book)(Mary)(John) -&gt; Giving1(John, Mary, book) -&gt; John gave Mary a book Problem: fixed arguments Requires separate Giving predicate for each syntactic subcategorisation frame(number/type/position of arguments). Separate predicates have no logical relation: if Giving3(a, b, c, d, e) is true, what about Giving2(a, b, c, d) and Giving1(a, b, c). Solution: Reification of events 事件具象化 Reification of eventsJohn gave Mary a book -&gt; ∃e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Reify: to “make real” or concrete, i.e., give events the same status asentities. In practice, introduce variables for events, which we can quantify over Entailment relations: automatically gives us logical entailment relations between events1234[John gave Mary a book on Tuesday] -&gt; [John gave Mary a book]∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) ∧ Time(e, Tuesday)-&gt;∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Semantic ParsingAka semantic analysis. Systems for mapping from a text string to any logical form. Motivation: deriving a meaning representation from a sentence. Application: question answering Method: Syntax driven semantic analysis with semantic attachments Syntax Driven Semantic Analysis Principle of compositionality: the construction of constituent meaning is derived from/composed of the meaning of the constituents/words within that constituent, guided by word order and syntactic relations. Build up the MR by augmenting CFG rules with semantic composition rules. Add semantic attachments to CFG rules. Problem: encounter invalide FOL for some (base-form) MR, need type-raise. Training Semantic attachmentsE.g123456VP → Verb NP : &#123;Verb.sem(NP.sem)&#125;Verb.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y)NP.sem = Meat-&gt;VP.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) (Meat)= λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) The MR for VP, is computed by applying the MR function to VP’s children. Complete the rule:123456S → NP VP : &#123;VP.sem(NP.sem)&#125;VP.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat)NP.sem = AyCaramba-&gt;S.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) (AyCa.)= ∃e. Serving(e) ∧ Server(e, AyCaramba) ∧ Served(e, Meat) Lexical semanticsthe meaning of individual words. EVALUATION CONCEPTS AND METHODSInstrinsic vs. extrinsic evaluationExtrinsicUse something external to measure the model. End-to-end evaluation, the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Put each model in a task: spelling corrector, speech recognizer, MT system Run the task, get an accuracy for A and for B How many misspelled words corrected properly How many words translated correctly Compare accuracy for A and B Unfortunately, running big NLP systems end-to-end is often very expensive. IntrinsicMeasures independenly to any application. Train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. Which means whichever model assigns a higher probability to the test set Perplexity It is intrinsic. Intuition based on Shannon game:The best language model is one that best predicts an unseen test set(e.g. next word), gives the highest P(sentence) to the word that actually occurs. Definition： Perplexity is the inverse probability of the test set, normalized by the number of words(lie between 0-1). So minimizing perplexity is the same as maximizing probability Cannot divide 0, so use smoothing. Bad approximation: unless the test data looks just like the training data, so generally only useful in pilot experiments. Human evaluationE.g to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying togold labels match. We will refer to these human labels as the gold labels. Precision, Recall, F-measure To deal with unbalanced lables Application: text classification, parsing. Evaluation in text classification: the 2 by 2 contingency table, golden lable is true or false, the classifier output is positive or negative. Precision% of positive items that are golden correct, from the view of classifier Recall% of golden correct items that are positive, from the view of test set. F-measure Motivation: there is tradeoff between precision and recall, so we need a combined meeasure that assesses the P/R tradeoff. The b parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of b &gt; 1 favor recall, while values of b &lt; 1 favor precision. Balanced F1 measure with beta =1, F = 2PR/(P+R) Confusion matrixRecalled that confusion matrix’s row represent golden label, column represent the classifier’s output, to anwser the quesion：for any pair of classes(c1,c2), how many test sample from c1 were incorrectly assigned to c2&gt; Recall: Fraction of samples in c1 classified correctly, CM(c1,c1)/sum(CM(c1,:)) Precision: fraction of samples assigned c1 that are actually c1, CM(c1,c1)/sum(CM(:,c1)) Accuracy: sum of diagnal / all CorrelationWhen two sets of data are strongly linked together we say they have a High Correlation.Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases. Pearson correlation: covariance of the two variables divided by the product of their standard deviations. Spearman correlation: the Pearson correlation between the rank values of the two variables]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 11 - Java | 测试 Testing - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-11-testing%2F</url>
    <content type="text"><![CDATA[测试如何知道自己的程序是否真的在工作？在现实世界中，程序员相信他们的代码，因为代码通过了他们自己编写的测试。常用的测试有 Ad Hoc Testing， Unit test 和 Integration Testing。 Ad Hoc Testing，是指没有计划和记录的软件测试，除非发现缺陷，不然一般只运行一次。 Unit test程序可分解为单元（或程序中可测试的最小部分），它严格测试代码的每个单元，最终确保项目正确运行。好处是： Unit test 保证良好的代码结构（每个 method “只打一份工”），帮助我们较好地解析任务， 允许我们考虑每个方法的所有边界情况，并单独测试它们。 让我们每次只专注于一个单元，进行测试，debug，对准确度有信心后，再进行下一个单元的开发。相比于一次性写完所有代码，再测试debug，Unit test 减少了 debugging 时间。 坏处是： 测试也要花时间 测试本身也是有可能出错的，测试可能不全面，不规范，或者有bug 有些单元是依赖于其他单元的 Unit testing 无法保证各个模块的交互，无法保证整个系统作为一个整体是否正常工作。 JUnitJUnit是一个给Java做测试的框架，由Erich Gamma（Design Patterns）和Kent Beck（eXtreme Programming）编写。JUnit使用Java的 reflection 功能（Java程序可以检查自己的代码）和注释。JUnit允许我们： 定义并执行测试和测试套件 使用测试作为规范的有效手段 使用测试来支持重构 将修改的代码集成到构建中JUnit可用于多个IDE，例如BlueJ，JBuilder和Eclipse在一定程度上具有JUnit集成。 1234567import org.junit.Test;import static org.junit.Assert.*;@Testpublic void testMethod() &#123; assertEquals(&lt;expected&gt;, &lt;actual&gt;);&#125; assertEquals测试一个变量的实际值是否等于它的期望值。JUnit test 各个测试方法，必须是非静态的（JUnit的设计人员设计规定的）。 Integration Testing鉴于 Unit testing 无法保证，有交互的多个模块，作为一个整体是否正常工作。我们可能需要 integration testing，把各个模块合并，作为一个组合，进行测试（也可以把 Unit test 组合起来变成 integration testing）。 Integration testing 一般都比较麻烦，也不容易自动化，而且一般是在比较高的抽象层进行测试，可能会漏掉微小的错误。 当把所有模块都作为一个整体，也就是整个系统作为测试对象时，就是 system testing。 Test driven developmentTDD开发步骤： 明确一项新功能需求。 为该功能编写 Unit test。 运行测试，按理应该无法通过测试（因为还没写功能程序）。 编写通过实现该功能的代码，通过测试。 可选：重构代码，使其更快，更整洁等等。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 10 - Java | LinkedList 还是 ArrayList - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-10-java-which-list%2F</url>
    <content type="text"><![CDATA[Java 提供了 ArrayList, ArrayDeque 和 LinkedList 几个API.队列 queue, 通俗的含义, 就是不能插队, 只能在末尾插入.Deque 就是双端队列 Double Ended Queue。双端队列是具有动态大小的序列容器，可以在两端（前端或后端）扩展或收缩（定义来源 cplusplus.com）. CS61b的project 1a就是实现两种双端队列（array based 和 linkedklist based）. 不同的API, 在考虑什么时候应该用哪个时, 我们需要考虑它们的性能差异: 搜索/定位：与LinkedList搜索操作相比，ArrayList搜索操作更快。 ArrayList的get(int index)性能是O(1)的，而LinkedList的性能是O(n)。因为ArrayList基于array数据结构，可以直接用靠 array index 索引元素。 删除/插入：LinkedList 操作性能是O(1)，而ArrayList的性能从O(n)（删除/插入第一个元素）到O(n)（最后一个元素）都有可能。因为LinkedList的每个元素都包含两个指向其相邻前后元素的指针（地址），因此仅需要改变，被删节点的prev和next指针位置。而在ArrayList中，需要移动剩余元素，来重新填充array空间。 内存开销：LinkedList的每个元素都有更多的内存开销(额外的指针), 而ArrayLists没有这个开销。但是，ArrayLists需要占用初始容量。一般ArrayList的默认初始容量非常小（Java 1.4 - 1.8使用10）。但是，往ArrayLists添加元素时， 它可能会适当地增大容量，所以如果添加了很多元素，则必须不断调整数组的大小，那样也可能会导致元素频繁挪动位置。 综上所述： 如果在应用中需要频繁插入和删除，那么选择LinkedList。 假如一开始，就知道后面要添加大量元素，那就使用较高的初始容量来构造ArrayList。 大部分用例中, 相比LinkedList, 人们更偏爱ArrayList以及ArrayDeque。如果你不确定应该选哪个, 那么就直接考虑ArrayList吧(参考).]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抽象编程 - C++ 算法与数据结构 Stanford cs106b]]></title>
    <url>%2FNOTE-CS106B-Programming-Abstractions-Stanford%2F</url>
    <content type="text"><![CDATA[Note CS106B Stanford Programming AbstractionsTopics:Recursion, algorithms analysis (sort/search/hash), dynamic data structures (lists, trees, heaps), data abstraction (stacks, queues, maps), implementation strategies/tradeoffs Purposes: become acquainted with the C++ programming language learn more advanced programming techniques explore classic data structures and algorithms and apply these tools to solving complex problemsReference Text Book: Data Structures &amp; Algorithm Analysis in C++, 4th ed, by Mark A. Weiss Text Book: Programming Abstractions in C++ 1st Edition by Eric Roberts Text Book: Algorithms, 4th Edition Blog: Red Blob Games, Amit’s A* Pages Coding style Works correctly in all situations: Using a listing of specific test cases to exercise the program on. The overall approach is straight-forward, data structure is cleanly organized, tasks are nicely decomposed, algorithms are clear and easy to follow, comments are helpful, layout is consistent.CommentingExamples of information you might include in comments: General overview. What are the goals and requirements of this program? this function? The overview comment should also contain author and version information: who worked on this file and when. Data structures. How is the data stored? How is it ordered, searched, accessed? Design decisions. Why was a particular data structure or algorithm chosen? What other strategies were tried and rejected? Error handling. How are error conditions handled? What assumptions are made? What happens if those assumptions are violated? Nitty-gritty code details. Comments are invaluable for explaining the inner workings of particularly complicated (often labeled “clever”) paths of the code. Planning for the future. How might one make modifications or extensions later? And more… (This list is by no means exhaustive) ADTDefinitionAn abstract data type is a set of objects together with a set of operations. Abstract data types are mathematical abstractions; nowhere in an ADT’s definition is there any mention of how the set of operations is implemented.Objects such as lists, sets, and graphs, along with their operations, can be viewed as ADTs.Also there are search tree, set, hash table, priority queue. Client uses class as abstraction Invokes public operations only Internal implementation not relevant! Client can’t and shouldn’t muck with internals Class data should private Imagine a “wall” between client and implementor Wall prevents either from getting involved in other’s business Interface is the “chink” in the wall Conduit allows controlled access between the two Consider Lexicon Abstraction is a word list, operations to verify word/prefix How does it store list? using array? vector? set? does it matter to client? Why ADTs? Abstraction: Client insulated from details, works at higher-level Encapsulation: Internals private to ADT, not accessible by client Independence: Separate tasks for each side (once agreed on interface) Flexibility: ADT implementation can be changed without affecting client Vector and list in the STLThe C++ language includes, in its library, an implementation of common data structures.This part of the language is popularly known as the Standard Template Library (STL). In general, these data structures are called collections or containers. IteratorsIn the STL, a position is represented by a nested type, iterator. Getting an Iterator iterator begin( ) returns an appropriate iterator representing the first item in thecontainer. iterator end( ) returns an appropriate iterator representing the endmarker in thecontainer (i.e., the position after the last item in the container). Iterator Methods itr++ and ++itr advances the iterator itr to the next location. Both the prefix and postfix forms are allowable. itr returns a reference to the object stored at iterator itr’s location. The reference returned may or may not be modifiable (we discuss these details shortly). itr1==itr2 returns true if iterators itr1 and itr2 refer to the same location and false otherwise. itr1!=itr2 returns true if iterators itr1 and itr2 refer to a different location and false otherwise. Container Operations that require IteratorsThe three most popular methods that require iterators are those that add or remove from the list (either a vector or list) at a specified position: iterator insert( iterator pos, const Object &amp; x ): adds x into the list, prior to theposition given by the iterator pos. This is a constant-time operation for list, but not forvector. The return value is an iterator representing the position of the inserted item. iterator erase( iterator pos ): removes the object at the position given by the iterator. This is a constant-time operation for list, but not for vector. The return value is the position of the element that followed pos prior to the call. This operation invalidates pos, which is now stale, since the container item it was viewing has been removed. iterator erase( iterator start, iterator end ): removes all items beginning at position start, up to, but not including end. Observe that the entire list can be erased by the call c.erase( c.begin( ), c.end( ) ) Range for loopC++11 also allows the use of the reserved word auto to signify that the compiler will automatically infer the appropriate type, for simple data type: 12for( auto x : squares ) cout&lt;&lt; x; for complicate data type like map: Each element of the container is a map&lt;K, V&gt;::value_type, which is a typedef for std::pair&lt;const K, V&gt;. Consequently, you’d write this as 123for (auto&amp; kv : myMap) &#123; std::cout &lt;&lt; kv.first &lt;&lt; " has value " &lt;&lt; kv.second &lt;&lt; std::endl;&#125; RecursionHelper Function No clear definition of helper function How to utilize helper function to help constructing recursion algarithm: construct a same-name recursive function with extra parameters to pass in. In some other cases, decomposition with several step into a function is itself a helper function, which help to make the main function simple and clean. Exhaustive recursionPermutations/subsets are about choice Both have deep/wide tree of recursive calls Depth represents total number of decisions made Width of branching represents number of available options per decision Explores every possible option at every decision point, typically very expensive, N! permutations, 2N subsets Recursive BacktrackingPartial exploration of exhaustive space. In the case that if we are interested in finding any solution, whichever one that works out first is fine. If we eventually reach our goal from here, we have no need to consider the paths not taken. However, if this choice didn’t work out and eventually leads to nothing but dead ends; when we backtrack to this decision point, we try one of the other alternatives. The back track based on the stacks of recursion, if a stack return false (or fail result), we back to previous stack and try another way(un-making choice). Need something return(normally bool) to step out of the entire recursion once any one solution found. One great tip for writing a backtracking function is to abstract away the details of managing the configuration (what choices are available, making a choice, checking for success, etc.) into other helper functions so that the body of the recursion itself is as clean as can be. This helps to make sure you have the heart of the algorithm correct and allows the other pieces to be developed, test, and debugged independently. PointerlvalueIn C++, any expression that refers to an internal memory location capable of storing data is called an lvalue (pronounced “ell-value”).x = 1.0; Declaring pointer variables123456789101112131415161718192021222324252627282930313233int main() &#123; -------------------------------------------------- // Declaration, in the stack // Not yet initialized! int num; int *p, *q; // If cout &lt;&lt; num &lt;&lt; p &lt;&lt; q &lt;&lt; endl; // There will be junk number, junk address. // If now *p=10, it may blow up, because what *p point to is an address points to somewhere around that could be invalid. --------------------------------------------------- // new operator allocate memory from the heap, returns address p = new int; // P -----&gt; [ int ] （heep 1000） *p = 10; // P -----&gt; [ 10 ] （heep 1000） q = new int; // P -----&gt; [ int ] （heep 1004） *q = *p; // q -----&gt; [ 10 ] （heep 1004） q = p; // q -----&gt; [ 10 ] （heep 1000） // [ 10 ] （heep 1004） became orphan, and could not be reclaim back --------------------------------------------------- delete p; // [ 10 ] （heep 1000）memory was reclaimed and free, // and available for others as [ ]（heep 1000）, // but p still hold the address delete q; // bad idea, [ 10 ]（heep 1000） already been reclaimed! q = NULL; // NULL is zero pointer, means the pointer does not hold any address, // used as sentinel value, sometimes better than delete. // Accessing "deleted" memory has unpredictable consequences --------------------------------------------------- // int *p declaration reserves only a single word, which is large enough to hold a machine address. // ≠ // int *p = NULL declare pointer p as nullptr --------------------------------------------------- (*newOne).name = name // "." &gt; "*" newOne-&gt;name = name Use of pointerBig program that contains a certain amout of classes and objects that are share some relationship. Instead of copying data from each other, using pointer to point to specific data is better: Saves space by not repeating the same information. If some objects gets new information to update, change in one place only! Dynamic allocation Request memoryTo acquire new memory when you need it and to free it explicitly when it is no longer needed. Acquiring new storage when the program is running. While the program is running, you can reserve part of the unallocated memory, leaving the rest for subsequent allocations.The pool of unallocated memory available to a program is called the heap.int *p = new int; //new operator to allocate memory from the heapIn its simplest form, the new operator takes a type and allocates space for a variable of that type located in the heap.The call to new operator will return the address of a storage location in the heap that has been set aside to hold an integer. Free occupied memoryDelete which takes a pointer previously allocated by new and returns the memory associated with that pointer to the heap. TreeTree terminology Node, tree, subtree, parent, child, root, edge, leaf For any node ni, the depth of ni is the length of the unique path from the root to ni. The height of ni is the length of the longest path from ni to a leaf Rules for all trees Recursive branching structure Single root node Every node reachable from root by unique path Binary treeEach node has at most 2 children. Binary search tree All nodes in left subtree are less than root, all nodes in right subtree are greater. Arranged for efficient search/insert. It is the basis for the implementation of two library collections classes, set and map. Most operations’ average running time is O(log N). Operating on trees Many tree algorithms are recursive Handle current node, recur on subtrees Base case is empty tree (NULL) Tree traversals to visit all nodes, order of traversal: Pre: cur, left, right In: left, cur, right Post: left, right, cur Others: level-by-level, reverse orders, etc Balanced Search TreesBinary search tree have poor worst-case performance.To make costs are guaranteed to be logarithmic, no matter what sequence of keys is used to construct them, the ideal is to keep binary search trees perfectly balanced. Unfortunately, maintaining perfect balance for dynamic insertions is too expensive. So consider data structure that slightly relaxes the perfect balance requirement to provide guaranteed logarithmic performance not just for the insert and search operations, but also for all of the ordered operations (except range search). AVL treeAdelson-Velskii and Landis tree is a binary search tree with a balance condition. Track balance factor for each node: Height of right subtree - height of left subtree information is kept for each node (in the node structure) For every node in the tree, the height of the left and right subtrees can differ by at most 1 (Balance factor = 0 or 1). When balance factor hits 2, restructure Rotation moves nodes from heavy to light side Local rearrangement around specific node When finished, node has 0 balance factor Single rotation: one time rotation between new insert node and its parent node Double rotation: two single rotation of the new insert node 2-3 treesAllow the nodes in the tree to hold more than one key: 3-nodes, which hold three links and two keys. Definition: A 2-3 search tree is a tree that is either empty or A 2-node, with one key (and associated value) and two links, a left link to a 2-3 search tree with smaller keys, and a right link to a 2-3 search tree with larger keys A 3-node, with two keys (and associated values) and three links, a left link to a 2-3 search tree with smaller keys, a middle link to a 2-3 search tree with keys between the node’s keys, and a right link to a 2-3 search tree with larger keys A perfectly balanced 2-3 search tree is one whose null links are all the same distance from the root. The concept guarantee that search and insert operations in a 2-3 tree with N keys are to visit at most lg N nodes. But its dicrect implementation is inconvenient: Not only is there a substantial amount of code involved, but the overhead incurred could make the algorithms slower than standard BST search and insert. Consider a simple representation known as a red-black BST that leads to a natural implementation. Priority QueuesA priority queue is a data structure that allows at least the following two operations: insert, and deleteMin, which finds, returns, and removes the minimum element in the priority queue. Binary HeapA heap is a binary tree that is completely filled, with the possible exception of the bottom level, which is filled from left to right. Such a tree is known as a complete binary tree. Structure A heap data structure consist of an array (of Comparable objects) and an integer representing the current heap size. For any element in array position i, the left child is in position 2i, the right child is in the cell after the left child [2i + 1], and the parent is in position [i/2]. Heap-Order Property For every node X, the key in the parent of X is smaller than (or equal to) the key in X. So to make find minimum operation quick. Basic Heap Operation insert: To insert an element X into the heap, create a hole in the next available location. Then Percolate up - swap X with its parent index (i/2) so long as X has a higher priority than its parent. Continue this process until X has no more lower priority parent. 1234567//Percolate upint hole = ++size; binaryQueue[0]=std::move(*newOne); for (;(priority&lt;binaryQueue[hole/2].priority || (priority==binaryQueue[hole/2].priority &amp;&amp; name&lt;binaryQueue[hole/2].name) );hole/=2) &#123; binaryQueue[hole] = std::move(binaryQueue[hole/2]); &#125; binaryQueue[hole] = std::move(binaryQueue[0]); deleteMin: When the minimum is removed, a hole is created at the root. Move the last element X in the heap to place in the root hole. Then Percolate down - swapp X with its more urgent-priority child [index (i2 or i2+1)] so long as it has a lower priority than its child. Repeat this step until X has no more higher priority child. 1234567891011//Percolate downint child; for (; hole*2&lt;=size;hole=child) &#123; child = hole*2; if ( child!=size &amp;&amp; (binaryQueue[child+1].priority&lt;binaryQueue[child].priority || (binaryQueue[child+1].priority==binaryQueue[child].priority &amp;&amp; binaryQueue[child+1].name&lt;binaryQueue[child].name)) ) ++child; if ( binaryQueue[child].priority&lt;priority_tobePerD || (binaryQueue[child].priority==priority_tobePerD &amp;&amp; binaryQueue[child].name&lt;name_tobePerD) ) &#123; binaryQueue[hole] = std::move(binaryQueue[child]); &#125; else break; &#125; Use integer division to avoid even odd index. Algorithm AnalysisSpace/time, big-O, scalability Big-O Computational complexity: The relationship between N and the performance of an algorithm as N becomes large Big-O notation: to denote the computational complexity of algorithms. Standard simplifications of big-O Eliminate any term whose contribution to the total ceases to be significant as N becomes large. Eliminate any constant factors. Worst-case versus average-case complexityAverage-case performance often reflects typical behavior, while worst-case performance represents a guarantee for performance on any possible input. Predicting computational complexity from code structure Constant time: Code whose execution time does not depend on the problem size is said to run in constant time, which is expressed in big-O notation as O(1). Linear time: function that are executed exactly n times, once for each cycle of the for loop, O(N) Quadratic time: Algorithms like selection sort that exhibit O(N2) performance are said to run in quadratic tim For many programs, you can determine the computational complexity simply by finding the piece of the code that is executed most often and determining how many times it runs as a function of N Space/time In general, the most important measure of performance is execution time. It also possible to apply complexity analysis to the amount of memory space required. Nowadays the memory is cheap, but it still matters when designing extreamly big programs, or APPs on small memory device, such as phones and wearable devices. SortingThere are lots of different sorting algoritms, from the simple to very complex. Some optimized for certain situations (lots of duplicates, almost sorted, etc.). So why do we need multiple algorithms? Selection sort Select smallest and swap to front/backend 12345678910void SelectionSort(Vector&lt;int&gt; &amp;arr)&#123; for (int i = 0; i &lt; arr.size()-1; i++) &#123; int minIndex = i; for (int j = i+1; j &lt; arr.size(); j++) &#123; if (arr[j] &lt; arr[minIndex]) minIndex = j; &#125; Swap(arr[i], arr[minIndex]); &#125; Selection sort analysisCount work inside loops: First iteration does N-1 compares, second does N-2, and so on. One swap per iteration O(N2) Insertion sort As sorting hand of just-dealt cards, each subsequent element inserted into proper place Start with first element (already sorted) Insert next element relative to first Repeat for third, fourth, etc. Slide elements over to make space during insert123456789void InsertionSort(Vector&lt;int&gt; &amp;v)&#123; for (int i = 1; i &lt; v.size(); i++) &#123; int cur = v[i]; // slide cur down into position to left for (int j=i-1; j &gt;= 0 &amp;&amp; v[j] &gt; cur; j--) v[j+1] = v[j]; v[j+1] = cur; &#125;&#125; Insertion sort analysisBecause of the nested loops, each of which can take N iterations, insertion sort is O(N2). HeapsortPriority queues can be used to sort in O(N log N) time. The algorithm based on this idea is known as heapsort. Heapsort analysisThe building of the heap, uses less than 2N comparisons. In the second phase, the ith deleteMax uses at most less than 2*log (N − i + 1) comparisons, for a total of at most 2N log N − O(N) comparisons (assuming N ≥ 2). Consequently, in the worst case, at most 2N log N − O(N) comparisons are used by heapsort. Merge sort Inspiration: Algorithm like selection sort is quadratic growth (O(N2)). Double input -&gt; 4X time, halve input -&gt; 1/4 time.Can recursion save the day? If there are two sorted halves, how to produce sorted full result? Divide and conquer algorithm Divide input in half Recursively sort each half Merge two halves together “Easy-split hard-join” No complex decision about which goes where, just divide in middle Merge step preserves ordering from each half Merge depends on the fact that the first element in the complete ordering must be either the first element in v1 or the first element in v2, whichever is smaller. 12345678910111213141516171819202122232425void MergeSort(Vector&lt;int&gt; &amp;v)&#123; if (v.size() &gt; 1) &#123; int n1 = v.size()/2; int n2 = v.size() - n1; Vector&lt;int&gt; left = Copy(v, 0, n1); Vector&lt;int&gt; right = Copy(v, n1, n2); MergeSort(left); MergeSort(right); v.clear(); Merge(v, left, right); &#125;&#125;void Merge(Vector&lt;int&gt; &amp;v,Vector&lt;int&gt; &amp;left,Vector&lt;int&gt; &amp;right) &#123; int l=0, r=0; while(l&lt;left.size() &amp;&amp; r&lt;right.size()) &#123; if (left[l]&lt;right[r]) v.add(left[l++]); else v.add(right[r++]); &#125; while(l&lt;left.size()) v.add(left[l++]); while(r&lt;right.size()) v.add(right[r++]);&#125; Mergesort analysisThe time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge, which is linear. T(N) = N + 2T(N/2). log N levels * N per level= O(NlogN). Mergesort uses the lowest number of comparisons of all the popular sorting algorithms.Theoretical result show that no general sort algorithm could be better than NlogN.But there is still better in practice: The running time of mergesort, when compared with other O(N log N) alternatives, depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array). These costs are language dependent. In Java, when performing a generic sort (using a Comparator), an element comparison can be expensive, but moving elements is cheap (because they are reference assignments, rather than copies of large objects). In C++, in a generic sort, copying objects can be expensive if the objects are large, while comparing objects often is relatively cheap because of the ability of the compiler to aggressively perform inline optimization. QuicksortMost sorting programs in use today are based on an algorithm called Quicksort, which employs a Divide and conquer strategy as merge sort, but instead take a different approach to divide up input vector into low half and high half. Quicksort uses a few more comparisons, in exchange for significantly fewer data movements. The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efficiently. “Hard-split easy-join”, Each element examined and placed in correct half, so that join step become trivial. Choose an element (pivot) to serve as the boundary between the small and large elements. Partitioning: Rearrange the elements in the vector so that all elements to the left of the boundary are less than the pivot and all elements to the right are greater than or possibly equal to the pivot. Sort the elements in each of the partial vectors.12345678void Quicksort(Vector&lt;int&gt; &amp;v, int start, int stop)&#123; if (stop &gt; start) &#123; int pivot = Partition(v, start, stop); Quicksort(v, start, pivot-1); Quicksort(v, pivot+1, stop); &#125;&#125; Quicksort performance analysisThe running time of quicksort is equal to the running time of the two recursive calls plus the linear time spent in the partition (the pivot selection takes only constant time). T(N) = T(i) + T(N − i − 1) + cN, where i = |S1| is the number of elements in S1.There are thre cases Ideal 50/50 split: The pivot is in the middle, T(N) = cN + 2T(N/2) =&gt; O(NlogN) Average bad 90/10 split: N per level, but more levels, solve N*(9/10)k = 1, still k = O(NlogN) Worst N-1/1 split: The pivot is the smallest element, all the time. Then i = 0, T(N) = T(N − 1) + cN, N &gt; 1. With N levels! O(N2) In a vector with randomly chosen elements, Quicksort tends to perform well, with an average-case complexity of O(N log N). In the worst case — which paradoxically consists of a vector that is already sorted — the performance degenerates to O(N2). Despite this inferior behavior in the worst case, Quicksort is so much faster in practice than most other algorithms that it has become the standard. Design StrategyWhen an algorithm is given, the actual data structures need not be specified. It is up to the programmer to choose the appropriate data structure in order to make the running time as small as possible. There are many to be considered: algorithms, data structure, space-time tradeoff, code complexity. Dynamic ProgrammingTo solve optimization problems in which we make a set of choices in order to arrive at an optimal solution. As we make each choice, subproblems of the same form often arise. Dynamic programming is effective when a given subproblem may arise from more than one partial set of choices; the key technique is to store the solution to each such subproblem in case it should reappear. Unlike divide-and-conquer algorithms which partition the problem into disjoint subproblems, dynamic programming applies when the subproblems overlap. “Programming” in this context refers to a tabular method. When should look for a dynamic-programming solution to a problem? Optimal substructure: a problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. Overlapping subproblems: When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problemhas overlapping subproblems. In contrast, a problem for which a divide-andconquer approach is suitable usually generates brand-new problems at each step of the recursion. General setps of Dynamic Programming Characterize the structure of an optimal solution. Recursively define the value of an optimal solution. Compute the value of an optimal solution, typically in a bottom-up fashion. Construct an optimal solution from computed information. Greedy AlgorithmsGreedy algorithms work in phases. In each phase, a decision is made in a locally optimal manner, without regard for future consequences. When the algorithm terminates, we hope that the local optimum is equal to the global optimum. If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution. Huffman Codes A Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations. We merely select the two smallest trees. If we maintain the trees in a priority queue, ordered by weight, then the running time is O(C logC), since there will be one buildHeap, 2C − 2 deleteMins, and C − 2 inserts. A simple implementation of the priority queue, using a list, would give an O(C2) algorithm. The choice of priority queue implementation depends on how large C is. In the typical case of an ASCII character set, C is small enough that the quadratic running time is acceptable. Divide and ConquerTraditionally, routines in which the text contains at least two recursive calls and subproblems be disjoint (that is, essentially nonoverlapping) are called divide-and-conquer algorithms. Divide: Smaller problems are solved recursively (except, of course, base cases). Conquer: The solution to the original problem is then formed from the solutions to the subproblems.We have already seen several divide-and-conquer algorithms: mergesort and quicksort, which have O(N log N) worst-case and averagecase bounds, respectively. Backtracking AlgorithmsSee Recursive BacktrackingIn some cases, the savings over a brute-force exhaustive search can be significant.The elimination of a large group of possibilities in one step is known as pruning. How to evaluate/compare alternatives Often interested in execution performance: Time spent and memory used Should also consider ease of developing, verifying, maintaining codeQuicksort strategy Picking the pivotPicking a good pivot improves performance, but also costs some time. If the algorithm spends more time choosing the pivot than it gets back from making a good choice, you will end up slowing down the implementation rather than speeding it up. The popular, uninformed choice is to use the first element as the pivot. This is acceptable if the input is random, but if the input is presorted or in reverse order, then the pivot provides a poor partition. A safe approach is to choose the pivot element randomly. On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all. A good estimate can be obtained by picking three elements randomly and using the median of these three as pivot. The randomness turns out not to help much, so the common course is to use as pivot the median of the left, right, and center elements. Quicksort partitioning strategyA known method that is very easy to do it wrong or inefficiently. General process: The first step is to get the pivot element out of the way by swapping it with the last element. Two pointers, i point to the first element and j to the next-to-last element. What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part. “Small” and “large” are relative to the pivot. While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot. We move j left, skipping over elements that are larger than the pivot. When i and j have stopped, i is pointing at a large element and j is pointing at a small element. If i is to the left of j (not yet cross), those elements are swapped. Repeat the process until i and j cross The final is to swap the pivot element with present i element One important detail we must consider is how to handle elements that are equal to the pivot? Suppose there are 10,000,000 elements, of which 500,000 are identical (or, more likely, complex elements whose sort keys are identical). To get an idea of what might be good, we consider the case where all the elements in the array are identical. If neither i nor j stops, and code is present to prevent them from running off the end of the array, no swaps will be performed. Although this seems good, a correct implementation would then swap the pivot into the last spot that i touched, which would be the next-to last position (or last, depending on the exact implementation). This would create very uneven subarrays. If all the elements are identical, the running time is O(N2). If both i and j stop, there will be many swaps between identical elements. The partition creates two nearly equal subarrays. The total running time would then be O(N log N). Thus it is better to do the unnecessary swaps and create even subarrays than to risk wildly uneven subarrays. Small arrays For very small arrays (N ≤ 20), quicksort does not perform as well as insertion sort. Furthermore, because quicksort is recursive, these cases will occur frequently. A common solution is not to use quicksort recursively for small arrays, but instead use a sorting algorithm that is efficient for small arrays, such as insertion sort. A good cutoff range is N = 10, although any cutoff between 5 and 20 is likely to produce similar results. This also saves nasty degenerate cases, such as taking the median of three elements when there are only one or two. Text editor case study Buffer requirements Sequence of characters + cursor position Operations to match commands above What to consider? Implementation choices performance implications Buffer class interface 1234567891011121314class Buffer &#123; public: Buffer(); ~Buffer(); void moveCursorForward(); void moveCursorBackward(); void moveCursorToStart(); void moveCursorToEnd(); void insertCharacter(char ch); void deleteCharacter(); void display(); private: // TBD!&#125;; Buffer layered on Vector Need character data + cursor Chars in Vector&lt;char&gt; Represent cursor as integer index Minor detail – is index before/after cursor? Buffer contains: AB|CDE 1234// for Buffer classprivate: Vector&lt;char&gt; chars;int cursor; Performance insertCharacter() and deleteCharacter() is linear, other operation is just O(1) Space used ~1 byte per char Buffer layered on Stack Inspiration: add/remove at end of vector is fast If chars next to cursor were at end… Build on top of stack? Another layered abstraction! How is cursor represented? Buffer contains:AB|CDEThere is no explicit cursor representation, instead using two stack to represent a whole data structure being seperated by the implicit cursor. 123// for Buffer classprivate: Stack&lt;char&gt; before, after; Performance moveCursorToStart(), moveCursorToEnd() operation is linear, other operation is just O(1) Space used ~2 byte per char Buffer as double linked list Inspiration: contiguous memory is constraining Connect chars without locality Add tail pointer to get direct access to last cell Add prev link to speed up moving backwards Buffer contains:AB|CDE 1234567// for Buffer classprivate: struct cellT &#123; char ch; cellT *prev, *next; &#125;; cellT *head, *tail, *cursor; Cursor design To cell before or after? 5 letters, 6 cursor positions… Add “dummy cell” to front of list Performance destruction is linear, other operation is just O(1) Space used ~9 byte per char Compare implementations table th:nth-of-type(1) { width: 200px; } table th:nth-of-type(2) { width: 80px; } table th:nth-of-type(3) { width: 80px; } Operation Vector Stack Single linked list Double linked list Buffer() O(1) O(1) O(1) O(1) ~Buffer() O(1) O(1) O(N) O(N) moveCursorForward() O(1) O(1) O(1) O(1) moveCursorBackward() O(1) O(1) O(N) O(1) moveCursorToStart() O(1) O(N) O(1) O(1) moveCursorToEnd() O(1) O(N) O(N) O(1) insertCharacter() O(N) O(1) O(1) O(1) deleteCharacter() O(N) O(1) O(1) O(1) Space used 1N 2N 5N 9N Space-time tradeoff Doubly-linked list is O(1) on all six operations But, each char uses 1 byte + 8 bytes of pointers =&gt; 89% overhead! Compromise: chunklist Array and linked list hybrid Shares overhead cost among several chars Chunksize can be tuned as appropriate Cost shows up in code complexity Cursor must traverse both within and across chunks Splitting/merging chunks on insert/deletes Implementing MapMap is super-useful, support any kind of dictionary, lookup table, index, database, etc.Map stores key-value pairs, support fast access via key, operations to optimize: add, getValueHow to make it work efficiently? Implement Map as Vector Layer on Vector, provides convenience with low overhead Define pair struct, to olds key and value together, Vector&lt;pair&gt; Vector sorted or unsorted? If sorted, sorted by what? Sorting: Provides fast lookup, but still slow to insert (because of shuffling) How to implement getValue, add? Does a linked list help? Easy to insert, once at a position But hard to find position to insert… Implementing Map as tree Implementatation Each Map entry adds node to tree, node contains: string key, client-type value, pointers to left/right subtrees Tree organized for binary search, Key is used as search field getValue: Searches tree, comparing keys, find existing match or error add: Searches tree, comparing keys, overwrites existing or adds new node Private members for Map 1234567891011121314151617template &lt;typename ValType&gt; class Map &#123; public: // as before private: struct node &#123; string key; ValType value; node *left, *right; &#125;; node *root; node *treeSearch(node * t, string key); void treeEnter(node *&amp;t, string key, ValType val); DISALLOW_COPYING(Map)&#125;; Evaluate Map as tree Space used: Overhead of two pointers per entry (typically 8 bytes total) Runtime performance: Add/getValue take time proportional to tree height(expected to be O(logN)) Degenerate trees The insert order is “sorted”: 2 8 14 15 18 20 21, totally unbalanced with height = 7 The insert order is “alternately sorted”: 21 2 20 8 14 15 18 or 2 8 21 20 18 14 15 Association: What is the relationship between worst-case inputs for tree insertion and Quicksort? What to do about it: AVL tree Compare Map implementations Operation Vector BST Sorted Vector getValue O(N) O(lgN) O(lgN) add O(N) O(lgN) O(N) Space used N 9N N Hashing Hash table ADT Hash table data structure: A list of keys and TableSize Hash function: A mapping that map each key into some number in the range 0 to TableSize-1 and distributes the keys evenly among the appropriate cell HashingThe major problems are choosing a function, deciding what to do when two keys hash to the same value (this is known as acollision), and deciding on the table size RehashingIf the table gets too full, the running time for the operations will start taking too long, and insertions might fail for open addressing hashing with quadratic resolution. A solution is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table. The Big-FiveIn C++11, classes come with five special functions that are already written for you. These are the destructor, copy constructor, move constructor, copy assignment operator, and move assignment operator. Collectively these are the big-five. DestructorThe destructor is called whenever an object goes out of scope or is subjected to a delete. Typically, the only responsibility of the destructor is to free up any resources that were acquired during the use of the object. This includes calling delete for any corresponding news, closing any files that were opened, and so on. The default simply applies the destructor on each data member. ConstructorA constructor is a method that describes how an instance of the class is constructed. If no constructor is explicitly defined, one that initializes the data members using language defaults is automatically generated. Copy Constructor and Move Constructor Copy Assignment and Move Assignment (operator=)By Defaults, if a class consists of data members that are exclusively primitive types and objects for which the defaults make sense, the class defaults will usually make sense.The main problem occurs in a class that contains a data member that is a pointer. The default destructor does nothing to data members that are pointers (for good reason—recall that we must delete ourselves). Furthermore, the copy constructor and copy assignment operator both copy the value of the pointer rather than the objects being pointed at. Thus, we will have two class instances that contain pointers that point to the same object. This is a so-called shallow copy (contrast to deep copy). To avoid shallow copy, ban the copy funtionality by calling DISALLOW_COPYING(ClassType). As a result, when a class contains pointers as data members, and deep semantics are important, we typically must implement the destructor, copy assignment, and copy constructors ourselves. Explicit constructor:All one-parameter constructors should be made explicit to avoid behind-the-scenes type conversions. Otherwise, there are somewhat lenient rules that will allow type conversions without explicit casting operations. Usually, this is unwanted behavior that destroys strong typing and can lead to hard-to-find bugs.The use of explicit means that a one-parameter constructor cannot be used to generate an implicit temporary 1234567891011class IntCell &#123;public: explicit IntCell( int initialValue = 0 ) : storedValue&#123; initialValue &#125; &#123; &#125; int read( ) const &#123; return storedValue; &#125;private: int storedValue; &#125;;IntCell obj; // obj is an IntCellobj = 37; // Should not compile: type mismatch Since IntCell constructor is declared explicit, the compiler will correctly complain that there is a type mismatch TemplateType-independentWhen we write C++ code for a type-independent algorithm or data structure, we would prefer to write the code once rather than recode it for each different type Function template A function template is not an actual function, but instead is a pattern for what could become a function. An expansion for each new type generates additional code; this is known as code bloat when it occurs in large projects.Class template12345678template &lt;typename Object&gt;class MemoryCell &#123; public: explicit MemoryCell( const Object &amp; initialValue = Object&#123; &#125; ) : storedValue&#123; initialValue &#125; &#123; &#125; private: Object storedValue;&#125;; MemoryCell is not a class, it is only a class template. It will be a class if specify the Object type. MemoryCell&lt;int&gt; and MemoryCell&lt;string&gt; are the actual classes. Graph AlgorithmsDefinitions: vertices, edges, arcs, directed arcs = digraphs, weight/cost, path, length, acyclic(no cycles) Topological Sort A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from vi to vj, then vj appears after vi in the ordering. A topological ordering is not possible if the graph has a cycle To find a topological ordering, define the indegree of a vertex v as the number of edges (u, v), then use a queue or stack to keep the present 0 indegree vertexes. At each stage, as long as the queue is not empty, dequeue a 0 indegree vertexes in the queue, enqueue each new generated 0 indegree vertexes into the queue. Sortest-Path Algorithms Breadth-first search Explores equally in all directions To find unweighted shortest paths Operates by processing vertices in layers: The vertices closest to the start are evaluated first, and the most distant vertices are evaluated last. Dijkstra’s Algorithm Also called Uniform Cost Search, cost matters Instead of exploring all possible paths equally, it favors lower cost paths. Dijkstra’s algorithm proceeds in stages. At each stage, while there are still vertices waiting to be known: Selects a vertex v, which has the smallest dv among all the unknown vertices, and declares v as known stage. For each of v’s neighbors, w, if the new path’s cost from v to w is better than previous dw, dw will be updated. But w will not be marked as known, unless at next while-loop stage, dw happens to be the smalles. The above steps could be implemented via a priority queue. A proof by contradiction will show that this algorithm always works as long as no edge has a negative cost. If the graph is sparse, with |E| =θ(|V|), this algorithm is too slow. In this case, the distances would need to be kept in a priority queue. Selection of the vertex v is a deleteMin operation. The update of w’s distance can be implemented two ways. One way treats the update as a decreaseKey operation. An alternate method is to insert w and the new value dw into the priority queue every time w’s distance changes. Greedy Best First Search(Heuristic search) With Breadth First Search and Dijkstra’s Algorithm, the frontier expands in all directions. This is a reasonable choice if you’re trying to find a path to all locations or to many locations. However, a common case is to find a path to only one location. A modification of Dijkstra’s Algorithm, optimized for a single destination. It prioritizes paths that seem to be leading closer to the goal. To make the frontier expand towards the goal more than it expands in other directions. First, define a heuristic function that tells us how close we are to the goal, design a heuristic for each type of graph 123def heuristic(a, b): # Manhattan distance on a square grid return abs(a.x - b.x) + abs(a.y - b.y) Use the estimated distance to the goal for the priority queue ordering. The location closest to the goal will be explored first. This algorithm runs faster when there aren’t a lot of obstacles, but the paths aren’t as good(not always the shortest). A* Algorithm Dijkstra’s Algorithm works well to find the shortest path, but it wastes time exploring in directions that aren’t promising. Greedy Best First Search explores in promising directions but it may not find the shortest path. The A* algorithm uses both the actual distance from the start and the estimated distance to the goal. Compare the algorithms: Dijkstra’s Algorithm calculates the distance from the start point. Greedy Best-First Search estimates the distance to the goal point. A* is using the sum of those two distances. So A* is the best of both worlds. As long as the heuristic does not overestimate distances, A* does not use the heuristic to come up with an approximate answer. It finds an optimal path, like Dijkstra’s Algorithm does. A* uses the heuristic to reorder the nodes so that it’s more likely that the goal node will be encountered sooner. Conclusion: Which algorithm should you use for finding paths on a map? If you want to find paths from or to all all locations, use Breadth First Search or Dijkstra’s Algorithm. Use Breadth First Search if movement costs are all the same; use Dijkstra’s Algorithm if movement costs vary. If you want to find paths to one location, use Greedy Best First Search or A*. Prefer A in most cases. When you’re tempted to use Greedy Best First Search, consider using A with an “inadmissible” heuristic. If you want the optimal paths, Breadth First Search and Dijkstra’s Algorithm are guaranteed to find the shortest path given the input graph. Greedy Best First Search is not. A* is guaranteed to find the shortest path if the heuristic is never larger than the true distance. (As the heuristic becomes smaller, A turns into Dijkstra’s Algorithm. As the heuristic becomes larger, A turns into Greedy Best First Search.) Advanced Data StructuresRed-Black TreesRed-black tree leads to a natural implementation of the insertion algorithm for 2-3 trees RBT definition Red-black tree means encoding 2-3 trees in this way: red links, which bind together two 2-nodes to represent 3-nodes, and black links, which bind together the 2-3 tree. An equivalent definition is to define red-black BSTs as BSTs having red and black links and satisfying the following three restrictions: Red links lean left. No node has two red links connected to it. The tree has perfect black balance : every path from the root to a null link has the same number of black links. A 1-1 correspondence: If we draw the red links horizontally in a red-black BST, all of the null links are the same distance from the root, and if we then collapse together the nodes connected by red links, the result is a 2-3 tree. RBT implementaion Color representation: Each node is pointed to by precisely one link from its parent, Encode the color of links in nodes, by adding a boolean instance variable color to our Node data type, which is true if the link from the parent is red and false if it is black. By convention, null links are black. For clarity, define constants RED and BLACK for use in setting and testing this variable. Rotation To correct right-leaning red links or two red links in a row conditions. takes a link to a red-black BST as argument and, assuming that link to be to a Node h whose right link is red, makes the necessary adjustments and returns a link to a node that is the root of a red-black BST for the same set of keys whose left link is red. Actually it is switching from having the smaller of the two keys at the root to having the larger of the two keys at the root. Flipping colors to split a 4-node In addition to flipping the colors of the children from red to black, we also flip the color of the parent from black to red. Keeping the root black. Insertion Maintain the 1-1 correspondence between 2-3 trees and red-black BSTs during insertion by judicious use of three simple operations: left rotate, right rotate, and color flip. If the right child is red and the left child is black, rotate left. If both the left child and its left child are red, rotate right. If both children are red, flip colors. Deletion Assignments Name Hash Game of Life Serafini Recursion Boggle! Patient Queue Huffman Encoding Trailblazer]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs106b</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 09 - Java | 双向链表 Doubly Linked List - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-09-java-doubly-linked-list%2F</url>
    <content type="text"><![CDATA[双向链表 Doubly Linked List前面介绍过单向链表，不过单向链表有几个缺点. 第一个就是它的addLast操作非常慢。单向链表只有一个变量保存列表头的地址, 以及每个节点对后面节点的单向引用(链接). 对于很长的列表，addLast方法必须遍历整个列表, 一直到找到列表末尾才能执行插入操作. 那么如何解决呢? 最直观的解决方案就是加个’车尾’, 如图 这样我们就可以直接通过last.next引用末尾位置.不过另一个问题并没有解决, 就是删除列表最后一项removeLast这个操作还是很慢。因为在目前的结构设计下, 我们需要先找到倒数第二项，然后将其下一个指针设置为null。而要找到倒数第二节点, 我们就得先找到倒数第三个节点…… 以此类推。也就是说，对于删除末尾的操作，还是要几乎遍历整个列表。 反方向的链接基于前面单向链表构建双向链表, 一个比较有效的方法是额外为每个节点添加一个指向前面节点的链接/指针.12345public class OneNode &#123; public OneNode prev; //指向前 public int item; public OneNode next; //指向后&#125; 增加这些额外的指针会导致额外的代码复杂度, 以及额外的内存开销, 这就是追求时间效率的代价. Sentinel 与尾节点双向链表的一个设计初衷，就是为了解决单向链表针对列表末尾位置的操作效率不高的问题，除了sentinel和反方向的链接还不够，我们还需要一个节点（指针）能够直接帮我们定位到列表末端。可以考虑添加一个的尾节点last， 这样的列表就可以支持O(1)复杂度的addLast,getLast 和 removeLast操作了。 循环双端链表上面的尾节点设计虽然没什么错误，但有点瑕疵：最后一个尾节点指针有时指向前哨节点，有时指向一个真正的节点。更好的方法是使双向链表首尾相连, 构成一个循环，即前后节点共享唯一的一个前哨节点。 这样的设计相对更整洁，更美观(主观上的), sentinel的prev就指向列表最后一个节点, sentinel的next指向列表第一个节点.12345678910111213public class LinkedListDeque&lt;GType&gt; &#123; private class OneNode &#123; public OneNode prev; //sentinel's forward link always points to the last element public GType item; public OneNode next; //sentinel's backward link always points to the first element public OneNode(OneNode p, GType i, OneNode n) &#123; prev = p; item = i; next = n; &#125; &#125;&#125; 然后修改构造函数:123456789101112131415/** Creates an empty deque. */public LinkedListDeque()&#123; sentinel = new OneNode(null,null, null); sentinel.prev = sentinel; sentinel.next = sentinel; size = 0;&#125;/** Creates a deque with x */public LinkedListDeque(GType x)&#123; sentinel = new OneNode(null, null, null); sentinel.next = new OneNode(sentinel, x,sentinel); sentinel.prev = sentinel.next; size = 1;&#125; 如果是初始化空列表, 那么其实就是一个自己指向自己的sentinel节点. 如果是非空列表, 那么sentinel节点和真实的节点就构成了一个最简单的二元循环体. 针对列表末尾位置的操作双端链表结构优雅，虽然某些操作如addFirst等编码复杂度会提高, 但不影响速度. 更重要的是, 相比单向链表, 它反而使得addLast, moveLast等方法的代码实现变得简单了, 而且还进一步提升了运行速度(O(n)到O(c)).1234567891011121314151617181920212223/** Adds an item to the back of the Deque. * O(c) */public void addLast(GType x)&#123; OneNode oldBackNode = sentinel.prev; OneNode newNode = new OneNode(oldBackNode, x, sentinel); sentinel.prev = newNode; oldBackNode.next = newNode; size += 1;&#125;/** Removes and returns the item at the front of the Deque. * If no such item exists, returns null.O(c). */public GType removeFirst()&#123; if (isEmpty())&#123; return null; &#125; OneNode oldFrontNode = sentinel.next; sentinel.next = oldFrontNode.next; oldFrontNode.next.prev = sentinel; size -= 1; return oldFrontNode.item;&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 08 - Java | 单向链表 Singly Linked List - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-08-java-singly-linked-list%2F</url>
    <content type="text"><![CDATA[链表 Linked List前面有介绍以array为基础搭建的列表，支持自动扩容, 各种插入，删除速度都很快.这里再介绍另一种方案, 链表, 也可以实现列表自动扩容. 带链接的节点链表的核心组成是带链接的节点, 每个节点就像火车车厢, 有钩子连接下一节车厢. 以int节点为例:123456789public class IntNode &#123; public int item; public IntNode next; public IntNode(int i, IntNode n) &#123; item = i; next = n; &#125;&#125; next就是这个链接, 每一个节点就是其上一个节点的next. 嵌套类 Nested static class这个节点作为一个相对独立的数据结构, 我们更希望让他单独作为一个类来维护. 再另外创建一个名为LinkedList的class与用户进行交互. 这样还有另一个好处就是提供一个命名为LinkedList的类给用户交互，用户更直观地知道自己是在调用链表。如果直接与node类交互，用户可能会困扰. 但同时考虑到这个node类只有LinkedList会调用，所以我们可以把node类嵌套进LinkedList中，也就是嵌套类，在类中定义类。1234567891011121314151617181920public class LinkedList&lt;XXX&gt; &#123; private class OneNode &#123; public XXX item; public OneNode next; public OneNode(XXX i, OneNode n) &#123; item = i; next = n; &#125; &#125; private OneNode first; private int size; public LinkedList(XXX x) &#123; first = new OneNode(x, null); size = 1; &#125; //下面是各种方法...&#125; 以上定义使用了泛型。声明OneNode实例first为私有变量, 是为了防止用户错误地摆弄链接指向，private和public的使用参考. 静态与非静态嵌套类123456789class OuterClass &#123; ... static class StaticNestedClass &#123; ... &#125; class InnerClass &#123; ... &#125;&#125; 如果嵌套类不需要使用LinkedList的任何实例方法或变量，那可以声明嵌套类为static。像静态类方法一样，静态嵌套类不能直接引用其外部类中定义的实例变量或方法, 只能通过实例对象引用来使用它们。同时外部类不能直接访问静态嵌套类的成员变量，但可以通过静态嵌套类来访问。 非静态嵌套类一般叫做内部类inner class。与实例方法和变量一样，内部类与其外部类的实例关联，并且可以直接访问该对象的方法和变量。另外，因为内部类与一个实例相关联，所以它不能自己定义任何静态成员。一个内部类的实例作为成员存在于其外部类的实例中, InnerClass的一个实例只能存在于OuterClass的一个实例中，并且可以直接访问它的外部实例的方法和变量。 作为OuterClass的成员，嵌套类可以声明为private，public，protected或package private。外部类只能声明为public或package private。更多详情参考官网. 补充必要的实例方法插入的操作核心是改变链接指向， 比如原来是A-&gt;B-&gt;D, 要插入C, 则把C.next指向D,然后把B.next改为指向C, 变为A-&gt;B-&gt;C-&gt;D123456789101112131415161718192021222324252627282930313233/** 在列表开头插入 x. */public void addFirst(XXX x) &#123; first = new OneNode(x, first); size += 1;&#125;/** 返回列表第一个元素. */public XXX getFirst() &#123; return first.item;&#125;/** 在列表末尾插入 x. */public void addLast(XXX x) &#123; size += 1; OneNode p = first; /* 把 p 当做指针顺藤摸瓜一直挪到列表末尾. */ while (p.next != null) &#123; p = p.next; &#125; p.next = new OneNode(x, null);&#125;/** 删除列表末尾的元素. */public void removeLast()&#123; //自行补充...&#125;public int size() &#123; return size;&#125; 可以看到，如果用户不小心把某节点x指回自己x.next=x,那就会进入死循环，所以我们需要把OnoNode实例first声明为私有变量已提供必要的保护。 超载 overloading如果想初始化一个空列表, 可以:12345/** 构造一个空列表. */public LinkedList() &#123; fist = null; size = 0;&#125; 即使原来已经有一个带参数x的构造器了, 这里再加一个同名构造器也没问题. 因为Java允许有不同参数的方法重名, 叫超载 overloading. 程序不变条件 invariants上面超载了一个初始化空列表的构造器,加入初始化一个空列表，然后直接调用addLast，程序会报错, 因为null没有next. 有几种修改方法, 比如用if else这种加特例的方法. 这个方案虽然可以能解决问题，但是必要时应该避免加入特例代码, 毕竟有特例就意味着增加了复杂性和额外的代码特例记忆需求, 而人记忆是有限的. 一个更简洁（尽管不太显而易见）的解决方案是修改数据结构本身，让所有LinkedList，维护起来都没有差别，即使是空的。如果把列表比做拉货的火车，那么货物就是列表承载的数据。一列火车如果只有车厢而没有车头（或者车尾）的话是没有意义的，因为没有动力。所以不管火车有没有拉货，有车厢还是没车厢，要称之为火车我们至少需要一个火车头。我们可以通过创建一个特殊节点, 称为前哨节点 sentinel。前哨节点将保存一个值，具体数值我们不关心，它只是作为火车头，不装货。所以我们要修改LinkedList为：12345678910111213141516171819202122/* 第一个元素 （假如有的话）就是 sentinel.next. */public class LinkedList&lt;XXX&gt; &#123; private class OneNode &#123; //... &#125; private OneNode sentinel; private int size; /** 构造一个空列表. */ public LinkedList() &#123; sentinel = new OneNode(null, null); size = 0; &#125; /** 构造一个初始元素为x的列表. */ public LinkedList(XXX x) &#123; sentinel = new OneNode(null, null); sentinel.next = new OneNode(x, null); size = 1; &#125;&#125; 对于像LinkedList这样简单的数据结构来说，特例不多我们也许可以hold住, 一旦后续遇到像树tree等更复杂的数据结构，控制特例数量就显得极为重要了。所以现在就要培养自己的这方面的习惯，保持程序不变条件成立 Invariants。所谓 invariants 就是指数据结构任何情况下都是不会出错（除非程序有bug）. 具有前哨节点的LinkedList至少具有以下 invariants： 列表默认存在前哨节点。 列表第一个元素（如果非空的话）总是在sentinel.next.item。 size变量始终是已添加的元素总数。 不变条件使得代码的推敲变得更加容易，同时给程序员提供了能够确保代码正常工作的具体目标。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Bash 直接启动 sublime 或 atom 等编辑器以打开或新建文件]]></title>
    <url>%2FLaunch-editor-in-Gitbash%2F</url>
    <content type="text"><![CDATA[程序员或者其他需要码字多的人，经常要使用编辑器如sublime、atom 和 Typora等。如果每次都要用鼠标点击才能用sublime打开文件，或者在编辑器中新建文件，那么就会有点麻烦！但你可以用一句命令解决！ 配置在Git Bash中用各种文本编辑器打开文件或者直接新建文件。这里以atom为例。 常规步骤 打开Git Bash并cd到你的目标文件夹, 或者直接在目标文件中右键打开Git Bash. atom xxx.md 就会在弹出的atom窗口中打开名为xxx.md的markdown文件, 如果没有这个文件, 会自动创建一个. 适用于其他类型文件, 如.java等. 如果想用sublime, 可以用subl xxx.java, 同理notepad++ 可以用 notepad++ xxx.java等。 (若出现错误,看下面) 若系统无法识别命令一般使用sublime或者notepad++的用户, 可能会出现error: 系统无法识别命令...之类的, 可以这么解决: 方法1新建一个文件命名为subl（注意不能有后缀名），内容：12#!/bin/sh&quot;D:\Sublime Text 3\sublime_text.exe&quot; $1 &amp; 第一行指明这是个 shell 脚本.第二行的字符串是sublime的安装目录, 示例只是我电脑的目录, 注意这里要改为你自己的目录,第二行的$1 是取的命令之后输入的参数第二行的&amp;是此命令在后台打开，这样sublime打开之后，就不会阻塞你的git bash 文件保存到 C:\Program Files (x86)\Git\mingW32\bin 目录下(你的git目录可能与我的不一样，注意改成你自己的) 同理适用于其他编辑器，比如用chrome打开.html文件等。如果不想每次都新建一个文件，可以用下面的方法2。 方法2 找到 C:\Users\你的计算机名目录，如果你的计算机名是Administrator，那么你就要去C:\Users\Administrator目录下, 这里一般存放着windows系统的我的文档, 桌面等文件夹. 在该目录下用Git Bash输入notepad .bashrc, 这会用windows记事本新建并打开一个文件.bashrc，这个文件没有名称只有后缀名。.bashrc里面可以给Git Bash设置命令的别名, 设置路径等。 在.bashrc文件加入下面一行文本alias notepad++=&quot;/D/Notepad++/notepad++.exe&quot;, 这里你需要修改为你电脑的安装路径。alias就是别名的意思，当我们执行notepad++的时候，实际执行的是=后面的语句. 重新打开Git Bash, 设置才能生效，如果不想关掉在打开的话，可以直接在bash下输入source ~/.bashrc就可以立刻加载修改后的设置，设置立即生效。现在在bash下输入notepad++ test.py, 就直接打开了notepad++并创建了这个叫test的Python文件。这里的别名不一定非要取notepad++，随你想叫什么都行。 同理也可以扩展到别的文本编辑器，alias atom=&quot;atom的路径&quot;, alias sublime=&quot;sublime的路径&quot;等. 最后还要注意一点，上面所说的路径最好不要有空格，括号等，否则会造成命令无效. .bashrc还有很多有用的配置,可以根据需要进行扩展. 比如很多程序猿会选择修改删除命令rm(此命令不加任何参数的话，会直接删除文件, 可能会造成误删的后果)。这个时候可以给rm加个参数-i，意为在删除的时候给出提示。在文件.bashrc里添加这行代码alias rm=&quot;rm -i&quot;。但这里不建议这么做，因为rm=&quot;rm -i&quot;是一个定时炸弹，在使用它之后，习惯了之后, 你会本能地期望rm在删除文件之前会提示你。但是，总有一天你可能会用一个没有rm alias 别名的系统, 这时若你也直接随手一甩rm, 本以为会有提示, 结果发现数据真的被删除了。 在任何情况下，预防文件丢失或损坏的好方法就是进行备份。 所以如果你想个性化删除命令, 最好不要动rm，而是创建属于你的命令，比如trash, myrm, delete等, 用alias trash=&#39;/bin/rm -irv&#39;会创建一条把文件放入垃圾回收站的命令.]]></content>
      <categories>
        <category>提高效率</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>Git Bash</tag>
        <tag>Sublime</tag>
        <tag>Atom</tag>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 07 - Java | 用数组构建数据列表 list - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-07-java-array-based-list%2F</url>
    <content type="text"><![CDATA[列表 List前面说到Java的数组无法更改长度，那么也就无法实现插入或者删除数组成员。Java提供了功能更丰富的数据结构 - 列表（list）。所谓列表，即有序的集合（序列），用户可以精确地控制每个元素插入到列表中的哪个位置。用户可以通过整数索引（列表中的位置）来访问元素，并搜索列表中的元素（详细可进一步参考oracle官网）。 这里我们尝试以java的array为基础实现一个列表，目标是实现自动扩容 (Java中的ArrayList不仅仅有自动扩容, 也继承了[List]的其他功能)。在探索的过程中, 可以顺带学习很多相关的内容.使用自上而下的设计思想搭建一个框架:先写出最基础的部分, 也就是一个构造器，前面学过了整数数组，我们直接拿来用123456789101112131415161718/** Array based list. */// index 0 1 2 3 4 5 6 7// items: [6 9 -1 2 0 0 0 0 ...]// size: 5public class AList &#123; private int[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() &#123; items = new int[100]; size = 0; &#125; /** 下面添加其他方法 */&#125; 然后思考我们需要什么功能，把功能需求转化为实例方法instance method的形式，先把方法的外壳描绘出来，注释上该方法的功能（目的），输入值，返回值是什么之类的。具体的功能实现可以先空着，之后一步步丰富。 公共 vs 私有 Public vs. Private在上面的代码块中，可以看到 items 和 size 都被声明为 private 私有变量, 这样就只能被所在的java文件内调用. 私有变量和方法的设计初衷是服务于程序的内部功能实现, 而不是用来和外部程序(用户)进行交互的. 设置成私有, 可以避免这些变量和方法被外部程序直接调用, 避免用户通过不恰当/容易出错的方式修改某些变量. 在程序说明文档中, 一般也会明确说明程序提供什么公共变量和方法给用户调用. 因此我们这里也提供几个 public 方法让用户调用, 这样用户就能按照我们设计的方式来访问数据。分别是getLast() - 访问列表最后一个元素，get(int i)访问第i个元素, 和size()访问列表的大小.12345678910111213141516/** 程序内的方法可以访问 private 变量 *//** 返回列表末尾的值. */public int getLast() &#123; return items[size - 1];&#125;/** 返回第 i 个值 (0 是第一个). */public int get(int i) &#123; return items[i];&#125;/** 返回列表元素长度. */public int size() &#123; return size;&#125; 泛型数组我们不仅希望我们的列表可以存整数，也可以存其他类型的数据，可以通过泛型解决，泛型的介绍参考这篇文章. 泛型数组跟前面介绍的泛型示例有一个重要的语法差异：Java不允许我们创建一个通用对象的数组，原因这里不细展开。 假如我们用Item来标识泛型, 那么在上面的列表类中构建泛型数组时, 我们不能用items = new Item[8];, 而要用items = (Item []) new Object[8];, 即使这样也会产生一个编译警告，但先忍着, 后面会更详细地讨论这个问题。12345678910public class AList&lt;Item&gt; &#123; private Item[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() &#123; items = (Item[]) new Object[100]; //会有编译警告, 暂时不管, 后面会解释 size = 0; &#125;&#125; 数组扩容 Resize一个列表应该支持基本的插入和删除数据的操作，但是因为数组本身无法更改长度，所以我们就需要一个方法，在给数组在插入新数据时，先检查长度容量是否足够，如果不够，那么就要增加长度。我们考虑简单的情况, 即需要在数组末尾插入或者删除数据怎么办 插入元素：123456789101112/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; /** 检查长度容量是否足够，如果不够，那么就要增加长度 */ if (size == items.length) &#123; Item[] temp = (Item[]) new Object[size + 1]; System.arraycopy(items, 0, temp, 0, size); items = temp; &#125; items[size] = x; size = size + 1;&#125; 创建新array并把旧数据复制过去的过程通常称为“resizing”。其实用词不当，因为数组实际上并没有改变大小，只是把小数组上的数据复制到大数组上而已。 为了让代码更易于维护，可以把上面的代码中负责大小调整的部分包装在一个独立的method中123456789101112131415161718192021222324252627/** 改变列表容量, capacity为改变后的容量. */private void resize(int capacity) &#123; Item[] temp = (Item[]) new Object[capacity]; System.arraycopy(items, 0, temp, 0, size); items = temp;&#125;/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; if (size == items.length) &#123; resize(size + 1); &#125; items[size] = x; size = size + 1;&#125;``` 删除元素：```java/** 删去列表最后一个值，并返回该值 */public int removeLast() &#123; Item x = getLast(); items[size - 1] = null; // 曾经引用“删除”的元素的内存地址被清空 size = size - 1; return x;&#125; 事实上即使没有items[size - 1] = null;,也可以达到删除元素的目的.删除对改存储的对象的引用, 是为了避免“loitering”。所谓 loitering，可以理解为占着茅坑不拉屎的对象，它们已经没啥用了，却还是占用着内存。如果这个对象是些几十兆的高清图片，那么就会很消耗内存。这也是为什么安卓手机越用越慢的一个原因。 当引用/内存地址丢失时，Java会销毁对象。如果我们不清空引用，那么Java将不会垃圾回收这些本来预计要删除的对象, 因为它们实际还被列表引用着。 扩容效率分析我们直觉也会感觉到，如果按照现在的设计，即每插入一个新元素，就重新复制一遍数组，这样随着数组越来越大，效率肯定会越来越差。事实上也是这样，如果数组目前长度是100个内存块，那么插入1000次，需要创建并填充大约50万个内存块（等差数列求和N(N+1)/2，101+102+…+1000 ≈ 500000）。但假如我们第一次就扩容到1000，那么就省却了很多运算消耗。可惜我们不知道用户需要插入多少数据，所以要采取其他方法-几何调整。也就是与其按照size + FACTOR这样的速率增加容量, 不如按照size * RFACTOR成倍扩容, 前者的增加速率为1, 后者为 RFACTOR, 只要设置 RFACTOR 大于1, 就能减少扩容的次数.123456789/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; if (size == items.length) &#123; resize(size * RFACTOR); //用 RFACTOR 作为因子扩容数组, &#125; items[size] = x; size = size + 1;&#125; 目前我们解决了时间效率问题, 但代价是需要更大的内存空间, 也就是空间效率下降了. 假设我们插入了十亿个item，然后再删去九亿九千万个项目。在这种情况下，我们将只使用10,000,000个内存块，剩下99％完全没有使用到。 为了解决这个问题，我们可以在数组容量利用率比较低时把容量降下来. 定义利用率 R 为列表的大小除以items数组的长度。一般当R下降到小于0.25时，我们将数组的大小减半。 其他功能比如排序等, 在后面介绍链表的文章中再讨论.]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 06 - Java | array 数组 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-06-java-array%2F</url>
    <content type="text"><![CDATA[数组 Array数组是一种特殊的对象，有一个固定的数组长度参数N，由一连串（N个）连续的带编号的内存块组成，每个都是相同的类型(不像Python可以包含不同类型)，索引从0到N-1编号。A[i]获得数组A的第i个元素。这与普通的类实例不同，类实例有具体变量名命名的内存块。 数组实例化，包含对象的数组 Array Instantiation, Arrays of Objects要创建最简单的整数数组, 有三种方式:123x = new int [3]; //创建一个指定长度的数组，并用默认值（0）填充每个内存块。y = new int [] &#123;1，2，3，4，5&#125;; //创建一个合适大小的数组，以容纳指定的初始值int [] z = &#123;9，10，11，12，13&#125;; //省略了new，只能结合变量声明使用。 创建一组实例化对象:12345678910public class DogArrayDemo &#123; public static void main(String[] args) &#123; /* Create an array of two dogs. */ Dog[] dogs = new Dog[2]; dogs[0] = new Dog(8); dogs[1] = new Dog(20); /* Yipping will result, since dogs[0] has weight 8. */ dogs[0].makeNoise(); &#125;&#125; 注意到new有两种不同的使用方式：一种是创建一个可以容纳两个Dog对象的数组，另外两个创建各个实际的Dog实例。 数组复制123x = new int[]&#123;-1, 2, 5, 4, 99&#125;;int[] b = &#123;9, 10, 11&#125;;System.arraycopy(b, 0, x, 3, 2); //效果类似于Python的`x[3:5] = b[0:2]` System.arraycopy的五个参数分别代表： 待复制的数组(源) 源数组复制起点 目标数组 目标数组粘贴起点 有多少项要复制 2D数组Java的二维数组实质上是一数组的数组, 即每一个数组元素里面也是一个数组。1234567891011121314151617int[][] matrix; //声明一个引用数组的数组matrix = new int[4][]; //创建四个内存块, 用默认null值填充, 之后用于储存对整数数组的引用, 即地址,int[] rowZero = matrix[0];/** 实例化整数数组, 把其地址/引用分别赋值给/储存到 matrix 的第N个内存块*/matrix[0] = new int[]&#123;1&#125;;matrix[1] = new int[]&#123;1, 1&#125;;matrix[2] = new int[]&#123;1, 2, 1&#125;;matrix[3] = new int[]&#123;1, 3, 3, 1&#125;;int[] rowTwo = matrix[2];rowTwo[1] = -5;/** 创建四个内存块, 其中每个被引用的整数数组长度为4,每个元素都是0.*/matrix = new int[4][4];int[][] matrixAgain = new int[][]&#123;&#123;1&#125;, &#123;1, 1&#125;,&#123;1, 2, 1&#125;, &#123;1, 3, 3, 1&#125;&#125;;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 05 - Java | 数据类型 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-05-java-variable-types%2F</url>
    <content type="text"><![CDATA[数据类型数据类型是程序设计语言描述事物、对象的方法。Java数据类型分为基本类型（内置类型）和引用类型(扩展类型）两大类。基本类型就是Java语言本身提供的基本数据类型，比如，整型数，浮点数，字符，布尔值等等。而引用类型则是Java语言根据基本类型扩展出的其他类型，Java要求所有的引用扩展类型都必须包括在类定义里面，这就是Java为什么是面向对象编程语言的原因… 上面的定义有点抽象，要理解数据类型，需要先理解一个问题: 神秘的海象问题12345678/** 尝试预测下面的代码运行时会发生什么。b的变化是否会影响a？提示：类似Python。 */Walrus a = new Walrus(1000, 8.3);Walrus b;b = a;b.weight = 5;System.out.println(a);System.out.println(b); 12345678/** 同样尝试预测下面的代码运行时会发生什么。x的改变是否影响y？ */int x = 5;int y;y = x;x = 2;System.out.println("x is: " + x);System.out.println("y is: " + y); 首先给出答案, b的变化会影响a, 但x的改变不影响y，具体见可视化过程.这里的差别虽然微妙, 但其背后的原理对于数据结构的效率来说是非常重要的，对这个问题的深入理解也将引导我们写出更安全，更可靠的代码。 基本类型 Primative Types计算机中的所有信息都以一系列1和0的形式存储在内存中，这些二进制的0和1就是比特位（bits）。比如72和“H”在内存一般以01001000的形式存储，对他们的形式是一样的。一个引申问题就是：Java代码如何解释01001000，怎么知道应该解释为72还是“H”？ 通过类型types，预先定义好类型即可, 以下代码1234char x = 'H';int y = x;System.out.println(x);System.out.println(y); 会分别得到“H”和72. 在这种情况下，x和y变量都包含几乎相同的bits，但是Java解释器在输出时对它们进行了不同的处理。 Java有8种基本类型：byte，short，int，long，float，double，boolean和char。 变量声明 Declaring Variables计算机的内存可以视为包含大量用于存储信息的内存比特位，每个位都有一个唯一的地址。现代计算机可以使用许多这样的位。 当你声明一个特定类型的变量时，Java会用一串连续的内存位存储它。例如，如果你声明一个int，你会得到一个长度32的内存list，里面有32bits。Java中的每个数据类型都有不同的比特数。 除了留出内存空间外，Java解释器还会在一个内部表中创建一个条目，将每个变量名称映射到内存块中第一个位置（表头list head）。 例如，如果声明了int x和double y，那么Java可能会决定使用计算机内存的352到384位来存储x，而20800到20864位则用来存储y。然后解释器将记录int x从352开始，y从20800开始。 在Java语言里无法知道变量的具体内存位置，例如你不能以某种方式发现x在位置352。不像C++这样的语言，可以获取一段数据的确切地址。Java的这个特性是一个折衷！隐藏内存位置自然意味着程序猿的控制权更少，就无法做某些类型的优化。但是，它也避免了一大类非常棘手的编程错误。在现在计算成本如此低廉的时代，不成熟的优化还不如少点bug。 当声明一个变量时，Java不会在预留的内存位置中写入任何内容, 也即没有默认值。因此，如果没有赋值, Java编译器会阻止你使用变量。 以上只是内存分配的简要说明, 堆和栈的介绍可以参考我的CS106B C++笔记。 引用类型 Reference Types所有基本数据类型之外的类型都是引用类型。引用类型顾名思义，就是对对象的引用。在java中内存位置是不开放给程序员的, 但我们可以通过引用类型访问内存中某处对象。所有引用类型都是 java.lang.Object 类型的子类。 对象实例化 Object Instantiation对象实例化：当我们使用new（例 new Dog）实例化对象时，Java首先为类的每个实例变量分配一串长度合适的bits位，并用缺省值填充它们。然后，构造函数通常（但不总是）用其他值填充每个位置.123456789public static class Walrus &#123; public int weight; public double tuskSize; public Walrus(int w, double ts) &#123; weight = w; tuskSize = ts; &#125;&#125; 用new Walrus(1000, 8.3)创建一个Walrus实例后, 我们得到分别由一个32位(int weight = 1000)和一个64位(double tuskSize = 8.3)的内存块组成的实例：通过程序可视化过程)来更好地理解. 当然在Java编程语言的实际实现中，实例化对象时都有一些额外的内存开销, 这里不展开. 通过 new 实例化对象，new 会返回该对象的内存地址给我们，但假如我们没有用一个变量去接收这个地址，那么我们就无法访问这个对象。之后该对象会被作为垃圾回收。 引用变量声明 Reference Variable Declaration前面有提到，我们需要声明变量来接受实例化的对象在内存中的地址。当声明任何引用类型的变量（比如array, 前面的Dog类等）时，Java都会分配一串64位的内存位置. 这个64位的内存块仅用于记录变量的内存地址, 所谓内存地址, 可以理解为内存(房子)的编号(地址), 一般是内存块的表头位置的64位表达式1234Walrus someWalrus; // 创建一个64位的内存位置someWalrus = new Walrus(1000, 8.3); //创建一个新的实例/** 内存地址由 new 返回, 并被复制/赋值给 someWalrus 对应的内存位置*/ 比如, 假设weight是从内存位5051956592385990207开始存储的，后面连续跟着其他实例变量，那么就可以把5051956592385990207存储在Dog变量中。5051956592385990207由64位的二进制0100011000011100001001111100000100011101110111000001111000111111表达，这样smallDog的内存就可以抽象的理解为一个表smallDog: 0100011000011100001001111100000100011101110111000001111000111111 -&gt; 具体存放实例的内存(Walrus: weight=1000, tuskSize=8.3)‘-&gt;’可以理解为指针. 实例化数组在前面有介绍过，数组array是引用类型，是对象，故数组变量只是存储内存位置。 前面有提到，如果丢失了引用变量存储的内存地址，那么该地址对应的对象就找不回来了。例如，如果一个特定的 Walrus 地址的唯一副本存储在x中，那么x = null这行代码将删去地址，我们则丢失了这个 Walrus 对象。这也不一定是坏事，很多时候在完成了一个对象后就不在需要了，只需简单地丢弃这个参考地址就可以了。 Java 等值规则 Rule of Equals对于y = x，Java解释器会将x的位拷贝到y中,这个规则适用于java中任何使用=赋值的语法, 是理解开头的”神秘的海象”问题的关键. 基本类型变量的位, 存储赋值的值（基本类型）在内存中值(具体位数取决于具体的类型) 1234int x = 5; // 此时是把内存中的某一个地址 p 复制给 xint y;y = x; // y 也指向 px = 2; // 把一个新的内存地址 new p 复制给x, 但y还是指向原来的p x的位存储的是基本类型int 5(32 bits), x = 2是把新的基本类型int 2复制给x, 但y还是指向原来的int 5， 所以y没变化。 引用类型 reference type 变量的位, 存储赋值的值（引用类型）在内存中的地址(固定的64 bits) 1234Dog a = new Dog(5); // 创建一个64位的内存位, 并赋值一个新的实例 pDog b; // 仅创建一个64位的内存位, 没有引用内存地址(null)b = a; // 把a的位（是实例 p 的内存地址）复制给b, 这样 b 也是指向实例 pb.weight = 21; // 此时修改b, 会改写b指向的内存实例 p a和b只存储地址, 而它们的地址都指向相同的实例； 如果对 b 的修改本质是对 p的修改, 那么输出a.weight的时候, 就会变成21. 参数传递 Parameter Passing给函数传递参数，本质上也是赋值操作，参考上面的等值规则，也即复制这些参数的bits给函数，也称之为pass by value。Java的参数传递都是pass by value。至于传递过去的参数会不会因为函数内部的操作而更改，其判断原理在上面的等值规则已经阐明。 通用数据类型 Generic在定义类的时候，有时候我们可能希望这个类能够接受任何类型的数据，而不仅仅是限定了基本类型中的任何一种。比如我们想实现一个类似PPT的类，自然需要这个PPT类能够接收各种类型的字符，数字，并呈现出来。这个时候就需要使用泛型 Generic, 也即通用数据类型。 在2004年，Java的设计者在语言中加入了泛型，使​​我们能够创建包含任何引用类型的数据结构（引用类型和基本类型的解释参考另一篇文章, ）。方法就是在类声明的类名后面，使用一个任意的占位符，并用尖括号括住&lt;随便什么字符&gt;。然后，在任何你想使用泛型的地方，改用占位符。比如1234567public class PPT &#123; public class PPT &#123; public int item; ... &#125; ...&#125; 改为1234567public class PPT&lt;xxx&gt; &#123; public class PPT &#123; public xxx item; ... &#125; ...&#125; &lt;xxx&gt;里面的名称并不重要, 改成其他也行, 只是一个标识符, 用来接受参数, 当用户实例化这个类时, 必须使用特殊的语法PPT&lt;String&gt; d = new PPT&lt;&gt;(&quot;hello&quot;); 由于泛型仅适用于引用类型，因此我们不能将基本类型int等放在尖括号内。相反，我们使用基本类型的引用版本，比如对于int, 用 Integer，PPT&lt;Integer&gt; d = new PPT&lt;&gt;(&quot;10&quot;); 总结使用方法: 在一个实现某数据结构的.java文件中，在类名后面, 只指定泛型类型一次。 在其他使用该数据结构的java文件中，声明实例变量时要指定所需的类型。 如果您需要在基本类型上实例化泛型，请使用Integer, Double, Character, Boolean, Long, Short, Byte, Float，而不是其基本类型。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 04 - Java | 类 class 02 类与实例 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-04-java-class-02-class-instance%2F</url>
    <content type="text"><![CDATA[Class前面提到，类的方法和变量细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。 类方法 vs 实例方法 Class Methods vs. Instance Methods参考上一篇文章的例子，类方法由类调用Dog.makeNoise();. 实例方法只能由实例调用bigDog.makeNoise();. 同理可推, 类方法无法调用实例变量. 可以看到实例方法更具体, 更贴近实体世界, 那我们仍需要类方法, 因为: 有些类不需要实例化, 毕竟我们也经常需要处理抽象的概念, 这些抽象概念在人类认知范畴内是统一的, 比如数学计算, 我们需要计算某个数值的平方根, x = Math.sqrt(100);, 拿来就用, 不需要先实例化. 这点在Python中体现得很好. 有些类有静态方法, 是有实际作用的。例如，若想比较一个类里面的不同实例, 比如两只狗的重量。比较简单的方法就是使用一个比较狗的重量的类方法: 123456789public static Dog maxDog(Dog d1, Dog d2) &#123; if (d1.weight &gt; d2.weight) &#123; return d1; &#125; return d2;&#125;Dog d = new Dog(15);Dog d2 = new Dog(100);Dog.maxDog(d, d2); 这个时候, 若使用实例方法也可以, 但没那么直观：12345678910/** 我们使用关键字this来引用当前对象d。*/public Dog maxDog(Dog d2) &#123; if (this.weight &gt; d2.weight) &#123; return this; &#125; return d2;&#125;Dog d = new Dog(15);Dog d2 = new Dog(100);d.maxDog(d, d2); 类变量 vs 实例变量 Class Variables vs. Instance Variables静态变量的也是有用处的。这些变量一般是类本身固有的属性。例如，我们可能需要用狗类的另一种生物学的统称“犬科”来作为类的说明， 这个时候可以用public static String binomen = &quot;犬科&quot;;，这个变量理论上是由类来访问的。虽然Java在技术上允许使用实例名称来访问静态变量，但是这有时候可能会令人困惑， 所以还是少用为好。 构造器 Constructors in Java与上面的DogLauncher实例化对象的方式相比, 我们更希望实例化可以带参数的，那样可以为我们节省手动给实例变量赋值的麻烦。为了启用这样的语法，我们只需把如下的构造函数直接添加进Dog类中：12345/**注意：构造函数与class类同名 */public Dog(int w) &#123; weight = w; &#125; 然后在DogLauncher里实例化一只狗时, 直接Dog d = new Dog(20);即可. 在以上代码的基础上, 后续当我们想使用new和参数创建一只狗时，可以随时调用public Dog(int w)构造函数。对于熟悉Python的人来说，你可以理解java的构造函数为Python的__init__。 一些术语: 声明(declaration): Dog smalldog;声明一个类作为一个变量在内存中占位 实例化: new Dog(20), 如果没有把它作为值赋给一个类声明变量,那么这个实例化的值会被垃圾回收. 声明, 实例化并赋值: Dog smalldog = new Dog(5)]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 04 - Java | 类 class 01 变量和方法 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-04-java-class-01-intro%2F</url>
    <content type="text"><![CDATA[ClassJava的语法是为了更容易地模拟真实世界而设计的. 比如用程序实现一只狗, 可以用定义一个类class来描述它. 类class里面包括变量Variable，方法method（可以理解为Python的函数function）。变量可以储存数据，方法可以处理数据。变量必须在类中声明(即不能离开类独立存在)，不像Python或Matlab这样的语言可以在运行时添加新的变量。类的方法和变量又细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。实例instance的概念后面会解释。 类（静态）变量和方法 Class(Static) Variables and Methods静态变量和方法的特征就是有static字符在前面.以下代码定义了一个类来模拟狗，包含一个类变量作为这个类的说明，一个类方法用于发出叫声：12345678public class Dog &#123; public static String instruction = "狗类实例"; //类变量, 说明 public static void makeNoise() &#123; System.out.println("汪!"); &#125;&#125; 这里没有定义main(), 在这种情况下如何直接运行这个类(java Dog), 程序是会报错的123错误: 在类 Dog 中找不到 main 方法, 请将 main 方法定义为: public static void main(String[] args)否则 JavaFX 应用程序类必须扩展javafx.application.Application`. 你可以选择在里面添加一个main()方法. 但这次我们选择不定义具体的main(). 具体要如何运行, 我们可以另写一个类定义一个main()方法来调用这个类.12345public class DogLauncher &#123; public static void main(String[] args) &#123; Dog.makeNoise(); &#125;&#125; 这两种方式(在类A内部定义好main() vs. 在其他类B定义main()来调用A)没有优劣之分, 二者有不同的适用情况. 随着不断深入学习，二者的区分将变得更清晰。 注意到, 类变量和方法是有局限性的。现实世界中, 并不是所有的狗都是一样的特征，仅仅靠类这个概念是无法区分不同个体的狗, 除非你为不同的狗定义不同的类（以及里面的变量和方法）, 那么就会很繁琐痛苦. 也就是说，用类来模拟个体是低效的，我们要使用实例. 实例变量和对象实例化 Instance Variables and Object Instantiation Java的类定义就像定义一张蓝图, 我们可以在这个蓝图的基础上, 生成不同的实例instance. 实例是概念性的说法，本质上在Java里就是对象object。这样的特性提供了一个很自然而然地在java中模拟生成实体世界的方法：定义一个狗的类，在这个类的基础上，通过不同的特征参数实例化不同特征的狗（instances），并使类方法的输出取决于特定实例的狗的属性。1234567891011121314/** 一只狗的类:*/public class Dog &#123; public int weight; public void makeNoise() &#123; if (weight &lt; 10) &#123; System.out.println("嘤嘤嘤!"); &#125; else if (weight &lt; 30) &#123; System.out.println("汪汪汪"); &#125; else &#123; System.out.println("嗷呜!"); &#125; &#125; &#125; 这里的方法和变量没有static, 所以是实例（非静态）方法和变量. 如果直接用 Dog 类来调用这些方法, 会报错:123456public class DogLauncher &#123; public static void main(String[] args) &#123; Dog.weight = 21; Dog.makeNoise(); &#125;&#125; 123456DogLauncher.java:3: 错误: 无法从静态上下文中引用非静态 变量 weight Dog.weight = 21; ^DogLauncher.java:4: 错误: 无法从静态上下文中引用非静态 方法 makeNoise() Dog.makeNoise(); ^ 这个时候, 你需要实例化一只狗, 让这个实例来调用非静态变量和方法:1234567public class DogLauncher &#123; public static void main(String[] args) &#123; Dog biglDog = new Dog(); biglDog.weight = 5; biglDog.makeNoise(); &#125;&#125; 运行时，这个程序将会创建一个重量为5的狗，这个狗就会“嗷呜”叫。 总的来说，之所以需要实例方法和变量，是因为我们需要模拟个体，一只具体的狗，并让它发出声音。这个weight和makeNoise()只能由具体的狗调用。狗类不能调用，也没有调用的意义, 毕竟每只狗的重量和声音都不同的. 在设计程序时, 如果其中一个方法我们只打算让特定的实例来调用它(而不让类去调用它), 那么这个方法应该设计成实例方法。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 03 - Java | 代码风格 注释 Javadoc - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-03-java-code-style-comments%2F</url>
    <content type="text"><![CDATA[代码风格与注释 Code style and comments在学习和实践过程中，我们应该努力保持代码可读性。良好的编码风格的一些最重要的特点是： 一致的风格（间距，变量命名，缩进风格等） 大小（线不太宽，源文件不要太长） 描述性命名（变量，函数，类），例如变量或函数名称为年份或getUserName而不是x或f。让代码本身提供可解读性。 避免重复的代码：几乎不会有两个重要的代码块几乎相同，除了一些改变。 适当的评论, 使其他读者也能轻松理解你的代码 行注释: //分隔符开头行被当做注释。 Block（又名多行注释）注释: /*, */, 但我们更推荐javadoc形式的注释。 JavadocJavadoc: / **，*/, 可以（但不总是）包含描述性标签。 借助javadoc工具可以生成HTML格式的API文档。第一段是方法的描述。描述下面是不同的描述性标签, 比如参数 @param， 返回值 @return， 可能抛出的任何异常 @throws123456789/** * @author 名字，邮箱&lt;address @ example.com&gt; * @version 1.6 版本 * @param * @return */public class Test &#123; // class body&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 02 - Java | 语法基础 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-02-java-basic-syntax%2F</url>
    <content type="text"><![CDATA[Java基本语法12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello world!"); &#125;&#125; 上面的程序由一个类声明组成，该声明使用关键字public class声明。 Java所有的代码都应该包含在class里面。 真正负责运行的代码，是一个名为main的method，它声明为public static void main(String[] args)。 public：公共的，大部分方法都是以这个关键字开始的，后面会进一步解释。 static：这是一个静态方法，不与任何特定的实例关联，后面会解释。 void：它没有返回类型。 main：这是方法的名称。 String [] args：这是传递给main方法的参数。 使用大括号{ }来表示一段代码的开始和结束。 声明必须以分号结尾 静态分类 Static Typing程序语言静态与动态的分类，可以参考oracle的说明文件，它解释了动态和静态类型之间的区别, 帮助你理解由程序的错误提示信息。两个主要区别:1. 动态类型语言在运行时执行类型检查，而静态类型语言在编译时执行类型检查。这意味如果以静态类型语言（如Java）编写的脚本包含错误，则在编译错误之前将无法编译. 而用动态类型语言编写的脚本可以编译，即使它们包含会阻止脚本正常运行（如果有的话）的错误。2. 静态类型语言要求你在使用它们之前声明变量的数据类型，而动态类型语言则不需要。考虑以下两个代码示例：123// Javaint num;num = 5; 12# Pythonnum = 5 这两段代码都创建一个名为num的变量并赋值为5. 不同之处在于Java需要将num的数据类型明确定义为int。因为Java是静态类型的，因此它期望变量在被赋值之前被声明。 Python是动态类型的，不需要定义类型, Python根据变量的值确定其数据类型。动态类型语言更加灵活，在编写脚本时可以节省时间和空间。但是，这可能会导致运行时出现问题。例如：123# pythonnumber = 5numbr = (number + 15) / 2 #注意错字 上面的代码本应创建一个值为5的可变数字，然后将其加上15并除以2以得到10. 但是，number在第二行的开头拼写错误。由于Python不需要声明变量，因此会不由分说直接创建一个名为numbr的新变量，并把本应分配给number的值分配给它。这段代码会很顺利编译，但是如果程序试图用number来做某事，程序员假设它的值是10，那么后续就无法产生期望的结果,而且还很难注意到问题。 Java的compiler其中一个关键作用是进行静态类型检查（static type check）。若前面定义了 int x = 0;, 那么后面若给x赋值其他的类型值x = &#39;horse&#39;;, compiler就会报错. 这样就保证了程序不会出现类型错误. 除了错误检查外, static types 也可以让程序媛/猿知道自己处的是什么对象. 总而言之，静态类型具有以下优点： 编译器确保所有类型都是兼容的，这使得程序员更容易调试他们的代码。 由于代码保证没有类型错误，所以编译后程序的用户将永远不会遇到类型错误。例如，Android应用程序是用Java编写的，通常仅以.class文件的形式分发，即以编译的格式。因此，这样的应用程序不应该由于类型错误而崩溃。 每个变量，参数和函数都有一个声明的类型，使程序员更容易理解和推理代码。 Code Style, Comments, Javadoc在学习和实践过程中，我们应该努力保持代码可读性。良好的编码风格的一些最重要的特点是： 一致的风格（间距，变量命名，缩进风格等） 大小（线不太宽，源文件不要太长） 描述性命名（变量，函数，类），例如变量或函数名称为年份或getUserName而不是x或f。让代码本身提供可解读性。 避免重复的代码：几乎不会有两个重要的代码块几乎相同，除了一些改变。 适当的评论, 使其他读者也能轻松理解你的代码 行注释: //分隔符开头行被当做注释。 Block（又名多行注释）注释: /*, */, 但我们更推荐javadoc形式的注释。 Javadoc: / **，*/, 可以（但不总是）包含描述性标签。 借助javadoc工具可以生成HTML格式的API文档。第一段是方法的描述。描述下面是不同的描述性标签, 比如参数 @param， 返回值 @return， 可能抛出的任何异常 @throws123456789/** * @author 名字，邮箱&lt;address @ example.com&gt; * @version 1.6 版本 * @param * @return */public class Test &#123; // class body&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 01 - Java | 安装 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-01-java-install%2F</url>
    <content type="text"><![CDATA[Hello World本系列资料来源于伯克利 Josh Hug 的 cs61b spring 2017和cs61b spring 2018. Java安装与配置安装Java，前往Oracle下载java sdk，我用的是Java SE 8u151/ 8u152 版本。安装sdk时会同时安装sdr。 Windows系统配置: 推荐安装git bash, 一切按照默认安装就好. 更新系统环境变量: 直接在运行中搜索Environment Variables, 选择编辑系统环境变量, 在弹出的框中选择高级-&gt;环境变量, 在弹出的框中系统变量里面 新建变量: 变量名 = JAVA_HOME, 变量值 = 你的jdk路径,如C:\Program Files\Java\jdk1.8.0_151 编辑Path: 在前面加入%JAVA_HOME%\bin;%PYTHON_HOME%;(请注意，不能有空格.) OS X系统配置: 安装Homebrew，一个非常好用的包管理工具。要安装，请在terminal终端输入ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;(注意：在此过程中，可能会提示输入密码。当输入密码时，终端上不会显示任何内容，但计算机还是会记录你的密码的。这是一个安全措施, 让其他人在屏幕上看不到你的密码。只需输入您的密码，然后按回车。) 然后，通过输入以下命令来检查brew系统是否正常工作brew doctor. 如果遇到警告，要求下载命令行工具，则需要执行此操作。请参考这个StackOverflow。 安装git：输入brew install git 安装并配置好java后，测试是否成功:随便在你喜欢的文件夹里新建一个java文件HelloWorld.java12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello world!"); &#125;&#125; 你可以选择用sublime来快速新建文件, 直接在你选择的文件里右键 git bash, 在git bash 里面键入subl HelloWorld.java, 还自动启动sublime并新建一个空白的HelloWorld.java文件, 把上面的代码复制进去并保存即可. (若出现类似提示: 找不到subl command, 解决办法请参考博文在Gitbash中直接启动sublime或atom等编辑器以打开或新建文件 )开始真正的测试。直接在之前打开的git bash中输入: ls, 会看到HelloWorld.java这个文件, ls会列出这个目录中的文件/文件夹 javac HelloWorld.java, 理论上这一步不会有任何输出，有的话可能是设置有问题。现在，如果你继续ls，会看到多了一个HelloWorld.class文件， 这是javac创建的。 java HelloWorld (注意没有.java), 会看到输出Hello World, 表明你的Java设置没有问题]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS入门到进阶]]></title>
    <url>%2Fcs%2F</url>
    <content type="text"><![CDATA[简介记录学习CS的学习笔记，内容包含基础知识的总结以及编程实现的整理。 Language:English 目录 CS入门 学习编写(至少)一种面向对象编程语言(C ++，Java®，Python®) 学习其他编程语言 测试你的代码 逻辑推理和离散数学 深入了解算法和数据结构 了解计算机操作系统 CS入门现在的入门课基本都是用Python语言。 计算机科学导论，优达学城 CS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML CMU 15213: Introduction to Computer Systems (ICS) 面向对象编程语言一般而言，建议先学Java，Python，再学C++。 这三种语言都基本掌握后，再根据自身的职业需求，选择其中一个语言（或者其他语言）进一步深入练习。因为学校课程主要以Python为主，所以目前我还是主要深入学习Python，这是我的Python学习笔记。 面向初学者程序员的在线资源： 编程方法学，斯坦福CS106A，Java 伯克利大学CS 61A计算机程序的结构与解读，Python Java编程简介，MIT Google的Python Class Google的C ++类 面向有经验的程序员的在线资源： 数据结构，伯克利大学 CS 61B，Java 计算机程序设计，Udacity，Python 抽象编程，斯坦福 CS106B，C ++最新作业：http://web.stanford.edu/class/cs106b/ 《数据结构与算法分析:C++描述》, Mark A. Weiss 其他编程语言根据实际需要自行选择一种或多种学习： JavaScript® CSS＆HTML Ruby® Lua PHP® Haskell Perl® Go Shell®脚本 Lisp® Scheme® 一些在线资源： CS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML Codecademy JavaScript Bento JavaScript Learning Track(Bento) Egghead.io 学习如何编程：JavaScript - Epicodus Inc. 学习：查询 CSS ＆ HTML Bento CSS Learning Track(Bento) Bento HTML Learning Track(Bento) 用破折号建立个人网站 使用Webflow构建响应式网站 使用骨架构建SaaS着陆页 建立动态网站 在1小时内编写个人启动页面：实用HTML和CSS简介 学习如何编程：CSS - Epicodus Inc. 从头开始学习HTML5编程 Ruby 学习如何编程：Ruby - Epicodus Inc. RubyMonk - 交互式Ruby教程 Haskell C9：功能编程基础知识 - Erik Meijer CIS 194：Haskell简介 - Brent Yorgey CS240h：Haskell的功能系统 - Bryan O’Sullivan edX：功能编程简介 - Erik Meijer 亚琛大学：功能编程 - JürgenGiesl Lua Lua Interactive Crash Course Lua Tutorial PHP 学习如何编程：PHP - Epicodus Inc. GO Go Tutorial 测试你的代码了解如何捕获错误，创建测试和破解软件. 软件测试，Udacity 软件调试，Udacity 逻辑推理和离散数学 数学计算机科学，麻省理工学院 数学思考导论，斯坦福大学，Coursera 概率图形模型，斯坦福大学，Coursera 博弈论，斯坦福大学和不列颠哥伦比亚大学，Coursera 算法和数据结构了解基本数据类型(堆栈，队列和袋子)，排序算法(快速排序，合并，堆栈)，数据结构(二叉搜索树，红黑树，哈希表)和Big O. 算法简介，麻省理工学院，2011秋季 算法，普林斯顿大学，Part 1 ＆ Part2 算法：设计和分析，斯坦福大学 算法，第4版，by Robert Sedgewick and Kevin Wayne 参考:Guide to technical development from Google educationOS Free Programming Books]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>计算机科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能AI入门到进阶]]></title>
    <url>%2Fai%2F</url>
    <content type="text"><![CDATA[简介记录学习AI的学习笔记，内容包含基础知识的总结以及编程实现的整理。 Language:English 目录 人工智能 机器学习 深度学习 自然语言处理 计算机视觉 机器人 大数据 MapReduce 人工智能机器学习 Coursera Machine Learning， 吴恩达的简化版机器学习 Machine Learning, 吴恩达的机器学习课程 这个比较深入 Deep Learning, 吴恩达的深度学习课程 Neural Networks for Machine Learning, Hinton的神经网络课程 深度学习 Deep learning, Coursera Machine Learning Practical: DNN, CNN, RNN 每个lab的答案在下一个lab branch里，即lab1的答案可以在lab2 branch里面看到。这个代码全部用Python class，比coursera的难度高点。 自然语言处理 自然语言处理, 斯坦福 加速自然语言处理, 爱丁堡大学 深度学习处理自然语言，斯坦福 计算机视觉 图像识别：卷积神经网络，李飞飞，斯坦福 机器人 机器人入门，斯坦福 大数据 Hadoop和MapReduce入门，优达学城 MapReduce极限计算，爱丁堡大学 并行计算入门：MPI, openMP, and CUDA, 斯坦福 参考:Guide to technical development from Google educationOS Free Programming Books]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>计算机视觉</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
</search>
