<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Natural Language Understanding - Informatics - University of Edinburgh]]></title>
    <url>%2Fnatural-language-understanding%2F</url>
    <content type="text"><![CDATA[自然语言理解（爱丁堡大学）知识点汇总References:Natural language understandingCS224n: Natural Language Processing with Deep LearningLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning Meaning representationsTradition solution of usable meaning in a computer: Use e.g. WordNet, a resource containing lists of synonym sets and hypernyms. To convert natural language into values that computer understands, represent words as discrete symbols: Words can be represented by one-hot vectors, Vector dimension is the vocabulary. But there is no natural notion of similarity for one-hot vectors! So learn to encode similarity in the vectors themselves. The core idea is representing words by their context, building a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. Distributional models of meaning = vector-­space models of meaning = vector semantics.word vectors = word embeddings = word representations. Four kinds of vector modelsSparse vector representations:1, Mutual-­information weighted word co-­occurrence matrices Dense vector representations:2, Singular value decomposition (SVD): A special case of this is called LSA - Latent Semantic Analysis3, Neural­‐network­‐inspired models (skip­‐grams, CBOW)4, Brown clusters Prediction-­based models learn embeddings as part of the process of word prediction. Train a neural network to predict neighboring words. The advantages:· Fast, easy to train (much faster than SVD)· Available online in the word2vec package· Including sets of pretrained embeddings Word representation and Word2vecWord2vec is a framework for learning word vectors representation.Idea:1, We have a large corpus of text2, Every word in a fixed vocabulary is represented by a vector3, Go through each position t in the text, which has a center word c and context (“outside”) words o4, Use the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa)5, Keep adjusting the word vectors to maximize this probability 在上面第四点, 如果是给定中心词，计算上下文词, 那么就是 Skip-grams model, 比如 Given word wt, in a context window of 2C words, predict 4 context words [wt-2, wt-1, wt+1, wt+2]Skip-grams 给予模型跳词能力，比如 “I hit the tennis ball” 有三个trigrams: “I hit the”, “hit the tennis”, “the tennis ball”. 但是，这个句子也同样包含一个同样重要但是N-Gram无法提取的trigram:“hit the ball”. 而使用 skip-grams 允许我们跳过 “tennis” 生成这个trigram. 反之，给定 bag-of-words context, predict target word, 那就是 Continuous Bag of Words, CBOW model. 缺点：因为output size 等于 vocabulary，softmax计算会非常昂贵。解决办法是使用负采样 negative sampling。 Word2vec的本质是遍历语料库的每一个词wi，捕捉wi与其上下文位置目标词的同时出现的概率。 目标函数 Obejective funtion (cost or loss function) J(θ):For each position t = 1, … , T, predict context words within a window of fixed size m, given center word, use chain rule to multiply all the probability to get the likelihood L(θ):The θ is the vectors representations, which is the only parameters we needs to optimize(其实还有其他hyperparameters，这里暂时忽略). The loss function is the (average) negative log likelihood: Minimizing objective function ⟺ Maximizing predictive accuracy. The problem is how to calculate P(wt+j | wt; θ): 每个词由两个向量表示（Easier optimization. Average both at the end）：vw when w is a center word, Uw when w is a context word. Then for a center word c and a “outside” word o:The numerator contains dot product, compares similarity of o and c, larger dot product = larger probability. The denominator works as a normalization over entire vocabulary. 高频词二次采样 subsampling二次采样是指当决定是否选取一个词作为样本时，它被跳过的概率正比于它出现的概率，这样不仅可以降低无意义但高频的词(“the”, “a”等)的重要性，也可以加快采样速度。$$P(w_i) = (\sqrt{\frac{z(w_i)}{0.001}} + 1) \cdot \frac{0.001}{z(w_i)}$$ z(wi) 是词wi在语料库中的占比，如果“peanut”在10亿语料库中出现了1,000次, 那么z(‘peanut’) = 1E-6. Negative sampling负采样是指每个训练样本仅更新模型权重的一小部分：only the output that represents the positive class(1) + other few randomly selected classes(0) are evaluated.该论文指出 负采样5-20个单词适用于较小的数据集，对于大型数据集只需要2-5个单词。 修改目标函数，选择k个负样本（即除了概率最高的那个目标词之外的其他词）：_{j=0} 这样可以最大化真正的外部词出现的概率，最小化随机负采样的词概率。 负面样本的选择是基于unigram分布 f(wi): 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。 负采样的优点是：· Training speed is independent of the vocabulary size· Allowing parallelism.· 模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。 与传统的NLP方法比较在word2vec出现之前，NLP使用经典且直观的共生矩阵（co-occurrence matrix）来统计词语两两同时出现的频率，参考ANLP - Distributional semantic models。缺点也明显，词汇量的增加导致矩阵增大，需要大量内存，随之而来的分类模型出现稀疏性问题，模型不稳定。虽然可以使用SVD来降维，但是一个n×m矩阵的计算成本是O(mn2)浮点数（当n&lt;m），还是非常大的。而且很难并入新词或新文档。 目前融合了两种方法的优点的Glove是最常用的。 GloveMorphological Recursive Neural Network (morphoRNN)Limitation of word2vec:• Closed vocabulary assumption• Cannot exploit functional relationships in learning: 如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher To walk closer to open vocabulary, use compositional representations based on morphemes. Instead of word embedding, embed morphemes - the smallest meaningful unit of language. Compute representation recursively from morphemes, word embedding 由 morphemes embedding 拼接而来. 与基础版的morphoRNN结构相同，Context-insensitive Morphological RNN model (cimRNN) 考察 morphoRNN 在不参考任何上下文信息情况下， 仅仅用 morphemic representation 构造词向量的能力。训练时，给每个词xi定义损失函数s(xi)为新构造的词向量pc(xi)和参考词向量pr(xi)之间的欧几里得距离平方 该cimRNN模型没有机会改进可能被估计不足的罕见词的表达. Context-sensitive Morphological RNN (csmRNN) 在学习语素组成时同时参考语境信息，在训练过程中，神经网络顶层的更新将一直反向传播直至底层的语素层。 Compositional character representations在自然语言处理中使用 word 作为基本单位的问题在于词汇量太大了，所以几乎所有主流模型都会省略很多词，比如Bengio的RNNs语言模型就把所有出现频率&lt;3的单词统一标记为一个特殊词。但这样的操作也只是把词汇量降到了16,383。又比如word2vec模型只考虑出现频率最高的30,000个词。 所以寻找其他有限集合的语言单位成为替代选择，比如字母 character（更确切地说是 unicode code points），比如前面提到的 Morphemes，还有其他比如 Character n-grams，Morphological analysis等，这些可以统称为 subwords units。 然后再通过 subwords 来重构 word representation，进而构建整个文本的meaning representation. 构建 word representation 最简单的方法就是把 subwords vectors 相加、平均或者拼接等，但更好的是使用非线性的方法，比如 Bidirectional LSTMs, Convolutional NNs 等。 哪种方式构建 subword representations 比较好？在 word representation 的重构中，涉及了几个变量:1, Subword Unit 2, Composition Function• Linear Vector operation• Bi-LSTMs• Convolutional NNs 3, Language Typology Type example Morphology analysis Fusional (English) ‘reads’ read-s read-3SG.SG Agglutinative (Turkish) ‘If I read …’ oku-r-sa-m read-AOR.COND.1SG Root&amp;Pattern (Arabic) ‘he wrote’ k(a)t(a)b(a) write-PST.3SG.M Reduplication (Indonesian) ‘children’ anak~anak child-PL 除了语言模型外, 其他NLP任务如SQuAd问答数据集上的很多优秀模型，也会加入character embedding. 但目前 Character-level models 并不具有触及实际 morphology 的模型预测能力。 神经网络语言模型如何构建一个神经网络语言模型?语言模型的目的是输入一串字符, 输出下一个字符的概率分布, 可以使用 fixed-window neural Language Model, 类似于N-Gram, 仅考虑前(n-1)个窗口长度序列, “as the proctor started the clock the students opened their _“ 得到定长的输入序列, 而 Feedforward neural networks 的输入就是要求固定长度的向量. 用前馈神经网络做语言模型的优点（相对于N-Gram）就是没有了稀疏性问题，而且模型的大小也控制在 O(n)（N-Gram是O(exp(n))） 固定长度的前馈神经网络的固有缺陷就是它要求输入和输出都是固定长度的, 仅考虑前的(n-1)长度的序列, 很多时候会丢失NLP中的长距离依赖信息, 跟N-Gram的有一样的缺陷。而且实际的应用中语句的长度是不固定的，最好有一个神经网络可以接受任意长度的输入序列, 输出任意长度的序列。循环神经网络 (Recurrent neural networks, aka RNNs) 就可以解决这个问题. 循环神经网络语言模型不同于前馈神经网络使用输入序列的每一个词单独训练一行(或一列, 取决于矩阵的设计)参数矩阵, RNNs的设计核心是用输入序列的每一个词, 反复地训练同一个参数, 即”共享参数”. 因为参数共享:1, 模型大小不会随着输入序列长度增加而增加。2, 每一步的计算，理论上都使用到了之前的历史信息，所以理论上可以更好的捕捉长距离依赖（但实际上表现并不好，看后面的梯度消失与爆炸）.3, 模型有更好的泛化能力 使用基于Softmax的RNNs语言模型等同于解决矩阵分解问题, 参考Breaking the Softmax Bottleneck: A High-Rank RNN Language Model。 循环神经网络语言模型使用损失函数评估模型表现: 损失函数 loss function on step t is usual 交叉熵 cross-entropy between predicted probability distribution and the true next word. 传统的统计语言模型使用困惑度(perplexity)来评估模型表现，但其实降低困惑度等价于减小损失函数. 循环神经网络 RNNs For a sequence of input data (sequence of words, or speech) and sequence of output problem (many to many):· feed input x(t) into the RNN: feeding one word (represented as vector) at a time, e.g one word in a sentence from left to right, x(1) corresponds to the second word of a sentence.· s(t) is the hidden state at time step t. It is calculated based on the previous hidden state and the input at the current step: s(t) = f(Ux(t) + Ws(t-1)). Function f is the activation.· o(t) is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. o(t) = softmax(Vs(t)). RNN shares the same parameters (U, V, W above) across all steps. In addition to the above normal many to many structure RNNs, there are other non-sequence input or output: Many to one, e.g. when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. One to many: Music generation. 除了应用于语言模型, RNNs 还可以应用于· tagging, e.g. part-of-speech tagging, named entity recognition (many to many RNNs)· sentence classification, e.g. sentiment classification (many to one RNNs)· generate text, e.g. speech recognition, machine translation, summarization RNNs BackpropagationBackpropagation Through Time (BPTT): Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. RNNs trained with BPTT have difficulties learning long-term dependencies (e.g. dependencies between steps that are far apart) due to what is called the vanishing/exploding gradient problem. 梯度消失与爆炸The Vanishing/Exploding Gradient problem。 RNNs shares the same matrix (w, u, etc.) at each time step during forward prop and backprop. 求导数时, 根据链式法则, loss对各参数的导数会转换为loss对输出y的导数,乘以y对隐含层的导数, 乘以隐含层相对隐含层之间的导数, 再乘以隐含层对参数的导数. 不同隐含层（举例如ht和hk）之间如果相隔太远, ht对hk的导数就变成多个jacobian矩阵的相乘， 对各个jacobian范数（norms）进行分析后，发现ht对hk的导数值在训练过程中会很快变得很极端（非常小或者非常大）。 Gradient作为传导误差以帮助系统纠正参数的关键角色，如果本身变得接近于0或者nan，那么我们就无法判断t和t+n的数据的依赖性（是没有依赖？还是因为vanish of gradient？还是因为参数设置错误？）。梯度衰减会直接降低模型学习长距离以来关系的能力，给定一个时间序列，例如文本序列，循环神经网络较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。 在使用RNN学习language model的时候，非常容易出现梯度爆炸，解决办法是使用 gradient clipping 梯度裁剪，就是通过把梯度映射到另一个大小的空间，以限制梯度范数的最大值On the difficulty of training Recurrent Neural Networks。 虽然梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。一个缓解梯度衰减的方案是使用更好的参数初始化方案和激活函数（ReLUs）A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. 不过更主流的解决梯度衰减的方案是使用更复杂的rnn隐含单元: Gated Recurrent Units (GRU) introduced by Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation and LSTMs. Long Short-Term MemoryOutside of the vanilla RNNs work flow, LSTMs use gated cells as memory to chose what error to be remembered. The cells take as input the previous state s(t-1) and current input x(t). Thus help to solve the long-term dependencies. Whether the gated cell let information flow through (open) or not (closed) depends on its inner sigmoid activation layer with a pointwise multiplication operation. A sigmoid function values between 0 and 1, it could be used to describe how much information is allowed to through the cell. Take a most basic sequence problem as example - predict next word: the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject. LSTM的直观理解，就是记忆细胞可以完整地保存信息，而新的输入可以导致旧信息的遗忘，或者被新信息覆盖。输出细胞决定是否输出目前时段的信息。 LSTM 用遗忘门来决定从 cell state 中丢弃哪些信息。Forget gate: Control how much information of pervious state $h_{t-1}$ should be forgetten in the current internal cell. Learned by a sigmoid layer called the “forget gate layer” 用输入门 Input gate 来决定有多少新信息是值得储存的（记忆）。Control how much new information is going to be remembered by internal state cell in current step t.1, a input gate (a sigmoid hidden layer) decides which values we’ll update.2, a traditional hidden(tanh/relu) layer creates a vector of new candidate values $\hat{C}_t$, that could be added to the state. 下一步就可以更新旧的 cell sate $C_{t-1}$.Input and forget gates together allow the network to control what information is stored and overwritten at each step. Combine the forget and remember information together to update the previous cell state. 最后，用一个输出门 Output gate 来决定要输出的内容。1, Run a sigmoid layer to decide what parts of the cell state we’re going to output.2, put the cell state through tanh pointwise operation (to push the values to be between −1 and 1) and multiply it by the output of the output gate, so that we only output the parts decided by the output gate. 总的来说, LSTM有输入门、遗忘门和输出门。这三个门形式上，都是关于旧 cell state 和输入向量的 Sigmoid 隐含神经网络层, 只是各自有各自的参数矩阵. Gated Recurring UnitGRU combines the forget and remember gates into one single gate. This combination leads to a simpler LSTMs model. This combined gate is called update gate. GRU first computes the update gate z (another layer) based on current input word vector and hidden state. Then there is a reset gate r similarly but with different weights. The new memory content h&#772; Final memory h at time step combines current and previous time steps: GRU intuition 重置门赋予了模型丢弃与未来无关的信息的能力。若重置门接近于0，则忽略之前的记忆，仅储存新加入的信息. 更新门控制过去的状态对现在的影响程度（即决定更新多少），如果接近于1，则 ht=zt*ht-1, 等同于把过去的信息完整复制到未来，相应地缓解梯度衰减。 短距离依赖的单元，过去的信息仅保留很短的时间，重置门一般很活跃，也就是数值在0和1之间频繁变动。 长距离依赖的单元，重置门较稳定（保留过去的记忆较长时间），而更新门较活跃。 不同RNNs变种的比较Vanilla RNNs Execution: Read the whole register h Update the whole register GRU Execution: Select a readable subset Read the subset Select a writable subset Update the subset Bidirectional RNNsBidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs. 门控循环神经网络的训练 把参数矩阵初始化为正交 把遗忘门的bias初始化为1，默认不遗忘 别忘了梯度裁剪 注意dropout在RNNs中的应用不同于DNN和CNN 神经网络语言模型的学习能力Character models are good at reduplication (no oracle, though), works well on language with reduplication patterns like Indonesian, Malay. Character NLMs learn word boundaries, memorize POS tags. What do NLMs learn about morphology?1, Character-level NLMs work across typologies, but especially well for agglutinative morphology.2, predictive accuracy is not as good as model with explicit knowledge of morphology (or POS).3, They actually learn orthographic similarity of affixes, and forget meaning of root morphemes accordong to qualitative analyses.4, More generally, they appear to memorize frequent subpatterns 总的来说，神经网络处理自然语言的能力并不特殊，表现的性能，跟神经网络本身的长处相匹配，如泛化、模式匹配、端到端应用的能力等。 Dependency parsing语言学里有两种角度看待语法结构 - Constituency and Dependency： Constituency: phrase structure grammar, 从句子成分构造的角度看，capture the configurational patterns of sentences，即把句子的语法理解为词组成分的递归嵌套. 可以用 context-free grammars (CFGs) 来表达语法规则，就是语法树。 Dependency syntax: 主要是从语义的角度来看，显示哪些单词依赖于（一般指修改或作为参数其参数）哪些单词。特别用于区分动词的主格（subject position or with nominative inflection）宾格（object position or with accusative inflection）. Dependencies can be identified even in non-configurational languages. A sentence dependency structure explains the dependency relation between its words: represented as a graph with the words as its nodes, linked by directed, labeled edges, with the following properties:• connected: every node is related to at least one other node, and (through transitivity) to ROOT;• single headed: every node (except ROOT) has exactly one incoming edge (from its head);• acyclic: the graph cannot contain cycles of directed edges. Dependency trees 有两种，如果dependency graph中有edges交叉则是non-projective, 反之则是 projective。A non-projective dependency grammar is not context-free. Motivation for Dependency parsing:• context-free parsing algorithms base their decisions on adjacency;• in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective);• we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present). Evaluation: accuracy (# correct dependencies with or ignore label)). Graph-based dependency parsingBased on maximum spanning trees (MST parser), views syntactic structure as a set of constraints Intuition as tagging problem: since each word has exactly one parent, the possible tags are the other words in thesentence (or a dummy node called root). If we edge factorize the score of a tree so that it is simply the product of its edge scores, then we can simply select the best incoming edge for each word. The tartget function is to find the highest scoring dependency tree in the space of all possible trees for a sentence. The score of dependency tree y for sentence x is:$$s(x,y) = \sum_{(i,j)\in y} s(i,j)$$$x = x_1…x_n, y$ is a set of dependency edges, with $(i, j) ∈ y$ if there is an edge from $x_i$ to $x_j$. Scoring edges with a neural networkThe function g(aj, ai) computes an association score telling us how much word wi prefers word wj as its head. Association scores are a useful way to select from a dynamic group of candidates, 跟注意力机制的similarity score 异曲同工，方程的形式也很相似。 Parsing 算法： start with a totally connected graph G, i.e., assume a directed edge between every pair of words; find the maximum spanning tree (MST) of G, i.e., the directed tree with the highest overall score that includes all nodes of G; this is possible in O(n2) time using the Chu-Liu-Edmonds algorithm; it finds a MST which is not guaranteed to be projective; 1, Each node j in the graph greedily selects the incoming edge with word, the highest score s(i,j) 2, If result were a tree, it would have to be the maximum spanning tree; If not, there must be a cycle. 3, Break the cycle by replacing a single incoming edge to one of the nodes in the cycle. To choose the node, decide recursively by identifying the cycle and contract it into a single node and recalculate scores of incoming and outgoing edges. Now call CLE recursively on the contracted graph. MST on the contracted graph is equivalent to MST on the original graph. 这里是指先识别出循环体saw ⇄ john②，然后在这个循环体范围内，使用CLE找出 root 进出这个循环体的最大概率路线(root → saw → john = 40) &gt; (root → john → saw = 29)③； 4, Greedily collect incoming edges to all nodes, find out to be a tree and thus the MST of the graph. 把循环体以及其包含的nodes合并为一个node wjs，并且已经有了进出wjs的最大概率路径，这样就可以在整个图上继续运行CLE算法找出最大概率路线(root → wjs → mary = 70) &gt; (root → mary → wjs = 40)④. Chu-Liu-Edmonds (CLE) Algorithm: In graph theory, Edmonds’ algorithm or Chu–Liu/Edmonds’ algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). It is the directed analog of the minimum spanning tree problem Transition-based dependency parsingAn extension of shift-reduce parsing (MALT parser), views syntactic structure as the actions of an automaton:• for a given parse state, the transition system defines a set of actions T which the parser can take;• if more than one action is applicable, a machine learning classifier is used to decide which action to take;• just like in the MST model, this requires a mechanism to compute scores over a set of (possibly dynamic) candidates.if si is the ith top element on stack, and bi the ith element on buffer, then we have the following transitions:• LEFT-ARC(l): adds arc s1 → s2 with label l and removes s2 from stack (|s| ≥ 2);• RIGHT-ARC(l): adds arc s2 → s1 with label l and removes s1 from stack (|s| ≥ 2);• SHIFT: moves b1 from buffer to stack; recondition: |b| ≥ 1.总的来说就是：父节点保留在stack中; 从始至终 root 一直都是父节点；从 buffer 中把候选词一个一个 push 到stack中，根据 classifier 预测的结果，分辨出哪个候选词是子节点，并把子节点 pop 出 stack；直到清空 buffer，stack 中只剩下 root。 Comparing MST and transition-based parsers:Both require dynamic classifiers, and these can be implemented using neural networks, conditioned on bidirectional RNN encodings of the sentence. The MST parser selects the globally optimal tree, given a set of edges with scores;• it can naturally handle projective and non-projective trees; A transition-based parser makes a sequence of local decisions about the best parse action;• it can be extended to projective dependency trees by changing the transition set; Accuracies are similar, but transition-based is faster; Recurrent neural network grammars (RNNGs)Widespread phenomenon: Polarity items can only appear in certain contexts, e.g. “anybody”. In linguistics, a polarity item is a lexical item that can appear only in environments associated with a particular grammatical polarity – affirmative or negative. A polarity item that appears in affirmative (positive) contexts is called a positive polarity item (PPI), and one that appears in negative contexts is a negative polarity item (NPI). The environment in which a polarity item is permitted to appear is called a “licensing context“. The lecture that I gave did not appeal to anybody;The lecture that I gave appealed to anybody. 也许“anybody”出现的条件是前面出现过“not”，那么应该可以使用 RNNs 模型来解码这点信息。然而:The lecture that I did not give appealed to anybody. 这说明 Language is hierarchical: The licensing context depends on recursive structure (syntax)。不能简单根据“not”是否出现来判断，而是需要看“not”修饰的成分，也就是说要考虑语法的合理。这就给文本生成任务（或者说构建语言模型）带来挑战。 Recurrent neural network grammars (Dyer et al. 2016)提出了一种具有明确短语结构的语言模型 RNNGs。 RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. 就是在使用 RNNs 构建语言模型，除了考虑历史词信息, 还会生成历史的语法结构, 并以此为参考预测语法结构和词语,以保证生成的语言符合语法结构。这里的语法是针对 phrase structure (constituency) grammars，所以 RNNGs 也是一种 constituency parsing： Generate symbols sequentially using an RNN Add some “control symbols” to rewrite the history periodically Periodically “compress” a sequence into a single “constituent” Augment RNN with an operation to compress recent history into a single vector (-&gt; “reduce”) RNN predicts next symbol based on the history of compressed elements and non-compressed terminals (“shift” or “generate”) RNN must also predict “control symbols” that decide how big constituents are 首先注意到，如果有序地去遍历语法树，得出的就是一个序列： What information can we use to predict the next action, and how can we encode it with an RNN? Use an RNN for each of: Previous terminal symbols Previous actions Current stack contents最后得出的 stack 就是完整的语法树（以序列的形式）。 Syntactic Composition人们通过较小元素的语义组合来解释较大文本单元的含义 - 实体，描述性词语，事实，论据，故事.When compressing “The hungry cat” into a single composite symbol, use Bi-LSTM to encode (NP The hungry cat). 基于此可以递归地解码更复杂的短语，比如(NP The (ADJP very hungry) cat), 只需要把原来的hungry替换为(ADJP very hungry)即可。 这种递归地堆栈符号的构建行为映射了符号对应的树结构 除此了使用 Bi-LSTM 解码，还可以使用 Attention：Replace composition with one that computes attention over objects in the composed sequence, using embedding of NT for similarity. Implement RNNGsStack RNNs Augment a sequential RNN with a stack pointer Two constant-time operations push - read input, add to top of stack, connect to current location of the stack pointer pop - move stack pointer to its parent A summary of stack contents is obtained by accessing the output of the RNN at location of the stack pointer Training RNNs: Each word is conditioned on history represented by a trio of RNNs backpropagate through these three RNNs, and recursively through the phrase structure S → NP VP. 完整的RNNGs模型，用 softmax 计算下一个 action 的概率分布： Parameter EstimationRNNGs jointly model sequences of words together with a “tree structure”. Any parse tree can be converted to a sequence of actions (depth first traversal) and vice versa (subject to wellformedness constraints). Inference problems of RNNGsAn RNNG is a joint distribution p(x,y) over strings (x) and parse trees (y). Two inference questions:• What is $p(x)$ for a given x? - language modeling• What is $max_yp(y | x)$ for a given x? - parsing Use importance sampling to do both by sampling from a discriminatively trained model. importance samplingAssume we’ve got a conditional distribution $q(y | x)$s.t. (i) $p(x, y) &gt; 0 \Rightarrow q(y | x) &gt; 0$(ii) $y \sim q(y | x)$ is tractable and(iii) $q(y | x)$ is tractable The importance weights $w(x,y) = \frac{p(x, y)}{q(y | x)}$ 从句子到语法树的seq2seq模型其实从句子到语法的映射类似于一个seq2seq模型。而直接的把语法树以字符序列的形式表达，使用简单的 RNNs 直接构建句子到语法序列的 seq2seq 模型效果也不错，比如：input: The hungry cat meows .output: S( NP( _ _ _ ) VP( _ ) _ )Vanilla RNNs 在模式匹配和计数方面非常出色，经验证明，训练有素的 seq2seq 模型通常会输出格式良好的字符串，见这篇文章 section 3.2 但潜在的问题是，seq2seq 模型并不要求输出是有正确括号字符（数量对齐，位置正确）。另外，理论上单个RNN也只能记忆括号结构一定的有限深度，因为 RNNs 只有固定的有限数量的隐藏单元。例如，它将为这些输出分配非零概率：S( NP( _ _ ) VP ( _ ) _ )S( NP( _ _ _ ) VP ( _ ) _ ) ) ) 理想情况下，模型应该给任何不完整的输出分配零概率。使用 RNNGs 是因为它本身能够履行这些限制， 保证生成完整正确的语法树。 从中可以看出，seq2seq模型可以用于快速原型和 baseline 搭建，但如果遇到要求输出遵守某些约束条件的问题，则需要直接执行这些约束条件。 ParsingParsing is a fundamental task in NLP. But what is parsing actually good for? Parsing breaks up sentences into meaningful parts or finds meaningful relationships, which can then feed into downstream semantic tasks:• semantic role labeling (figure out who did what do whom);• semantic parsing (turn a sentence into a logical form);• word sense disambiguation (figure out what the words in a sentence mean);• compositional semantics (compute the meaning of a sentence based on the meaning of its parts). Semantic role labeling (SRL)Semantic role labeling means identifying the arguments (frame elements) that participate in a prototypical situation (frame) and labeling them with their roles; SRL task is typically broken down into a sequence of sub-tasks: parse the training corpus; match frame elements to constituents; extract features from the parse tree; train a probabilistic model on the features. 所谓 frame elements 是针对 Frame Semantics 而言的。 SRL provides a shallow semantic analysis that can benefit various NLP applications; no parsing needed, no handcrafted features. Frame Semantics表达词义，除了 Firth, J.R. (1957) 的 “a word is characterized by the company it keeps”（也即是 word embedding 的理论基础）之外, 还有 Charles J. Fillmore 的 Frame Semantics. The basic idea is that one cannot understand the meaning of a single word without access to all the essential knowledge that relates to that word. A semantic frame is a collection of facts that specify “characteristic features, attributes, and functions of a denotatum, and its characteristic interactions with things necessarily or typically associated with it.” A semantic frame can also be defined as a coherent structure of related concepts that are related such that without knowledge of all of them, one does not have complete knowledge of any one; they are in that sense types of gestalt. Proposition BankPropBank is a version of the Penn Treebank annotated with semantic roles. More coarse-grained than Frame Semantics: End-to-end SRL system基本的结构单元是Bi-LSTM，用法是：· a standard LSTM layer processes the input in forward direction;· the output of this LSTM layer is the input to another LSTM layer, but in reverse direction;这些Bi-LSTM单元可以叠加起来构造更深层的神经网络. The input (processed word by word) features are:• argument and predicate: the argument is the word being processed, the predicate is the word it depends on;• predicate context (ctx-p): the words around the predicate; also used to distinguish multiple instances of the same predicate;• region mark (mr): indicates if the argument is in the predicate context region or not;• if a sequence has np predicates it is processed np times. Output: semantic role label for the predicate/argument pair using IOB tags (inside, outside, beginning). Training:• Word embeddings are used as input, not raw words;• the embeddings for arguments, predicate, and ctx-p, as well as mr are concatenated and used as input for the Bi-LSTM;• the output is passed through a conditional random field (CRF); allows to model dependencies between output labels;• Viterbi decoding is used to compute the best output sequence Model learns “syntax”(Maybe): it associates argument and predicate words using the forget gate: Semantic ParsingSemantic Parsing 指语义分析，把文本解析为任意的逻辑形式，比如 first-order logic(FOL).Sam likes Casey - likes(Sam, Casey);Anna’s dog Mr. PeanutButter misses her - misses(MrPB, Anna) ∧ dog(MrPB);Kim likes everyone - ∀x.likes(x, Kim).Predicate-argument structure is a good match for FOL, as well as structures with argument-like elements (e.g. NPs).Determiners, quantifiers (e.g. “everyone”, “anyone”), and negation can be expressed in FOL. However, much of natural language is unverifiable, ambiguous, non-canonical. That makes it hard to represent the wide-coverage meaning of arbitrary NL. Closed domains are easier, and can sometimes be harvested automatically, e.g. GEOQUERY dataset. This leads to a proliferation of domain-specific MRs.· Pairs of NL sentences with structured MR can be collected, e.g. IFTTT dataset (Quirk et al. 2015).· WikiTableQuestions· Google’s knowledge graph Viewing MR as a string, semantic parsing is just conditional language modeling. Trainable alternative to compositional approaches: encoder-decoder neural models. The encoder and decoder can be mixed and matched: RNN, top-down tree RNN. Works well on small, closed domains if we have training data, but there are many unsolved phenomena/ problems in semantics. Abstract meaning representation (AMR)• The edges (ARG0 and ARG1) are relations• Each node in the graph has a variable• They are labeled with concepts• d / dog means “d is an instance of dog”The dog is eating a bone(e / eat-01&nbsp;&nbsp;&nbsp;&nbsp;:ARG0 (d / dog)&nbsp;&nbsp;&nbsp;&nbsp;:ARG1 (b / bone)) The dog wants to eat the bone(want-01&nbsp;&nbsp;&nbsp;&nbsp;:ARG0 (d / dog)&nbsp;&nbsp;&nbsp;&nbsp;:ARG1 (e / eat-01&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:ARG0 d&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:ARG1 (b / bone))) CoreferenceCharles just graduated, and now Bob wants Anna to give him a job.Q: who does him refer to? MetonymyWestminster decided to distribute funds throughout England, Wales, Northern Island, and Scotlanddecided(Parliament, …) ImplicatureThat cake looks delicious - I would like a piece of that cake. Even more phenomena…• Abbreviations (e.g. National Health Service=NHS)• Nicknames (JLaw=Jennifer Lawrence)• Metaphor (crime is a virus infecting the city)• Time expressions and change of state• Many others 指代消解 Coreference Resolution待补 Unsupervised Part-of-Speech TaggingParts-of-speech(POS), word classes, or syntactic categories, 一般指八个词性：noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, 有时候是 numeral, article or determiner.1, noun 名詞 ( n. )2, pronoun 代名詞 ( pron. )3, verb 動詞 ( v. )4, adjective 形容詞 ( adj. )5, adverb 副詞 ( adv. )6, preposition 介系詞 ( prep. )7, conjunction 連接詞 ( conj. )8, interjection 感歎詞 ( int. ) Tagging is a task that take a sentence, assign each word a label indicating its syntactic category (part of speech). One common standard label is Penn Treebank PoS tagset. DT - DeterminerIN - Preposition or subord. conjunctionNN - Noun, singular or massNNS - Noun, pluralNNP - Proper noun, singularRB - AdverbTO - toVB - Verb, base formVBZ - Verb, 3rd person singular present Current PoS taggers are highly accurate (97% accuracy on Penn Treebank). But they require manually labelled training data, which for many major language is not available. Hence motivated for unsupervised PoS tagging. Hidden Markov ModelsThe unsupervised tagging models here are based on Hidden Markov Models (HMMs).The parameters of the HMM are θ = (τ, ω). They define:• τ : the probability distribution over tag-tag transitions;• ω: the probability distribution over word-tag outputs.The parameters are sets of multinomial distributions:• $ω = ω^{(1)} . . . ω^{(T)}$: the output distributions for each tag;• $τ = τ^{(1)} . . . τ^{(T)}$: the transition distributions for each tag;• $ω^{(t)} = ω_1^{(t)}. . . ω_W^{(t)}$: the output distribution from tag $t$;• $τ^{(t)} = τ_1^{(t)}. . . τ_T^{(t)}$: the transition distribution from tag $t$. Another way to write the model, often used in statistics and machine learning: $w_i | t_i = t ∼ Multinomial(ω^{(t)})$ So as tag, given that $t_{i−1} = t$, the value of $t_i$ is drawn from a multinomial distribution with parameters $τ^{(t)}$. How to estimate ω and τ without supervision. For inference (i.e., decoding, applying the model at test time), we need to know θ and then we can compute $P(t, w)$:For estimation (i.e., training the model, determining its parameters), we need a procedure to set θ based on data. Rely on Bayes Rule:\begin{equation}\begin{split} P(θ|w)&amp;=\frac{P(w|θ)P(θ)}{P(w)}\\ &amp;∝P(w|θ)P(θ)\\\end{split}\end{equation}Choose the θ that maximize the likelihood $P(w|θ)$. Basically, we ignore the prior. In most cases, this is equivalent to assuming a uniform prior.In unsupervised systems, can often use the expectation maximization (EM) algorithm to estimate θ. For examples, forward-backward algorithm for HMMs, inside-outside algorithm for PCFGs, k-means clustering. E-step: use current estimate of θ to compute expected counts of hidden events ($n(t,t^{\prime})$, $n(t,w)$).M-step: recompute θ using expected counts. But EM often fails, even very small amounts of training data have been show to work better than EM. Instead, use Bayesian HMM with Gibbs sampling. Bayesian HMMWhen training HMM model, we are not actually interested in the value of θ, we could simply integrate it out. This approach is called Bayesian integration. Integrating over θ gives us an average over all possible parameters values. Example: we want to predict a spinner result will be “a” or not?• Parameter θ indicates spinner result: $P(θ = a) = .45$, $P(θ = b) = .35$, $P(θ = c) = .2$;• define t = 1: result is “a”, t = 0: result is not “a”;• make a prediction about one random variable (t) based on the value of another random variable (θ). Maximum likelihood approach: choose most probable θ, $\hat{θ} = a$, and $P(t = 1|\hat{θ}) = 1$, so we predict $t = 1$. Bayesian approach:average over θ,$P(t = 1) = \sum_θ P(t = 1|θ)P(θ) = 1(.45) + 0(.35) + 0(0.2) = .45$, predict t = 0. Advantages of Bayesian integration:• accounts for uncertainty as to the exact value of θ;• models the shape of the distribution over θ;• increases robustness: there may be a range of good values of θ;• we can use priors favoring sparse solutions (more on this later). Dirichlet distributionChoosing the right prior can make integration easier. A $K$-dimensional Dirichlet with parameters $α = α_1 . . . α_K$ is defined as: $$ P(θ) = \frac{1}{Z} \prod_{j=1}^K θ_j^{α_j−1} $$ We usually only use symmetric Dirichlets, where $α_1 . . . α_K$ are all equal to β. We write Dirichlet(β) to mean $Dirichlet(β, . . . , β)$. To Bayesianize the HMM, we augment with it with symmetric Dirichlet priors: To simplify things, use a bigram version of the Bayesian HMM; If we integrate out the parameters θ = (τ, ω), we get: Use these distributions to find $P(t|w)$ using an estimation method called Gibbs sampling. Results: Integrating over parameters is useful in itself, even with uninformative priors $(α = β = 1)$; 总结：· Bayesian HMM improves performance by averaging out uncertainty;· allows us to use priors that favor sparse solutions as they occur in language data. Bias in NLPThe social impact of NLPOutcome of an NLP experiment can have a direct effect on people’s lives, e.g. 频繁出现亚马逊 Alexa 突然发出诡异笑声，给多名用户造成困惑和恐慌, 因为人们谈话中偶然包含 trigger 词：“Alexa, laugh” 而发出 - 亚马逊的解决方案是把 trigger 改为更难触发的 “Alexa, can you laugh” Chatbot 对于人们敏感问题的不恰当回答, 比如 “Should I kill myself?” - “Yes.”，这些回答对患有心理障碍的人群或者青少年儿童带来非常大的危害。 Microsoft 的 AI chatbot 上线仅一天, 就通过 twitter 和人交谈并学会涉及种族, 性别歧视等的话语, 典型的 “garbage in, garbage out” 现象. 其他涉及数据隐私等问题 语言的特性，导致NLP涉及的社会伦理问题非常多, 而且影响非常大：· 语言传递着信息、偏见，是政治性的、权力的工具, 同时比其他技术带有更明显的拟人化、人格化倾向，这可能给个人生活带来不便或危害，给整个社会带来舆论影响。· Any dataset carries demographic bias: latent information about the demographics of the people that produced it. That excludes people from other demographics. 同时人类本身的认知容易加深偏见:The availability heuristic: the more knowledge people have about a specific topic, the more important they think it must be. Topic overexposure creates biases that can lead to discrimination and reinforcement of existing biases. E.g. NLP focused on English may be self-reinforcing. NLP 实验本身容易加深偏见：• Advanced grammar analysis can improve search and educational NLP, but also reinforce prescriptive linguistic norms.• Stylometric analysis can help discover provenance of historical documents, but also unmask anonymous political dissenters. NLP 技术可能被不恰当地使用：• Text classification and IR can help identify information of interest, but also aid censors.• NLP can be used to discriminate fake reviews and news, and also to generate them. Word embeddings contain human-like biasesword2vec learns semantic/ syntactic relationships, also keep company with unsavoury stereotypes and biases?• Man:Woman - King:Queen• Man:Doctor - Woman:Nurse• Man:Computer Programmer - Woman:Homemaker Measure bias using implicit association tests:1, Compute similarity of group1 and stereotype1 word embeddings. Cosine similarity is use to measure association (in place of reaction time).2, Compute similarity of group1 and stereotype 2 word embeddings.3, Null hypothesis: if group1 is not more strongly associated to one of the stereotypes, there will be no difference in the means.4, Effect size measured using Cohen’s d.5, Repeat for group 2. Experiments• Uses GloVe trained on Common Crawl—a large-scale crawl of the web.• Removed low frequency names.• Removed names that were least “name-like” (e.g. Will) algorithmically.• Each concept is represented using a small set of words, designed for previous experiments in the psychology literature. Result:· flowers associate with pleasant, insects associate with unpleasant. $p &lt; 10^{−7}$· Men’s names associate with career, women’s names associate with family. $p &lt; 10^{−3}$· European American names associate with pleasant, African American names associate with unpleasant. $p &lt; 10^{−8}$ 这些结果的确真实地反映人类社会的现状。但大部分性别方面的偏见其实是反映了目前的社会分工，无所谓高低贵贱；人种的偏见倒是反映了历史问题对现在的影响，这种偏见是不符合道德的。人对于其他生物的偏见，虽然是没必要的，但人类的确倾向于喜爱行为“可爱”，外形“美好”的生物，比如大熊猫就是比鳄鱼受欢迎。 偏见的存在不一定合理。哪些偏见是不合理的，才是人们更应该去思考和讨论的地方。 Debiasing word embeddingsBolukbasi. et. al., 2016. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings提供了一个思路: 确认偏见的方向 中和抵消偏见: 对于非定性的词（如“医生”），通过投射来消除偏见 等价：让father - mother和boy - girl等距，让定性词间的距离只有性别的距离；或者让doctor - woman和doctor - man等距，消除非定性词的性别偏见。 什么词需要抵消偏见: 训练一个线性分类器来确定词是非定性还是非定性的, 结果当然是大部分英语词都是非定性的. If analogies reveal a gender dimension, use analogies on specific seed pairs to find it.y 轴下面的词属于定性词, 不需要中性化, 而y轴之上的词则需要进行中性化处理. 不同的偏见, 需要不同的 seed words; 一种偏见, 可以有多种 seed words 选择: 除了用”She-He”作为性别偏见的基准, 还有其他选择. 编码器—解码器 Sequence-to-sequence 和注意力机制当输入输出都是不定长序列时, 比如机器翻译这种任务，需要使用 Sequence-to-sequence（seq2seq）或者 encoder-decoder 神经网络结构。这种结构可以通过一种方法叫注意力机制来显著提高性能。 编码器—解码器 Sequence-to-sequence（seq2seq）编码器：所谓编码，就是把不定长的输入序列输入RNN，以得出某种定长的编码信息。解码器：所谓解码，就是把编码器编码后的信息（一般取编码器的RNN最终时刻的隐含层变量）输入到解码器的RNN中，每个t时刻的输出既取决于之前时刻（t-1）的输出又取决于编码信息。等同于一个以解码信息作为条件概率生成目标语言句子的语言模型。 所以 seq2seq 本质是一个条件概率语言模型：语言模型是指解码器每次会预测下一个出现的单词，条件概率是指预测是基于编码后的源句子。 注意力在传统的seq2seq模型中，解码器各个时刻都使用相同的编码信息，这就要求解码器把源输入序列的所有信息都解码并整合到最后时刻的隐含状态中，这个是很大的信息瓶颈。而人们知道，在实际任务中，比如机器翻译，目标句子的不同单词，一般只对应源句子的某一部分而已。如果能够让解码器在解码时，在不同时刻专注于源输入序列的不同部分，那么就可以突破这个瓶颈。 对于解码器的每一时间步的隐含状态st，可以衡量其与编码器的所有时间步隐含状态h0……et的相似性(或score评分) e = α(s, h)，简单的评分方式是元素间相乘, e = s*h（Bahanau的论文提供了更复杂的形式), 也可以参考论文Effective Approaches to Attention-based Neural Machine Translation探讨的集中评分方式, 这篇论文提供了一种 Bilinear 形式的相似性评分法, 就是在s和h之间以点乘的形式插入一个交互矩阵 interaction matrix. 对得出的评分求加权平均a = softmax(e), 得出的权值分布也称注意力权重 通过注意力权重把编码器隐含状态加权求和，得到注意力输出 A = Σah 最后把注意力输出和对应时间步的解码器隐含状态st拼接在一起 [A;st]，作为解码器rnn的隐含层.]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理快速入门 | 02 N-Gram 语言模型 - ANLP UoE]]></title>
    <url>%2FNOTE-ANLP-02-n-gram-model%2F</url>
    <content type="text"><![CDATA[生成模型在语言模型中的应用包括 N-Gram语言模型，朴素贝叶斯分类器，隐马尔可夫模型。 在概率统计理论中, 生成模型是指能够生成观测数据的模型，尤其是在给定某些隐含参数的条件下。它给观测值和标注数据序列指定一个联合概率分布。 生成模型的定义与判别模型相对应：生成模型是所有变量的全概率模型，而判别模型是在给定观测变量值前提下目标变量条件概率模型。因此生成模型能够用于模拟（即生成）模型中任意变量的分布情况，而判别模型只能根据观测变量得到目标变量的采样。判别模型不对观测变量的分布建模，因此它不能够表达观测变量与目标变量之间更复杂的关系。因此，生成模型更适用于无监督的任务，如分类和聚类。 N-Gram 语言模型如何训练一个语言模型? 在神经网络大热之前, 人们普遍使用N-Gram语言模型。就是收集不同n-gram频率的统计数据，并用它们预测下一个单词，概率模型基于前面提到的马尔可夫简化假设：若使用N-Gram 来预测下一个单词，出现概率仅取决于前面的(N-1)个单词. 一个N-Gram就是n个连续的单词 Unigram: “the”, “students”, “opened”, ”their” Bigram: “the students”, “students opened”, “opened their” trigrams: “the students opened”, “students opened their” 4-grams: “the students opened their” 通过计数来估计统计概率：P(wi| prefixes) = count(prefixes, wi)/count(prefixes) = count(“the students”)/count(“the students opened”) 在实际中，使用 log 转换来避免数值下溢，并且log 转换可以把乘法转换为加法, 计算更快. N-Gram模型的缺点很明显： 无法很好地解决NLP中的长距离依赖现象 N-gram只是在测试语料库与训练语料库比较相似时表现才比较好。 稀疏问题1：大多数高阶N-Gram几乎不会出现，我们不能简单地把这些定义为0概率的，因为语言是千变万化的，有些词组虽然少见但不代表不存在 稀疏问题2：少部分低阶n-gram在测试集中出现了但是在训练集中没有。比如需要预测 “students opened their _”, 但是训练集中没出现过“students opened their”。 一般而言，N越高，模型表现越好，但是更大的N使稀疏问题变得更糟。通常人们不会取大于5的N。 需要存储所有可能的N-Gram，所以模型的大小是 O(exp(n)),需要大量的内存. 针对数据稀疏问题, 可以使用各种平滑处理. Add alpha smoothing Assign equal probability to all unseen events. Applied in text classification, or domains where zeros probability is not common. Backoff smoothing Use information from lower order N-grams (shorter histories) Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram. Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. 对于像网络数据这种非常大的N-gram，使用stupid backoff. Interpolation smoothing Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts Simple interpolation: P(w3|w1,w2)=1P(w3|w1,w2)+λ2P(w3|w2)+λ3P(w3), Σλ=1. λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data Kneser-Ney smoothing这是目前表现最好的平滑方案. Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams. Use a better estimate for probabilities of lower-order unigrams, the continuation probability, P_continuatin(w) is how likely is w to appear as a novel continutaion. For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w. Every bigram type was a novel continuation the first time it was seen. normalized by the total number of word bigram types. To lower the probability of some fix bigram like “San Franscio” For bigram, Pkn(wi|wi-1)=max(count(wi-1,wi)-d, 0)/c(wi-1) +λ(wi-1)P_continuatin(wi), λ(wi-1) = d{w:count(wi-1,w)&gt;0}/c(wi-1), where {w:count(wi-1,w)&gt;0} is the number of word types that can follow wi-1, also is the # of word types we discounted, also is the # of times we applied normalized discount. For general N-gram,]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理快速入门 | 01 概率模型 - 语言模型 - ANLP UoE]]></title>
    <url>%2FNOTE-ANLP-01-probability-model%2F</url>
    <content type="text"><![CDATA[爱丁堡大学的自然语言处理入门课程笔记。 References:Accelerated natural language processingANLP revision guideLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning 概率模型 Probability model概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率 估算概率 Probability estimation相关频率/最大似然估计Relative frequency / maximum likelihood estimation p(X) = Count(x)/N 平滑处理 Smoothing一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。 加一（Laplace）平滑最简单的平滑法，为所有事件（不管有没出现过）的频次加一，这样保证了没有0概率事件出现。这种平滑效果很差，因为齐夫定律Zipf&#39;s law的关系: 在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。 会有很多长尾单词很少甚至几乎没有出现过, 所以在总数为1的概率池子了, 为了给这些长尾单词分配至少频次1的概率, 需要从真正出现的单词(所谓真实发生的事件)中分走很多概率. 更多高级的平滑方案参考N-gram部分。 语言模型 Language modeling语言模型: 一种用于计算连续的单词（就是句子）或者任何其他序列数据（比如语音）出现的概率的模型，最基本的应用是基于某种语言模型，预测下一个单词出现的概率 P(w|w1, w2, w3...)。 语言模型本身即是一种概率模型(或者说人们选择用概率模型来描述为语言建模). 因为近现代的自然语言处理主要集中在信息沟通传输方面(比如密码,语音识别, 机器翻译, 校正等), 而香农的信息传输模型使用条件概率来描述鉴定噪音中的真实信息. 如何表达: 一个句子发生的概率就是里面各个单词的概率的乘积。依赖于概率的链式法则, 一个位置的单词的概率，条件于该位置之前的句子部分的概率。所以通过链式法则得出的语言模型概率是冗长的条件概率乘积。 但我们可以通过马尔可夫性质 Markov property 把语言模型简化，一个位置单词出现的条件概率可以通过某种近似来逼近: 仅考虑离它最近的（若干）单词，得到P(the| water is so transparent that) ≈ P(the| that). Evaluation：一般通过困惑度（Perplexity）来衡量语言模型的好坏。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java | 对象 哈希 重写 @Override equals() hashcode()]]></title>
    <url>%2FNOTE-Java-miscellaneous-hashcode-equals%2F</url>
    <content type="text"><![CDATA[主要介绍： Hashcode（哈希码）与 equals（判断相等）的关系 Hashcode 方法的底层实现原理 开发中需要掌握的原则和方法 HashSet, HashMap, HashTableHashSet底层是调用HashMap. HashMap 使用hashCode和equals来进行对象比较。拿HashSet和add()举例(其余的数据结构,和 remove, contains等方法类似):假设HashSet里面已经有了obj1, 那么当调用HashSet.add(obj2)时: if (obj1 == obj2), 那么没有必要调用 hashCode(), 已经有了这个对象, 没必要添加了 else, if hashCode 不同，那么可以直接添加了, 没必要进一步调用 obj1.equals(obj2) 来判断对象是否相等 else hashCode 相同，那么需要进一步调用obj1.equals(obj2) 下面这段代码虽然 HashSet 只存了 a 对象，但当检查是否包含 b 对象时，返回true。12345HashSet&lt;String&gt; wordSet = new HashSet&lt;String&gt;();String a = "hello";String b = "hello";wordSet.add(a);return wordSet.contains(b); // return true 根据Javadoc for Set. adds the specified element e to this set if the set contains no element e2 such that (e==null ? e2==null : e.equals(e2)). 根据Javadoc for String.equals Compares this string to the specified object. The result is true if and only if the argument is not null and is a String object that represents the same sequence of characters as this object. Java的set是使用它包含的元素（对象）的 equals()来比较 b 和 a 的。这里 String 类的equals()method 是比较字符串值是否相等(准确的说，是先检查是不是引用同一个对象，再看是不是同一个类，再比较值)，而不是引用的对象是否一样，故b.equals(a)是 true。 同样的，remove 和 add 也会先进行类似检查。 问题是，为何 hashCode 不同，就没有进一步调用 equals()的必要呢？因为有一个前提是代码遵守The hashCode contract。 Hashcode and equals在Java中，每个对象都有一个hashCode，它有时容易被人遗忘或误用。有以下三点需要注意，避免掉入常见的陷阱。 The hashCode contract根据 The hashCode contract: Objects that are equal must have the same hash code within a running process. 除了字面意思，也有其他隐含的意思: 不相等的对象的hashcode也可能一样; 具有相同 hash code 的对象不一定相等. You must override hashCode() in every class that overrides equals(). Failure to do so will result in a violation of the general contract for Object.hashCode(), which will prevent your class from functioning properly in conjunction with all hash-based collections, including HashMap, HashSet, and Hashtable. — Effective Java, by Joshua Bloch 根据这个contract，可以延伸出以下实践原则： 一、 每当你 override equals 时，也要 override hashCode假如你需要使用不一样的equals判断标准，那么就需要重写equals。但假如仅仅重写equals，而不重写hashcode()，就可能会违背 The hashCode contract。 为什么？因为 hashCode method 需要同时适配真正使用到的 equals method 的判断标准。通过重写equals，我们重新声明了一种判断对象是否相等的标准，但原始的 hashCode method还是会将所有对象视为不同的对象。所以如果没有不重写hashcode，那么根据@Override equals 判断为相同的对象将拥有不同的hashcode（可能）。这样，即使已经有了这个object，在HashMap上调用 contains() 也会返回false。 例子：在Java的创建街道street这个类，在判断两条街道是否相同时，我们有自定义的规则 - 只要是在同一个城市，有同样的街道名，那么两个street就相等，即使他们是存放在不同内存位置的两个对象（Java 的 Object 原生的equals是根据引用的对象内存地址来比较判断的）。1234567891011121314151617181920212223242526272829public class Street &#123; private String name; private String city; // ... @Override public boolean equals(Object obj) &#123; if (!(obj instanceof Street)) return false; if (obj == this) return true; Street rhs = (Street) obj; return new EqualsBuilder(). // if deriving: appendSuper(super.equals(obj)). append(name, rhs.name). append(age, rhs.city). isEquals(); &#125; @Override public int hashCode() &#123; return new HashCodeBuilder(17, 31). // two randomly chosen prime numbers // if deriving: appendSuper(super.hashCode()). append(name). append(city). toHashCode(); &#125;&#125; 如果没有重写hashCode()， 那么两个名字和所在城市一样的，但引用不同地址的street就会按照默认的 hashcode() 返回不一样的code，但是根据重写的equals(), 他们是一样的, 这样就违背了 hashCode contract。 为了安全起见，让Eclipse IDE 生成 equals 和 hashCode 函数：Source &gt; Generate hashCode() and equals()... 为了提醒自己, 还可以配置Eclipse以检测是否有违反此规则的情况，并为仅重写了equals但没重写hashCode的情况显示错误：Preferences &gt; Java &gt; Compiler &gt; Errors/Warnings, then use the quick filter to search for “hashcode” HashCode collisionsHashCode collisions 指两个不同的对象具有相同的hashcode这种情况, 这不是什么严重的问题. 只是会导致更多的搜索步骤，太多collisions就可能会降低系统性能 但是，如果将HashCode错误地用作对象的唯一句柄，例如将其用作Map中的key，那么有时会得到错误的对象。虽然collisions一般很少见，但却是不可避免的。例如，字符串“Aa”和“BB”产生相同的hashCode：2112. 因此衍生出第二个原则 二、永远不要把hashcode当做key来使用 Java中有4,294,967,296个（232)可能的int值）。既然拥有40亿个插槽，collisions似乎几乎不可能对吧？ 但事实上，也不是那么不可能。试想，一个房间里有23名随机人员。你如何估计里面有两个人生日一样的概率？很低？因为一年有365天？事实上，概率约为50％！这种现象被称为生日问题(悖论)。 如果一个房间里有23个或23个以上的人，那么至少有两个人的生日相同的概率要大于50%。 问题的本质是”23人之中两两之间存在生日相同的概率””,而不是”其他22人与其中一个人的生日相同的概率”. 类比到hashcode里，这意味着有77,163个不同的对象，collisions概率是50%（假设有一个理想的hashCode函数，将对象均匀分布在所有可用的buckets中）。 HashCodes 会变HashCode 不保证在不同的执行过程中总能返回相同的code。根据JavaDoc：Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application. 这种情况并不常见，实际上，库中的某些类甚至指定了用于计算hashcode的精确公式（例如String）。对于这些类，hashcode总是相同的。但是，尽管大多数的hashCode方法提供了稳定的值，但我们不能依赖它。正如这篇文章所指出的那样，Java库实际上在不同的进程中返回不同的hashCode值，这往往会让人们感到困惑。 Google的Protocol Buffers就是一个例子。 因此，您不应该在分布式应用程序中使用hash code。即使两者相等，远程对象的 hash code 也可能与本地的不同。 三、不要在分布式应用程序中使用 hashCode此外，要意识到，hashCode函数的实现可能会随着版本的更改而改变。因此我们的代码最好不依赖任何特定的hash code 值。例如，你不应该使用hash code来保持某种状态，不然下次运行时，“相同”对象的hash code可能会不同。 所以最好的建议可能是：除非自己创建了基于 hashcode 算法，否则根本就不要使用 hashCode 呵呵…… 总结在依赖于 HashSet, HashMap, HashTable … 等数据结构的程序中： 仅重写 equals()，会导致业务出错 仅重写 hashcode(), 在比较两个对象时不会强制Java忽略内存地址 如果不涉及对象比较(比如仅仅是iteration), 那么不需要hashCode and/or equals 参考：https://eclipsesource.com/blogs/2012/09/04/the-3-things-you-should-know-about-hashcode/https://stackoverflow.com/questions/27581/what-issues-should-be-considered-when-overriding-equals-and-hashcode-in-java]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 13 实现继承 Implementation Inheritance - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-13-Implementation-Inheritance%2F</url>
    <content type="text"><![CDATA[Implementation Inheritance实现继承是subclass从superclass继承实现的关系。 default method 缺省方法（从 Java 8开始）除了signature之外，Java也允许subclass继承implementation。我们可以在List中列出已实现的method。这些方法是 default method，定义了List hypernyms的一些默认行为：default public void method() { ... }.我们可以自由调用interface中定义的方法，而不用操心具体的实现。default method 应该适用于实现接口的任何类型的对象！子类不必在任何地方重新实现 default method，可以直接调用。1234// Listdefault public void print() &#123; ...&#125; 不过，我们仍然可以重写 default method，在子类中重新定义该方法。这样，只要我们在LinkedLList上调用print()，它就会调用这个方法，而不是List的。12345// LinkedList@Overridepublic void print() &#123; ...&#125; Dynamic typeJava是通过一个叫“dynamic method selection”的特性，来确定要调用 default method 还是已经被子类重写的method。这个实例声明List&lt;String&gt; l = new LinkedList&lt;String&gt;();,指明l的类型是 List, 是 static type。由 new 生成的 object 本身是LinkedList类型，也从属于 List 类型。但是，因为这个对象本身是使用 LinkedList 构造函数实例化的，所以我们称之为 dynamic type。 Dynamic type 的名称起源于: 当l被重新分配指向另一种类型的对象时，比如说一个 ArrayList 对象，l的动态类型现在就变为 ArrayList. 因为它根据当前引用的对象的类型而改变, 所以是动态的。 Static vs. Dynamic Type: Java 每个变量都有一个static type （compile-time type），这是变量声明时指定的类型，在编译时会检查。 每个变量也有一个 Dynamic Type（run-time type），此类型在变量实例化（new）时指定，并在运行时检查。等同于地址指向的对象的类型。 当Java运行一个被overriden的方法时，它会在它的dynamic type 中搜索合适的 method signature 并运行。 注意，如果是overload:123456public static void peek(List&lt;String&gt; list) &#123; ...&#125;public static void peek(LinkedList&lt;String&gt; list) &#123; ...&#125; 对于上面的实例化的l, 当Java检查要调用哪个方法时，它会检查 static type (此时是List)并使用相同类型的参数调用该方法(也就是使用List作为签名的那个方法)。 总结：区别 Interface Inheritance vs Implementation InheritanceInterface Inheritance 接口继承（what）：指定 subclass 应该实现的功能，即只提供 method signature。 Implementation Inheritance 实现继承（how）：提供功能的实现方案，即提供 method implementation。允许代码再利用，也给subclass设计者提供了更多的自由度，由他们自行决定是否重写 default method。 Implementation inheritance 也有一些缺点： 人会犯错。我们有可能忘了自己曾经重写过一个方法。 如果两个接口给出冲突的 default method，则可能很难解决冲突。 无形中鼓励代码复杂化。 最后，注意从属和拥有的区别：subclass 和 superclass 是上下级从属分类，而不是拥有与被拥有的关系，不要跟 nested class 混淆。 Oracle：Interface MethodsDefault methods and abstract methods in interfaces are inherited like instance methods. However, when the supertypes of a class or interface provide multiple default methods with the same signature, the Java compiler follows inheritance rules to resolve the name conflict. These rules are driven by the following two principles:]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 12 接口继承 Interface Inheritance - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-12-Interface-Inheritance%2F</url>
    <content type="text"><![CDATA[Interfaces InheritanceMotivation:扩展代码适用性我们前面创建的 LinkedList and ArrayList 其实很相似 - 所有的method都一样. 如果我们需要写一个需要用到 list 的类比如WordUtils class, 我们如何让它既可以使用LinkedList又可以用ArrayList？简单的方法及时写两个同名不同参数的methods。即所谓method overloading。public static String longest(LinkedList&lt;String&gt; list)public static String longest(ArrayList&lt;String&gt; list) 但 overload 有几个缺点: 超级重复，写两个几乎相同的代码块。 产生更多需要维护的代码，那意味着如果你想对的方法做一个小优化或debug，你需要在对应每种list的方法中改变它。 如果我们想要适配更多的列表类型，不得不复制每个新列表类的方法。 另一种方法是使用 interface 接口。 Hypernyms, Hyponyms, and Interface Inheritance 上位词，下义词和接口继承首先要理解，上位词和下位词是语言学的定义，直接沿用到编程语言中。就像狗是哈士奇的上位词，哈士奇是狗的下义词，在Java把这种关系形式化：如果LinkedList是List的Hyponyms，那么LinkedList类是List的subclass，而List类是LinkedList类的superclass(超类/父类)。 在Java中，为了表达这种层次结构，我们需要： 为 hypernym - 通用列表 List 定义类型。 指定LinkedList和ArrayList是该类型的hyponyms。 1234public interface List&lt;Item&gt; &#123; public void addFirst(Item x); ...&#125; 这里的 List 是Java中的 interface 接口。本质上是一个指定list必须能够做什么的合约，具体如何做并不是它关心的。 123456public class ArrayList&lt;Item&gt; implements List&lt;Item&gt;&#123; // 具体的执行 public void addFirst(Item x) &#123; insert(x, 0); &#125;&#125; 指定ArrayList是List的hyponyms. implements List&lt;Item&gt;类似一种承诺，保证将拥有并定义在List interface 中制定的所有属性（变量）和行为（方法）。 List中指定的方法的具体实现过程就是在这种hyponyms中实现的。 这样就可以同时适配多种list：123456789101112131415public class WordUtils &#123; /** Returns the length of the longest word. */ public static String longest(List&lt;String&gt; list) &#123; ... return list.get(maxDex); &#125; public static void main(String[] args) &#123; ArrayList&lt;String&gt; someList = new ArrayList&lt;&gt;(); //or LinkedList&lt;String&gt; someList = new LinkedList&lt;&gt;(); ... System.out.println(longest(someList)); &#125;&#125; Overriding 重写如果subclass和superclass有signature一样的method, 那么subclass就是在 override 重写这个方法。 Override 要与 overloaded 区别开，重载的方法虽同名，却不同signature。 在子类中实现合约指定的功能时，需要在method的signature顶部包含@Override标签。1234@Overridepublic void addFirst(Item x) &#123; ...&#125; 值得注意的是，即使不包含这个@Override，仍然重写了这个方法。所以技术上讲，它不是必须的。但是，它可以作为一个保障, 提醒编译器我们打算重写此方法，就好像有一个校对员, 如果过程中出现问题, 编译器可以提醒。假设当我们想 override addLast，却不小心写成addLsat。此时如果不包含@Override，那么可能无法发现错误。如果有了@Override，编译器就会提示我们修复错误。 总结：Interface Inheritance接口继承是指subclass继承superclass的所有方法/行为的关系： 子类继承父类 Interfaces 接口列出所有方法的签名，就像‘合约’，但没有具体的实现 根据‘合约’，由子类来实现且必须实现（override 重写）每一个method，否则无法通过编译 继承关系可以延续多代。例如，B可以继承A，C可以继承B. GRoE根据Java的Golden Rule of Equals，每一个赋值a = b，本质上是把b中的bits拷贝到a中，着要求b和a的类型相同。 同理, 假设public static String longest(List&lt;String&gt; list)既接受List, 也接受ArrayList和LinkedList，但是由于ArrayList和List是不同的类，那怎么遵守GRoE呢？ 因为ArrayList与List有着上下位包含的关系，这意味着ArrayList应该能够赋值给List的内存位中. 1234public static void main(String[] args) &#123; List&lt;String&gt; someList = new SLList&lt;String&gt;(); someList.addFirst("elk");&#125; 这段代码运行时，会创建SLList并将其地址存储在someList变量中。然后将字符串“elk”插入到由addFirst引用的SLList中。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Accelerated Natural Language Processing - Informatics - University of Edinburgh]]></title>
    <url>%2FAccelerated-Natural-Language-Processing-UoE%2F</url>
    <content type="text"><![CDATA[自然语言处理入门，知识点汇总。 References:Accelerated natural language processingANLP revision guideLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning 概率模型 Probability model概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率 估算概率 Probability estimation相关频率/最大似然估计Relative frequency / maximum likelihood estimation p(X) = Count(x)/N 平滑 Smoothing一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。 Language modeling What: To compute the probability of sentence /sequence of words P(w1, w2, w3…), or to predict upcomming words P(w|w1, w2, w3…)… a language model is also a probability model. Why: the motivation is that probability is essential in identifying information in noisy, ambiguous inputs: speech recognition, machine translation, spelling correction… How: rely on chain rule of probability, the products of a sequence of conditional probability. Simplified by Markov Assumption: approximate the conditional probability by only accounting several prefixes,P(the| water is so transparent that) ≈ P(the| that) Evaluation: how good is the model GENERATIVE PROBABILISTIC MODELSGenerative(joint) models palce probabilities P(c,d) over both observed data d and the hidden variables c (generate the obersved data from hidden stuff). N-Gram Language Model Unigram P(w1,w2,w3..) ≈ P(w1)*P(w2)*P(w3) Bigram P(wn| w1,w2,w3..) ≈ P(wn| wn-1) Estimate probability by counting:P(wi| prefixes) = count(prefixes, wi)/count(prefixes) In practice, use log space to avoid underflow, and adding is faster than multiplying. Insufficient: long-distance dependencies N-grams only work well for word prediction if the test corpus looks like the training corpus. To deal with 0 probability, commonly use Kneser-Ney smoothing, for very large N-grams like web, use stupid backoff. Add alpha smoothing Assign equal probability to all unseen events. Applied in text classification, or domains where zeros probability is not common. Backoff smoothing Use information from lower order N-grams (shorter histories) Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram. Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. Interpolation smoothing Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts Simple interpolation: P(w3|w1,w2)=1P(w3|w1,w2)+λ2P(w3|w2)+λ3P(w3), Σλ=1. λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data Kneser-Ney smoothing Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams. Use a better estimate for probabilities of lower-order unigrams, the continuation probability, P_continuatin(w) is how likely is w to appear as a novel continutaion. For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w. Every bigram type was a novel continuation the first time it was seen. normalized by the total number of word bigram types. To lower the probability of some fix bigram like “San Franscio” For bigram, Pkn(wi|wi-1)=max(count(wi-1,wi)-d, 0)/c(wi-1) +λ(wi-1)P_continuatin(wi), λ(wi-1) = d{w:count(wi-1,w)&gt;0}/c(wi-1), where {w:count(wi-1,w)&gt;0} is the number of word types that can follow wi-1, also is the # of word types we discounted, also is the # of times we applied normalized discount. For general N-gram, Naive Bayes classifier Application: Text classification, to classify a text, we calculate each class probability given the test sequence, and choose the biggest one. Evaluation: precision, recall, F-measure Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 Text classificationOr text categorization, method is not limited to NB, see lab7.Spam email, gender/authorship/language identification, sentiments analysis,(opinion extraction, subjectivity analysis)… Sentiments analysis For sentiment(or other text classification), word occurrence may matter more than word frequency. Thus it often improves performance to clip the word counts in each document at 1. This variant binary NB is called binary multinominal naive Bayes or binary NB. Remove duplicates in each data sample - bag of words representation, boolean features. Binarized seems to work better than full word counts. Deal with negation: like, not like, A very simple baseline that is commonly used in sentiment to deal with negation is during text normalization to prepend the prefix NOT_ to every word after a token of logical negation Sentiment lexicons: lists of words that are preannotated with positive or negative sentiment. To deal with insufficient labeled training data. A common way to use lexicons in the classifier is to use as one feature the totalcount of occurrences of any words in the positive lexicon, and as a second feature the total count of occurrences of words in the negative lexicon. Using just two features results in classifiers that are much less sparse to small amounts of training data, and may generalize better. See lab8. Naive Bayes Assumptions Bags of words: a set of unordered words/features with its frequency in the documents, their order was ignored. Conditional independence: the probabilities P(w|C) are independence given the class, thus a sequence of words(w1,w2,w3…) probability coculd be estimate via prducts of each P(wi|C) by walking through every pisition of the sequence, noted that the orders in the sequnce does not matter. NB Training Each classes’ prior probability P(C) is the percentage of the classes in the training set. For the test set, its probability as a class j, is the products of its sequence probability P(w1, w2, w3…|Cj) and P(Cj), normalized by the sequence probability P(w1, w2, w3…), which could be calculated by summing all P(w1, w2, w3…|Cj)*P(Cj). The joint features probability P(w1, w2, w3…|C) of each class is calculated by naively multiplying each word’s MLE given that class. In practice, to deal with 0 probability, we dun use MLE, instead we use add alpha smoothing. Why 0 probability matters? Because it makes the whole sequence probability P(w1, w2, w3…|C) 0, then all the other features as evidence for the class are eliminated too. How: first extract all the vocabulary V in the training set. Then, for each feature/word k, its add alpha smoothing probability estimation within a class j is (Njk + alpha)/(Nj+V*alpha). This is not the actual probability, but just the numerator. Naive bayes relationship to language modelling When using all of the words as features for naive bayes, then each class in naive bayes is a unigram languange model. For each word, assign probability P(word|C), For each sentence, assign probability P(S|C) = P(w1,w2,w3…|C) Running multiple languange models(classes) to assign probabilities, and pick out the highest language model. Hidden Markov Model What: The HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence. parameter λ: A Transition probability matrix, B Emission probability Application: part-of-speech tagging, name entity recognition(NEr), parse tree, speech recognition Hidden: these tags, trees or words is not observed(hidden) The three fundamental problems of HMM: decoding: discover the best hidden state sequnce via Viterbi algorithm Probability of the observation: Given an HMM with know parameters λ and an observation sequence O, determine the likelihood P(O| λ) (a language model regardless of tags) via Forward algorithm Learning: Given only the observed sequence, learn the best(MLE) HMM parameters λ via forward-backward algorithm, thus training a HMM is an unsupervised learning task. Part-of-speech tagging Part-of-speech(POS), word classes, or syntactic categories, a description of eight parts-of-speech: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner. noun 名詞 (代號 n. ) pronoun 代名詞 (代號 pron. ) verb 動詞 (代號 v. ) adjective 形容詞 (代號 adj. ) adverb 副詞 (代號 adv. ) preposition 介系詞 (代號 prep. ) conjunction 連接詞 (代號 conj. ) interjection 感歎詞 (代號 int. ) Motivation: Use model to find the best tag sequnce T for an untagged senetnce S: argmax P(T|S) -&gt; argmax P(S|T)*P(T), where P(T) is the transition (prior) probabilities, P(S|T) is the emission (likelihood) probabilities. Parts-of-speech can be divided into two broad supercategories: closed class types and open class types Search for the best tag sequnce: Viterbi algorithm evaluation: tag accuracy Transition probability matrix Tags or states Each (i,j) represent the probability of moving from state i to j When estimated from sequnces, should include beginning and end markers. Tag transition probability matrix: the probability of tag i followed by j Emission probability Also called observation likelihoods, each expressing the probability of an observation j being generated from a states i. Word/symbol Penn Treebank Viterbi algorithm Decoding task: the task of determining which sequence of variables is the underlying source of some sequence of observations. Intuition: The probability of words w1 followed by w2 with tag/state i and j (i,j is index of all Tags), is the chain rule of the probability of i followed by j and the probability of i output wi P(w1|i) and P(w2 |j), then choose the maximum from all the possible i j. Then using chain rule to multiply the whole sequence of words. The value of each cell Vt(j) is computed by recursively taking the most probable path that could lead us to this cell from left columns to right. See exampls in tutorial 2 Since HMM based on Markov Assumptions, so the present column Vt is only related with the nearby left column Vt-1. Forward algorithm Compute the likelihood of a particular observation sequence. Implementation is almost the same as Viterbi. Yet Viterbi takes the max over the previous path probabilities whereas the forward algorithm takes the sum. HMM Traininglearning the parameters of an HMM Forward-backward algorithm inputs: just the observed sequence output: the converged λ(A,B). For each interation k until λ converged: Compute expected counts using λ(k-1) Set λ(k) using MLE on the expected counts. Context-free grammarCFG(phrase-structure grammar) consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered toLexicon gether, and a lexicon of words and symbols. Probabilistic Context-Free GrammarPCFG(Stochastic Context-Free Grammar SCFG (SCFG)), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability. G = (T,N,S,R,P) T, N: Terminal and Non-terminal S: starts symbol R: Derive rule/grammar, N -&gt; N/C P: a probability function, for a given N, ΣP(N-&gt;Ni/Ci)=1. Normally P(S-&gt;NP VP)=1, because this is the only rule for S. PCFG could generates a sentence/tree, thus it is a language model, assigns a probability to the string of words constituting a sentence The probability of a tree t is the product of the probabilities of the rules used to generate it. The probability of the string s is the sum of the probabilities of the trees/parses which have that string as their yield. The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence. Application: Probabilistic parsing Shortage: lack the lexicalization of a trigram model, i.e only a small fraction of the rules contains information about words. To solve this problem, use lexicalized PCFGs Lexicalization of PCFGs The head word of phrase gives a good representation of the phrase’s structure and meaning Puts the properties of words back into a PCFG Word to word affinities are useful for certain ambiguities, because we know the probability of rule with words and words now, e.g. PP attachment ambiguity Recursive Descent Parsing It is a top-down, depth-first parser: Blindly expand nonterminals until reaching a terminal (word). If multiple options available, choose one but store current stateas a backtrack point (in a stack to ensure depth-first.) If terminal matches next input word, continue; else, backtrack can be massively inefficient (exponential in sentence length) if faced with local ambiguity infinite loop CKY parsingDynamic programmingWell-formed substring tableFor parsing, subproblems are analyses of substrings, memoized in well-formed substring table(WFST, chart). Chart entries are indexed by start and end positions in the sentence, and correspond to: either a complete constituent (sub-tree) spanning those positions (if working bottom-up), or a prediction about what complete constituent might be found (if working top-down). The chart is a matrix where cell [i, j] holds information about the word span from position i to position j: The root node of any constituent(s) spanning those words Pointers to its sub-constituents (Depending on parsing method,) predictions about whatconstituents might follow the substring. Probability CKY parsing Noisy channel model: The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. a probability model using Bayesian inference, input -&gt; noisy/errorful encoding -&gt; output, see an observation x (a misspelled word) and our job is to find the word w that generated this misspelled word. P(w|x) = P(x|w)\*P(w)/P(x) Noisy channel model of spelling using naive bayes The noisy channel model is to maximize the product of likelihood(probability estimation) P(s|w) and the prior probability of correct words P(w). Intuitively it is modleing the noisy channel that turn a correct word ‘w’ to the misspelling. The likelihood(probability estimation) P(s|w) is called the the channel/error model, telling if it was the word ‘w’, how likely it was to generate this exact error. The P(w) is called the language model DISCRIMINATIVE PROBABILISTIC MODELSDiscriminative(conditional) models take the data as given, and put a probability over hidden structure given the data, P(c|d). Exponential (Log-linear, MaxEnt, Logistic) modelsMake probability model from the linear combination of weights λ and features f as votes, normalized by the total votes. It is a probabilistic distribution: it estimates a probability for each class/label, aka Softmax. It is a classifier, choose the highest probability label. Application: dependency parsing actions prediction, text classification, Word sense disambiguation Topics categorizationTraining discriminative model Features in NLP are more general, they specify indicator function(a yes/no[0,1] boolean matching function) of properties of the input and each class. Weights: low possibility features will associate with low/negative weight, vise versa. Define features: Pick sets of data points d which are distinctive enough to deserve model parameters: related words, words contians #, words end with ing, etc. Regularization in discriminative modelThe issue of scale: Lots of features sparsity: easily overfitting: need smoothing Many features seen in training never occur again in test Optimization problem: feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. See tutorial 4. Solution: Early stopping Smooth the parameter via L2 regularization. Smooth the data, like the add alpha smoothing, but hard to know what artificial data to create Generative vs. Discriminative Models Navie bayes models multi-count correlated evidence: each feature is multipled in, even when you have multiple features telling the same informaiton. Maxent: solve this issue by weighting features so that model expectations match the observed(empirical) expectations. LINGUISTIC AND REPRESENTATIONAL CONCEPTSRegular Expressionsa language for specifying text search strings. Parsing Parsing is a combination of recognizing an input string and assigning a correct linguistic structure/tree to it based on a grammar. The Syntactic, Statistical parsing are constituent-based representations(context-free grammars). The Dependency Parsing are based on dependency structure(dependency grammars). Syntactic ParsingSyntactic parsing, is the task of recognizing a sentence and assigning a correct syntactic structure to it. Syntactic parsing can be viewed as a search search space: all possible trees generated by the grammar search guided by the structure of the space and the input. search direction top-down: start with root category (S), choose expansions, build down to words. bottom-up: build subtrees over words, build up to S. Search algorithm/strategy: DFS, BFS, Recursive descent parsing, CKY Parsing Challenge: Structual Ambiguity Statistical ParsingOr probabilistic parsing, Build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers. motivation: to solve the problem of disambiguation algorithm: probability CKY parsing evaluation: Compare the output constituency parser with golden standard tree, a constituent(part of the output parser) marked as correct if it spans the same sentence positions with the corresponding constituent in golder standard tree. Then we get the precision, recall and F1 measure. constituency: S-(0:10), NP-(0:2), VP-(0:9)… Precission = (# correct constituents)/(# in parser output), recall = (# correct constituents)/(# in gold standard) Not a good evaluation, because it higher order constituent is marked wrong simply it contains a lower level wrong constituent. Dependency ParsingConstituencyPhrase structure, organizes words into nested constituents. Groups of words behaving as a single units, or constituents. Noun phrase(NP), a sequence of words surrounding at least one noun. While the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase Preposed or Postposed constructions. While the entire phrase can be placed differently, the individual words making up the phrase cannot be. Fallback: In languages with free word order, phrase structure(constituency) grammars don’t make as much sense. Headed phrase structure: many phrase has head, VP-&gt;VB, NP-&gt;NN, the other symbols excepct the head is modifyer. Dependency syntaxDependency structure shows which words depend on (modify or are arguments of) which other words. A fully lexicalized formalism without phrasal constituents and phrase-structure rules: binary, asymmetric grammatical relations between words. More specific, head-dependent relations, with edges point from heads to their dependents. Motivation: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. E.g. we may need both S → NP VP and S → VP NP, but could not tell too much information simply looking at the rule. Dependencies: Identifies syntactic relations directly. The syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words. Relation between phrase structure and dependency structure Convert phrase structure annotations to dependencies via head rules. (Convenient if we already have a phrase structure treebank.): For a given lexicalized constituency parse(CFG tree), remove the phrasal categories, remove the (duplicated) terminals, and collapse chains of duplicates. The closure of dependencies give constituency from a dependency tree Dependency parsing Motivation: context-free parsing algorithms base their decisions on adjacency; in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective); we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present). Approach: Transition-based dependency parsing Transition-based dependency parsingtransition-based systems use supervised machine learning methods to train classifiers that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators(actions). Bottom up Like shift-reduce parsing, but the ‘reduce’ actions are specialized to create dependencies with head on left or right. configuration：consists of a stack, an input buffer of words or tokens, and a set of relations/arcs, a set of actions. How to choose the next action: each action is predicted by a discriminative classifier(often SVM, could be maxent) over each legal move. features: a sequence of the correct (configuration, action) pairs f(c ; x). Evaluation: accuracy (# correct dependencies with or ignore label)). Dependency tree Dependencies from a CFG tree using heads, must be projective: There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words. But dependency theory normally does allow non-projective structures to account for displaced constituents. Bounded and unbounded dependenciesUnbounded dependency could be considered as long distance dependency Long-distance dependencies: contained in wh-non-subject-question, “What flights do you have from Burbank to Tacoma Washington?”, the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the VP. AmbiguityStructural ambiguityOccurs when the grammar can assign more than one parse to a sentence. Attachment ambiguityA sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. Coordination ambiguitydifferent sets of phrases can be conjoined by a conjunction like and. E.g green egg and bread. Coordination: The major phrase types discussed here can be conjoined with conjunctions like and, or, and but to form larger constructions of the same type. Global and local ambiguity global ambiguity: multiple analyses for a full sentence, like I saw the man with the telescope local ambiguity: multiple analyses for parts of sentence. the dog bit the child: first three words could be NP (but aren’t). Building useless partial structures wastes time. Morphology 构词学（英语言学分科学名：morphology，“组织与形态”；morphology (/mɔːrˈfɒlədʒi/[1]) is the study of words, how they are formed, and their relationship to other words in the same language.），又称形态学，是语言学的一个分支，研究单词（word）的内部结构和其形成方式。如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher。构词学正是研究这种单字间组成的关系，并试着整理出其组成的规则。 Challenge of rich MorphologyFor a morphologically rich language, many issues would arise because of the morphological complexity. These productive word-formation processes result in a large vocabulary for these languages Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages For POS, augmentations become necessary when dealing with highly inflected or agglutinative languages with rich morphology like Czech, Hungarian and Turkish., part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than asingle primitive tag. Dependency grammar is better than constituency in dealing with languages that are morphologically rich。 morphemesThe way words are built up from smaller meaning-bearing units. Lemma Lexeme, refers to the set of all the forms that have the same meaning, lemma refers to the particular form that is chosen by convention to represent the lexeme. E.g: run, runs, ran, running are forms of the same lexeme, with run as the lemma. AffixesAdding additional meanings of various kinds. “+ed, un+” suffix : follow the stem Plural of nouns ‘cat+s’ Comparative and superlative of adjectives ‘small+er’ Formation of adverbs ‘great+ly’ Verb tenses ‘walk+ed’ All inflectional morphology in English uses suffixes Prefix: precede the stem In English: these typically change the meaning Adjectives ‘un+friendly’, ‘dis+interested’ Verbs ‘re+consider’ Some language use prefixing much more widely Infix: inserted inside the stem Circumfix: do both(follow, precede) Morphological parsingMethod: Finite-state transducers Finite-state transducersFST, a transducer maps between one representation and another; It is a kind of FSA which maps between two sets of symbols. Root Root, stem and base are all terms used in the literature to designate that part of a word that remains when all affixes have been removed. The root word is the primary lexical unit of a word, and of a word family (this root is then called the base word), which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents. E.g: In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’. Stem Stem is of concern only when dealing with inflectional morphology Stemming: reduce terms to their stems in info retrieval, E.g: In the form ‘untouchables’ the stem is ‘untouchable’, ‘touched’ -&gt; ‘touch’; ‘wheelchairs’ -&gt; ‘wheelchair’. Inflectional vs. Derivational MorphologyInflectional· nouns for count (plural: +s) and for possessive case (+’s)· verbs for tense (+ed, +ing) and a special 3rd person singular present form (+s)· adjectives in comparative (+er) and superlative (+est) forms. Derivational· Changing the part of speech, e.g. noun to verb: ‘word → wordify’· Changing the verb back to a noun· Nominalization: formation of new nouns, often verbs or adjectives Inflectional Derivational does not change basic meaning or part of speech may change the part of speech or meaning of a word expresses grammatical features or relations between words not driven by syntactic relations outside the word applies to all words of the same part of speech, inflection occurs at word edges: govern+ment+s, centr+al+ize+d applies closer to the stem Open-class Closed-classClosed classes are those with relatively fixed membership prepositions: on, under, over, near, by, at, from, to, with determiners: a, an, the pronouns: she, who, I, others conjunctions: and, but, or, as, if, when auxiliary verbs: can, may, should, are particles: up, down, on, off, in, out, at, by numerals: one, two, three, first, second, third Open-class Nouns, verbs, adjectives, adverbs Word senseA discrete representation of an aspect of a word’s meaning.How: Distributional semantic models Word sense disambiguationWSD, The task of selecting the correct sense for a word, formulated as a classification task. Chose features: Directly neighboring words, content words, syntactically related words, topic of the text, part-of-speech tag, surrounding part-of-speech tags, etc … CollocationA sequence of words or terms that co-occur more often than would be expected by chance. Lexical semantic relationshipsRelations between word senses synonym代名词, When two senses of two different words (lemmas) are identical, or nearly identical, the two senses are synonyms. E.g. couch/sofa vomit/throw up filbert/hazelnut car/automobile hyponym下义词, One sense is a hyponym of another sense if the first sense is more specific, denoting a subclass of the other. E.g. car is a hyponym of vehicle; dog is a hyponym of animal, and mango is a hyponym of fruit. hypernymSuperordinate, 上位词, vehicle is a hypernym of car, and animal is a hypernym of dog. similarityOr distance, a looser metric than synonymy.Two ways to measure similarity: Thesaurus词库-based: are words nearby in hypernym hierarchy? Do words have similar definitions? Distributional: do words have similar distributional contexts Distributional semantic modelsVector semantics(embeddings): The meaning of a word is represented as a vector. Two words are similar if they have similar word contexts vector. Term-context matrix(Co-occurrence Matrices): a word/term is defined by a vector over counts of context words. The row represent words, columns contexts. Problem: simple frequency isn’t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. “the” and “of” are very frequent, but maybe not the most discriminative. Sulution: use Pointwise mutual information. Then the Co-occurrence Matrices is filled with PPMI, instead of raw counts. Measuring vectors similarity based on PPMI: Dot product(inner product): More frequent words will have higher dot products, which cause similarity sensitive to word frequency. Cosine: normalized dot product , Raw frequency or PPMI is non-negative, so cosine range [0,1]. Evaluation of similarity Intrinsic: correlation between algorithm and human word similarity ratings. Check if there is correlation between similarity measures and word frequency. Application: sentiment analysis, see lab8 Pointwise mutual informationPMI: do events x and y co-occur more than if they were independent? PMI between two words: Compute PMI on a term-context matrix(using counts):12345PMI(x, y) = log2( N·C(x, y)/C(x)C(y) )p(w=information,c=data) = 6/19p(w=information) = 11/19p(c=data) = 7/19PMI(information,data) = log2(6\*19/(11\*7)) PMI is biased towards infrequent events, solution: Add-one smoothingPPMIPositive PMI, could better handle low frequenciesPPMI = max(PMI,0) t-testThe t-test statistic, like PMI, can be used to measure how muchmore frequent the association is than chance. The t-test statistic computes the difference between observed and expected means, normalized by the variance. The higher the value of t, the greater the likelihood that we can reject the null hypothesis. Null hypothesis: the two words are independent, and hence P(a,b) = P(a)P(b) correctly models the relationship between the two words. Minimum Edit Distancethe minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.Algorithm: searching the shortest path, use Dynamic programming to avoid repeating, (use BFS to search the shortest path?) WordNetA hierarchically organizesd lexical database, resource for English sense relations Synset: The set of near-synonyms for a WordNet sense (for synonym set) Topic modellingInstead of using supervised topic classification – rather not fix topics in advance nor do manual annotation, Use clustering to teases out the topics. Only the number of topics is specified in advance. Latent Dirichlet allocation(LDA): each document may be viewed as a mixture of various topics where each document is generated by LDA. A topic is a distribution over words generate document: Randomly choose a distribution over topics For each word in the document randomly choose a topic from the distribution over topics randomly choose a word from the corresponding topic (distribution over the vocabulary) training: repeat until converge assign each word in each document to one of T topics. For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t) Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d) Meaning representation languageThe symbols in our meaning representations correspond to objects, properties, and relations in the world. Qualifications of MRL: Canonical form: sentences with the same (literal) meaning should have the same MR. Compositional: The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined. Verifiable: Can use the MR of a sentence to determine whether the sentence is true with respect to some given model of the world. Unambiguous: an MR should have exactly one interpretation. Inference: we should be able to verify sentences not only directly, but also by drawing conclusions based on the input MR and facts in the knowledge base. Expressivity: the MRL should allow us to handle a wide range of meanings and express appropriate relationships between the words in a sentence. Good MRL: First-order Logic First-order LogicFOL, Predicate logic, meets all of the MRL qualifications except compositionality. Expressions are constructed from terms: constant and variable symbols that represent entities function symbols that allow us to indirectly specify entities predicate symbols that represent properties of entities and relations between entities Terms can be combined into predicate-argument structures Logical connectives: ∨ - or, ∧ - and, ¬, ⇒ Quantifiers: ∀ (universal quantifier, i.e., “for all”), ∃ (existentialquantifier, i.e. “exists”) Predicates in FOL Predicates with multiple arguments represent relations between entities: member-of(UK, EU) “/N” to indicate that a predicate takes N arguments: member-of/2 Variables in FOL An expression consisting only of a predicate with a variable among its arguments is interpreted as a set: likes(x, Gim) is the set of entities that like Gim. A predicate with a variable among its arguments only has a truth value if it is bound by a quantifier: ∀x.likes(x, Gim) has an interpretation as either true or false. Universal Quantifier (∀): Cats are mammals has MR ∀x.cat(x) ⇒ mammal(x) Existential Quantifier (∃): Used to express that a property/relation is true of some entity, without specifying which one: Marie owns a cat has MR ∃x.cat(x) ∧ owns(Marie,x) Lambda λ ExpressionExtend FOL, to work with ‘partially constructed’ formula, Compositionality. E.g.： λx.sleep(x) is the function that takes an entity x to the FOL expression sleep(x). λx.sleep(x)(Marie) -&gt; sleep(Marie) Verbal (event) MRs： λz. λy. λx. Giving1(x,y,z) (book)(Mary)(John) -&gt; Giving1(John, Mary, book) -&gt; John gave Mary a book Problem: fixed arguments Requires separate Giving predicate for each syntactic subcategorisation frame(number/type/position of arguments). Separate predicates have no logical relation: if Giving3(a, b, c, d, e) is true, what about Giving2(a, b, c, d) and Giving1(a, b, c). Solution: Reification of events 事件具象化 Reification of eventsJohn gave Mary a book -&gt; ∃e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Reify: to “make real” or concrete, i.e., give events the same status asentities. In practice, introduce variables for events, which we can quantify over Entailment relations: automatically gives us logical entailment relations between events1234[John gave Mary a book on Tuesday] -&gt; [John gave Mary a book]∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) ∧ Time(e, Tuesday)-&gt;∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Semantic ParsingAka semantic analysis. Systems for mapping from a text string to any logical form. Motivation: deriving a meaning representation from a sentence. Application: question answering Method: Syntax driven semantic analysis with semantic attachments Syntax Driven Semantic Analysis Principle of compositionality: the construction of constituent meaning is derived from/composed of the meaning of the constituents/words within that constituent, guided by word order and syntactic relations. Build up the MR by augmenting CFG rules with semantic composition rules. Add semantic attachments to CFG rules. Problem: encounter invalide FOL for some (base-form) MR, need type-raise. Training Semantic attachmentsE.g123456VP → Verb NP : &#123;Verb.sem(NP.sem)&#125;Verb.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y)NP.sem = Meat-&gt;VP.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) (Meat)= λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) The MR for VP, is computed by applying the MR function to VP’s children. Complete the rule:123456S → NP VP : &#123;VP.sem(NP.sem)&#125;VP.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat)NP.sem = AyCaramba-&gt;S.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) (AyCa.)= ∃e. Serving(e) ∧ Server(e, AyCaramba) ∧ Served(e, Meat) Lexical semanticsthe meaning of individual words. EVALUATION CONCEPTS AND METHODSInstrinsic vs. extrinsic evaluationExtrinsicUse something external to measure the model. End-to-end evaluation, the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Put each model in a task: spelling corrector, speech recognizer, MT system Run the task, get an accuracy for A and for B How many misspelled words corrected properly How many words translated correctly Compare accuracy for A and B Unfortunately, running big NLP systems end-to-end is often very expensive. IntrinsicMeasures independenly to any application. Train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. Which means whichever model assigns a higher probability to the test set Perplexity It is intrinsic. Intuition based on Shannon game:The best language model is one that best predicts an unseen test set(e.g. next word), gives the highest P(sentence) to the word that actually occurs. Definition： Perplexity is the inverse probability of the test set, normalized by the number of words(lie between 0-1). So minimizing perplexity is the same as maximizing probability Cannot divide 0, so use smoothing. Bad approximation: unless the test data looks just like the training data, so generally only useful in pilot experiments. Human evaluationE.g to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying togold labels match. We will refer to these human labels as the gold labels. Precision, Recall, F-measure To deal with unbalanced lables Application: text classification, parsing. Evaluation in text classification: the 2 by 2 contingency table, golden lable is true or false, the classifier output is positive or negative. Precision% of positive items that are golden correct, from the view of classifier Recall% of golden correct items that are positive, from the view of test set. F-measure Motivation: there is tradeoff between precision and recall, so we need a combined meeasure that assesses the P/R tradeoff. The b parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of b &gt; 1 favor recall, while values of b &lt; 1 favor precision. Balanced F1 measure with beta =1, F = 2PR/(P+R) Confusion matrixRecalled that confusion matrix’s row represent golden label, column represent the classifier’s output, to anwser the quesion：for any pair of classes(c1,c2), how many test sample from c1 were incorrectly assigned to c2&gt; Recall: Fraction of samples in c1 classified correctly, CM(c1,c1)/sum(CM(c1,:)) Precision: fraction of samples assigned c1 that are actually c1, CM(c1,c1)/sum(CM(:,c1)) Accuracy: sum of diagnal / all CorrelationWhen two sets of data are strongly linked together we say they have a High Correlation.Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases. Pearson correlation: covariance of the two variables divided by the product of their standard deviations. Spearman correlation: the Pearson correlation between the rank values of the two variables]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 11 测试 Testing - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-11-testing%2F</url>
    <content type="text"><![CDATA[测试如何知道自己的程序是否真的在工作？在现实世界中，程序员相信他们的代码，因为代码通过了他们自己编写的测试。常用的测试有 Ad Hoc Testing， Unit test 和 Integration Testing。 Ad Hoc Testing，是指没有计划和记录的软件测试，除非发现缺陷，不然一般只运行一次。 Unit test程序可分解为单元（或程序中可测试的最小部分），它严格测试代码的每个单元，最终确保项目正确运行。好处是： Unit test 保证良好的代码结构（每个 method “只打一份工”），帮助我们较好地解析任务， 允许我们考虑每个方法的所有边界情况，并单独测试它们。 让我们每次只专注于一个单元，进行测试，debug，对准确度有信心后，再进行下一个单元的开发。相比于一次性写完所有代码，再测试debug，Unit test 减少了 debugging 时间。 坏处是： 测试也要花时间 测试本身也是有可能出错的，测试可能不全面，不规范，或者有bug 有些单元是依赖于其他单元的 Unit testing 无法保证各个模块的交互，无法保证整个系统作为一个整体是否正常工作。 JUnitJUnit是一个给Java做测试的框架，由Erich Gamma（Design Patterns）和Kent Beck（eXtreme Programming）编写。JUnit使用Java的 reflection 功能（Java程序可以检查自己的代码）和注释。JUnit允许我们： 定义并执行测试和测试套件 使用测试作为规范的有效手段 使用测试来支持重构 将修改的代码集成到构建中JUnit可用于多个IDE，例如BlueJ，JBuilder和Eclipse在一定程度上具有JUnit集成。 1234567import org.junit.Test;import static org.junit.Assert.*;@Testpublic void testMethod() &#123; assertEquals(&lt;expected&gt;, &lt;actual&gt;);&#125; assertEquals测试一个变量的实际值是否等于它的期望值。JUnit test 各个测试方法，必须是非静态的（JUnit的设计人员设计规定的）。 JUnit的术语 Test runner：测试运行器， 运行测试和报告结果的软件。实现方式：集成到IDE中，独立GUI，命令行等 Test suite：测试套件是一组测试用例。 Test case：测试用例用于测试单个方法对特定输入集的响应。 Unit test：单元测试的单元，是代码中我们能够相对合理地测试的最小的元素，通常是单个类。 常用的JUnit接口和方法@Before: Creates a test fixture by creating and initialising objects and values. @After: Releases any system resources used by the test fixture. Java usually does this for free, but files, network connections etc. might not get tidied up automatically. @Test：tests cases. static void assertTrue(boolean test), static void assertTrue(String message, boolean test), static void assertFalse(boolean test), static void assertFalse(String message, boolean test) Integration Testing鉴于 Unit testing 无法保证，有交互的多个模块，作为一个整体是否正常工作。我们可能需要 integration testing，把各个模块合并，作为一个组合，进行测试（也可以把 Unit test 组合起来变成 integration testing）。 Integration testing 一般都比较麻烦，也不容易自动化，而且一般是在比较高的抽象层进行测试，可能会漏掉微小的错误。 当把所有模块都作为一个整体，也就是整个系统作为测试对象时，就是 system testing。 Test driven developmentTDD开发步骤： 明确一项新功能需求。 为该功能编写 Unit test。 运行测试，按理应该无法通过测试（因为还没写功能程序）。 编写通过实现该功能的代码，通过测试。 可选：重构代码，使其更快，更整洁等等。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从程序员的视角理解计算机系统 - csapp - CMU 15213]]></title>
    <url>%2FNOTE-csapp%2F</url>
    <content type="text"><![CDATA[本笔记是csapp的学习笔记, 使用 CMU 15-213, UW CSE351 的课程视频, lab, 作业, project 辅助练习. Computer Systems: A Programmer’s Perspective (csapp), 豆瓣-深入理解计算机系统 卡内基梅隆大学 CMU 15-213 Introduction to Computer Systems (ICS) 华盛顿大学 UW CSE351: The Hardware/Software Interface Computer systemInformation is Bits + ContextStudy systems by tracing the lifetime of the hello program, from the time it is created by a programmer, until it runs on a system, prints its simple message, and terminates.12345#include &lt;stdio.h&gt;int main()&#123; printf("hello, world\n");&#125; The source program is a sequence of bits, each with a value of 0 or 1, organized in 8-bit chunks(bytes). Each byte represents some text character in the program. All information in a system — including disk files, programs stored in memory, user data stored in memory, and data transferred across a network—is represented as a bunch of bits. Programs are traslated by other programs into different formsThe hello program begins as a high-level C program because it can be read and understood by human beings in that form. However, in order to run hello.c on the system, the individual C statements must be translated by other programs into a sequence of low-level machine-language instructions. These instructions are then packaged in a form called an executable object program and stored as a binary disk file. Object programs are also referred to as executable object files. The programs that perform the four phases (preprocessor, compiler, assembler, and linker) are known collectively as the compilation system. Preprocessing phase.The preprocessor (cpp) modifies the original C program according to directives that begin with the # character. Compilation phase. The compiler (cc1) translates the text file hello.i into the text file hello.s, which contains an assembly-language program. Assembly language is useful because it provides a common output language for different compilers for different high-level languages. Assembly phase. Next, the assembler (as) translates hello.s into machinelanguage instructions, packages them in a form known as a relocatable object program, and stores the result in the object file hello.o. The hello.o file is a binary file whose bytes encode machine language instructions rather than characters. Linking phase. The printf function resides in a separate precompiled object file called printf.o, which must somehow be merged with our hello.o program. The linker (ld) handles this merging. Processors read and interpret instructions stored in memoryThe hello.c source program has been translated by the compilation system into an executable object file called hello that is stored on disk, to run the executable file on Unix:123unix&gt; ./hellohello, worldunix&gt; The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then performs the command. Hardware organization of a systemsHardware organization of a typical system. BusesElectrical conduits that carry bytes of information back and forth between the components. Buses are typically designed to transfer fixed-sized chunks of bytes known as words. USB: Universal Serial bus. Input/output (I/O) devicesThe system’s connection to the external world. Each I/O deviceisconnected to the I/O bus by either a controller or an adapter： Controllers are chip sets in the device itself or on the system’s main printed circuit board (often called the motherboard). An adapter is a card that plugs into a slot on the motherboard. Main MemoryA temporary storage device that holds both a program and the data it manipulates while the processor is executing the program. Physically, main memory consists of a collection of dynamic random access memory (DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address (array index) starting at zero*Processor: Central Processing Unit (CPU) PC: Program counter, a word-sized storage device (or register) at CPU core. At any point in time, the PC points at (contains the address of) some machine-language instruction in main memory. Register: a quickly accessible location available to CPU, Register file: an array of registers, each with its own unique name. Arithmetic/logic unit: ALU computes new data and address values. A processor repeatedly executes the instruction pointed at by the program counter and updates the program counter to point to the next instruction. The processor reads the instruction from memory pointed at by the PC, interprets the bits in the instruction, performs some simple operation dictated by the instruction, and then updates the PC to point to the next instruction. CPU operations examplesLoad: Copy a byte or a word from main memory into a register, overwriting the previous contents of the register. Store(write): Copy a byte or a word from a register to a location in main memory, overwriting the previous contents of that location. Operate: Copy the contents of two registers to the ALU, perform an arithmetic operation on the two words, and store the result in a register, overwriting the previous contents of that register. Jump: Extract a word from the instruction itself and copy that word into the program counter (PC), overwriting the previous value of the PC. Branch greater than (BGT): compares two registers and decides whether to branch (target would be the address to branch to), i.e. it is implementing the “if” decision. Running a programs Initially, the shell program is waiting for user types a command. As we type the characters “./hello” at the keyboard, the shell program reads each one into a register, and then stores it in memory. When we hit the enter key on the keyboard, the shell knows that we have finished typing the command. The shell then loads the executable hello file by executing a sequence of instructions that copies the code and data in the hello object file from disk to main memory. The data include the string of characters “hello, world\n” that will eventually be printed out. Using a technique known as direct memory access (DMA), the data travels directly from disk to main memory, without passing through the processor. Once the code and data in the hello object file are loaded into memory, the processor begins executing the machine-language instructions in the hello program’s main routine. These instructions copy the bytes in the hello, world\n string from memory to the register file, and from there to the display device, where they are displayed on the screen. CachesAn important lesson from this simple example is that a system spends a lot of time moving information from one place to another. From a programmer’s perspective, muchof this copying is overhead that slows down the “real work” of the program. Because of physical laws, larger storage devices are slower than smaller storage devices. Speed that processor read from: register &gt; memory &gt; disk. It is easier and cheaper to make processors run faster than it is to make main memory run faster. To deal with the processor-memory gap, system designers include smallerfaster storage devices called cache memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to need in the near future. The L1 and L2 caches are implemented with a hardware technology known as static random access memory (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3. By setting up caches to hold data that is likely to be accessed often, we can perform most memory operations using the fast caches. Storage Devices Form a Hierarchy Operating systemThe operating system has two primary purposes: (1) to protect the hardware from misuse by runaway applications, and (2) to provide applications with simple and uniform mechanisms for manipulating complicated and often wildly different low-level hardware devices. Think of the operating system as a layer of software interposed between the application program and the hardware, with fundamental abstractions: processes, virtual memory, and files. Process 进程A process is the operating system’s abstraction for a running program.A single CPU can appear to execute multiple processes concurrently by having the processor switch among them. The operating system performs this interleaving with a mechanism known as context switching. Thread 线程In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system. In most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources. Threads are an increasingly important programming model because of the requirement for concurrency in network servers, because it is easier to share data between multiple threads than between multiple processes, and because threads are typically more efficient than processes. Concurrency and ParallelismConcurrency: general concept of a system with multiple, simultaneous activities.Parallelism: the use of concurrency to make a system run faster. Parallelism could be achieved in different levels of abstraction in computer system. There are three common levels (from the highest to the lowest level in the system hierarchy): Thread-Level ConcurrencyWith threads, we can even have multiple control flows executing within a single process. When we construct a system consisting of multiple processors all under the control of a single operating system kernel, we have a multiprocessor system Multi-core processorsSeveral CPUs (referred to as “cores”) integrated onto a single integrated-circuit chip HyperthreadingSometimes called simultaneous multi-threading, is a technique that allows a single CPU to execute multiple flows of control. instruction-level parallelismAt a much lower level of abstraction, modern processors can execute multiple instructions at one time. Single-Instruction, Multiple-Data (SIMD) ParallelismAt the lowest level, special hardware that allows a single instruction to cause multiple operations to be performed in parallel. Memory, Data, &amp; Addressing十进制，2进制，16进制。内存位置。 Address and Pointers地址是内存的位置，指针是一种包含地址的数据对象。 Byte ordering: Endianness - big endian vs. little endian Addresses and pointer in C Variable declarations： int x, find location in memory in which to store integer. Pointer declarations use *: int *pointer, declares a variable pointer that is a pointer to a integer data item. Assignment to a pointer: pointer = &amp;x, assigns pointer to point to the address where x is stored. To use the value pointed to by a pointer, use *: if pointer = &amp;x, then x = *pointer +1 is the same as x = x + 1 *(&amp;x) = x]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>计算机入门</tag>
        <tag>计算机科学</tag>
        <tag>csapp</tag>
        <tag>C 语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 10 LinkedList 还是 ArrayList - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-10-java-which-list%2F</url>
    <content type="text"><![CDATA[Java 提供了 ArrayList, ArrayDeque 和 LinkedList 几个API.队列 queue, 通俗的含义, 就是不能插队, 只能在末尾插入.Deque 就是双端队列 Double Ended Queue。双端队列是具有动态大小的序列容器，可以在两端（前端或后端）扩展或收缩（定义来源 cplusplus.com）. CS61b的project 1a就是实现两种双端队列（array based 和 linkedklist based）. 不同的API, 在考虑什么时候应该用哪个时, 我们需要考虑它们的性能差异: 搜索/定位：与LinkedList搜索操作相比，ArrayList搜索操作更快。 ArrayList的get(int index)性能是O(1)的，而LinkedList的性能是O(n)。因为ArrayList基于array数据结构，可以直接用靠 array index 索引元素。 删除/插入：LinkedList 操作性能是O(1)，而ArrayList的性能从O(n)（删除/插入第一个元素）到O(n)（最后一个元素）都有可能。因为LinkedList的每个元素都包含两个指向其相邻前后元素的指针（地址），因此仅需要改变，被删节点的prev和next指针位置。而在ArrayList中，需要移动剩余元素，来重新填充array空间。 内存开销：LinkedList的每个元素都有更多的内存开销(额外的指针), 而ArrayLists没有这个开销。但是，ArrayLists需要占用初始容量。一般ArrayList的默认初始容量非常小（Java 1.4 - 1.8使用10）。但是，往ArrayLists添加元素时， 它可能会适当地增大容量，所以如果添加了很多元素，则必须不断调整数组的大小，那样也可能会导致元素频繁挪动位置。 综上所述： 如果在应用中需要频繁插入和删除，那么选择LinkedList。 假如一开始，就知道后面要添加大量元素，那就使用较高的初始容量来构造ArrayList。 大部分用例中, 相比LinkedList, 人们更偏爱ArrayList以及ArrayDeque。如果你不确定应该选哪个, 那么就直接考虑ArrayList吧(参考).]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抽象编程 - C++ 算法与数据结构 Stanford cs106b]]></title>
    <url>%2FNOTE-CS106B-Programming-Abstractions-Stanford%2F</url>
    <content type="text"><![CDATA[Stanford CS106B Programming Abstractions 和 CS106A 的学习笔记. 我的课程作业(cs106b spring 2017)实现代码见 https://github.com/ShootingSpace/cs106b-programming-abstraction Topics:Recursion, algorithms analysis (sort/search/hash), dynamic data structures (lists, trees, heaps), data abstraction (stacks, queues, maps), implementation strategies/tradeoffs Purposes: become acquainted with the C++ programming language learn more advanced programming techniques explore classic data structures and algorithms and apply these tools to solving complex problemsReference Text Book: Data Structures &amp; Algorithm Analysis in C++, 4th ed, by Mark A. Weiss Text Book: Programming Abstractions in C++ 1st Edition by Eric Roberts Text Book: Algorithms, 4th Edition Blog: Red Blob Games, Amit’s A* Pages Coding style Works correctly in all situations: Using a listing of specific test cases to exercise the program on. The overall approach is straight-forward, data structure is cleanly organized, tasks are nicely decomposed, algorithms are clear and easy to follow, comments are helpful, layout is consistent.CommentingExamples of information you might include in comments: General overview. What are the goals and requirements of this program? this function? The overview comment should also contain author and version information: who worked on this file and when. Data structures. How is the data stored? How is it ordered, searched, accessed? Design decisions. Why was a particular data structure or algorithm chosen? What other strategies were tried and rejected? Error handling. How are error conditions handled? What assumptions are made? What happens if those assumptions are violated? Nitty-gritty code details. Comments are invaluable for explaining the inner workings of particularly complicated (often labeled “clever”) paths of the code. Planning for the future. How might one make modifications or extensions later? And more… (This list is by no means exhaustive) ADTDefinitionAn abstract data type is a set of objects together with a set of operations. Abstract data types are mathematical abstractions; nowhere in an ADT’s definition is there any mention of how the set of operations is implemented.Objects such as lists, sets, and graphs, along with their operations, can be viewed as ADTs.Also there are search tree, set, hash table, priority queue. Client uses class as abstraction Invokes public operations only Internal implementation not relevant! Client can’t and shouldn’t muck with internals Class data should private Imagine a “wall” between client and implementor Wall prevents either from getting involved in other’s business Interface is the “chink” in the wall Conduit allows controlled access between the two Consider Lexicon Abstraction is a word list, operations to verify word/prefix How does it store list? using array? vector? set? does it matter to client? Why ADTs? Abstraction: Client insulated from details, works at higher-level Encapsulation: Internals private to ADT, not accessible by client Independence: Separate tasks for each side (once agreed on interface) Flexibility: ADT implementation can be changed without affecting client Vector and list in the STLThe C++ language includes, in its library, an implementation of common data structures.This part of the language is popularly known as the Standard Template Library (STL). In general, these data structures are called collections or containers. IteratorsIn the STL, a position is represented by a nested type, iterator. Getting an Iterator iterator begin( ) returns an appropriate iterator representing the first item in thecontainer. iterator end( ) returns an appropriate iterator representing the endmarker in thecontainer (i.e., the position after the last item in the container). Iterator Methods itr++ and ++itr advances the iterator itr to the next location. Both the prefix and postfix forms are allowable. itr returns a reference to the object stored at iterator itr’s location. The reference returned may or may not be modifiable (we discuss these details shortly). itr1==itr2 returns true if iterators itr1 and itr2 refer to the same location and false otherwise. itr1!=itr2 returns true if iterators itr1 and itr2 refer to a different location and false otherwise. Container Operations that require IteratorsThe three most popular methods that require iterators are those that add or remove from the list (either a vector or list) at a specified position: iterator insert( iterator pos, const Object &amp; x ): adds x into the list, prior to theposition given by the iterator pos. This is a constant-time operation for list, but not forvector. The return value is an iterator representing the position of the inserted item. iterator erase( iterator pos ): removes the object at the position given by the iterator. This is a constant-time operation for list, but not for vector. The return value is the position of the element that followed pos prior to the call. This operation invalidates pos, which is now stale, since the container item it was viewing has been removed. iterator erase( iterator start, iterator end ): removes all items beginning at position start, up to, but not including end. Observe that the entire list can be erased by the call c.erase( c.begin( ), c.end( ) ) Range for loopC++11 also allows the use of the reserved word auto to signify that the compiler will automatically infer the appropriate type, for simple data type: 12for( auto x : squares ) cout&lt;&lt; x; for complicate data type like map: Each element of the container is a map&lt;K, V&gt;::value_type, which is a typedef for std::pair&lt;const K, V&gt;. Consequently, you’d write this as 123for (auto&amp; kv : myMap) &#123; std::cout &lt;&lt; kv.first &lt;&lt; " has value " &lt;&lt; kv.second &lt;&lt; std::endl;&#125; RecursionHelper Function No clear definition of helper function How to utilize helper function to help constructing recursion algarithm: construct a same-name recursive function with extra parameters to pass in. In some other cases, decomposition with several step into a function is itself a helper function, which help to make the main function simple and clean. Exhaustive recursionPermutations/subsets are about choice Both have deep/wide tree of recursive calls Depth represents total number of decisions made Width of branching represents number of available options per decision Explores every possible option at every decision point, typically very expensive, N! permutations, 2N subsets Recursive BacktrackingPartial exploration of exhaustive space. In the case that if we are interested in finding any solution, whichever one that works out first is fine. If we eventually reach our goal from here, we have no need to consider the paths not taken. However, if this choice didn’t work out and eventually leads to nothing but dead ends; when we backtrack to this decision point, we try one of the other alternatives. The back track based on the stacks of recursion, if a stack return false (or fail result), we back to previous stack and try another way(un-making choice). Need something return(normally bool) to step out of the entire recursion once any one solution found. One great tip for writing a backtracking function is to abstract away the details of managing the configuration (what choices are available, making a choice, checking for success, etc.) into other helper functions so that the body of the recursion itself is as clean as can be. This helps to make sure you have the heart of the algorithm correct and allows the other pieces to be developed, test, and debugged independently. PointerlvalueIn C++, any expression that refers to an internal memory location capable of storing data is called an lvalue (pronounced “ell-value”).x = 1.0; Declaring pointer variables123456789101112131415161718192021222324252627282930313233int main() &#123; -------------------------------------------------- // Declaration, in the stack // Not yet initialized! int num; int *p, *q; // If cout &lt;&lt; num &lt;&lt; p &lt;&lt; q &lt;&lt; endl; // There will be junk number, junk address. // If now *p=10, it may blow up, because what *p point to is an address points to somewhere around that could be invalid. --------------------------------------------------- // new operator allocate memory from the heap, returns address p = new int; // P -----&gt; [ int ] （heep 1000） *p = 10; // P -----&gt; [ 10 ] （heep 1000） q = new int; // P -----&gt; [ int ] （heep 1004） *q = *p; // q -----&gt; [ 10 ] （heep 1004） q = p; // q -----&gt; [ 10 ] （heep 1000） // [ 10 ] （heep 1004） became orphan, and could not be reclaim back --------------------------------------------------- delete p; // [ 10 ] （heep 1000）memory was reclaimed and free, // and available for others as [ ]（heep 1000）, // but p still hold the address delete q; // bad idea, [ 10 ]（heep 1000） already been reclaimed! q = NULL; // NULL is zero pointer, means the pointer does not hold any address, // used as sentinel value, sometimes better than delete. // Accessing "deleted" memory has unpredictable consequences --------------------------------------------------- // int *p declaration reserves only a single word, which is large enough to hold a machine address. // ≠ // int *p = NULL declare pointer p as nullptr --------------------------------------------------- (*newOne).name = name // "." &gt; "*" newOne-&gt;name = name Use of pointerBig program that contains a certain amout of classes and objects that are share some relationship. Instead of copying data from each other, using pointer to point to specific data is better: Saves space by not repeating the same information. If some objects gets new information to update, change in one place only! Dynamic allocation Request memoryTo acquire new memory when you need it and to free it explicitly when it is no longer needed. Acquiring new storage when the program is running. While the program is running, you can reserve part of the unallocated memory, leaving the rest for subsequent allocations.The pool of unallocated memory available to a program is called the heap.int *p = new int; //new operator to allocate memory from the heapIn its simplest form, the new operator takes a type and allocates space for a variable of that type located in the heap.The call to new operator will return the address of a storage location in the heap that has been set aside to hold an integer. Free occupied memoryDelete which takes a pointer previously allocated by new and returns the memory associated with that pointer to the heap. TreeTree terminology Node, tree, subtree, parent, child, root, edge, leaf For any node ni, the depth of ni is the length of the unique path from the root to ni. The height of ni is the length of the longest path from ni to a leaf Rules for all trees Recursive branching structure Single root node Every node reachable from root by unique path Binary treeEach node has at most 2 children. Binary search tree All nodes in left subtree are less than root, all nodes in right subtree are greater. Arranged for efficient search/insert. It is the basis for the implementation of two library collections classes, set and map. Most operations’ average running time is O(log N). Operating on trees Many tree algorithms are recursive Handle current node, recur on subtrees Base case is empty tree (NULL) Tree traversals to visit all nodes, order of traversal: Pre: cur, left, right In: left, cur, right Post: left, right, cur Others: level-by-level, reverse orders, etc Balanced Search TreesBinary search tree have poor worst-case performance.To make costs are guaranteed to be logarithmic, no matter what sequence of keys is used to construct them, the ideal is to keep binary search trees perfectly balanced. Unfortunately, maintaining perfect balance for dynamic insertions is too expensive. So consider data structure that slightly relaxes the perfect balance requirement to provide guaranteed logarithmic performance not just for the insert and search operations, but also for all of the ordered operations (except range search). AVL treeAdelson-Velskii and Landis tree is a binary search tree with a balance condition. Track balance factor for each node: Height of right subtree - height of left subtree information is kept for each node (in the node structure) For every node in the tree, the height of the left and right subtrees can differ by at most 1 (Balance factor = 0 or 1). When balance factor hits 2, restructure Rotation moves nodes from heavy to light side Local rearrangement around specific node When finished, node has 0 balance factor Single rotation: one time rotation between new insert node and its parent node Double rotation: two single rotation of the new insert node 2-3 treesAllow the nodes in the tree to hold more than one key: 3-nodes, which hold three links and two keys. Definition: A 2-3 search tree is a tree that is either empty or A 2-node, with one key (and associated value) and two links, a left link to a 2-3 search tree with smaller keys, and a right link to a 2-3 search tree with larger keys A 3-node, with two keys (and associated values) and three links, a left link to a 2-3 search tree with smaller keys, a middle link to a 2-3 search tree with keys between the node’s keys, and a right link to a 2-3 search tree with larger keys A perfectly balanced 2-3 search tree is one whose null links are all the same distance from the root. The concept guarantee that search and insert operations in a 2-3 tree with N keys are to visit at most lg N nodes. But its dicrect implementation is inconvenient: Not only is there a substantial amount of code involved, but the overhead incurred could make the algorithms slower than standard BST search and insert. Consider a simple representation known as a red-black BST that leads to a natural implementation. Priority QueuesA priority queue is a data structure that allows at least the following two operations: insert, and deleteMin, which finds, returns, and removes the minimum element in the priority queue. Binary HeapA heap is a binary tree that is completely filled, with the possible exception of the bottom level, which is filled from left to right. Such a tree is known as a complete binary tree. Structure A heap data structure consist of an array (of Comparable objects) and an integer representing the current heap size. For any element in array position i, the left child is in position 2i, the right child is in the cell after the left child [2i + 1], and the parent is in position [i/2]. Heap-Order Property For every node X, the key in the parent of X is smaller than (or equal to) the key in X. So to make find minimum operation quick. Basic Heap Operation insert: To insert an element X into the heap, create a hole in the next available location. Then Percolate up - swap X with its parent index (i/2) so long as X has a higher priority than its parent. Continue this process until X has no more lower priority parent. 1234567//Percolate upint hole = ++size; binaryQueue[0]=std::move(*newOne); for (;(priority&lt;binaryQueue[hole/2].priority || (priority==binaryQueue[hole/2].priority &amp;&amp; name&lt;binaryQueue[hole/2].name) );hole/=2) &#123; binaryQueue[hole] = std::move(binaryQueue[hole/2]); &#125; binaryQueue[hole] = std::move(binaryQueue[0]); deleteMin: When the minimum is removed, a hole is created at the root. Move the last element X in the heap to place in the root hole. Then Percolate down - swapp X with its more urgent-priority child [index (i2 or i2+1)] so long as it has a lower priority than its child. Repeat this step until X has no more higher priority child. 1234567891011//Percolate downint child; for (; hole*2&lt;=size;hole=child) &#123; child = hole*2; if ( child!=size &amp;&amp; (binaryQueue[child+1].priority&lt;binaryQueue[child].priority || (binaryQueue[child+1].priority==binaryQueue[child].priority &amp;&amp; binaryQueue[child+1].name&lt;binaryQueue[child].name)) ) ++child; if ( binaryQueue[child].priority&lt;priority_tobePerD || (binaryQueue[child].priority==priority_tobePerD &amp;&amp; binaryQueue[child].name&lt;name_tobePerD) ) &#123; binaryQueue[hole] = std::move(binaryQueue[child]); &#125; else break; &#125; Use integer division to avoid even odd index. Algorithm AnalysisSpace/time, big-O, scalability Big-O Computational complexity: The relationship between N and the performance of an algorithm as N becomes large Big-O notation: to denote the computational complexity of algorithms. Standard simplifications of big-O Eliminate any term whose contribution to the total ceases to be significant as N becomes large. Eliminate any constant factors. Worst-case versus average-case complexityAverage-case performance often reflects typical behavior, while worst-case performance represents a guarantee for performance on any possible input. Predicting computational complexity from code structure Constant time: Code whose execution time does not depend on the problem size is said to run in constant time, which is expressed in big-O notation as O(1). Linear time: function that are executed exactly n times, once for each cycle of the for loop, O(N) Quadratic time: Algorithms like selection sort that exhibit O(N2) performance are said to run in quadratic tim For many programs, you can determine the computational complexity simply by finding the piece of the code that is executed most often and determining how many times it runs as a function of N Space/time In general, the most important measure of performance is execution time. It also possible to apply complexity analysis to the amount of memory space required. Nowadays the memory is cheap, but it still matters when designing extreamly big programs, or APPs on small memory device, such as phones and wearable devices. SortingThere are lots of different sorting algoritms, from the simple to very complex. Some optimized for certain situations (lots of duplicates, almost sorted, etc.). So why do we need multiple algorithms? Selection sort Select smallest and swap to front/backend 12345678910void SelectionSort(Vector&lt;int&gt; &amp;arr)&#123; for (int i = 0; i &lt; arr.size()-1; i++) &#123; int minIndex = i; for (int j = i+1; j &lt; arr.size(); j++) &#123; if (arr[j] &lt; arr[minIndex]) minIndex = j; &#125; Swap(arr[i], arr[minIndex]); &#125; Selection sort analysisCount work inside loops: First iteration does N-1 compares, second does N-2, and so on. One swap per iteration O(N2) Insertion sort As sorting hand of just-dealt cards, each subsequent element inserted into proper place Start with first element (already sorted) Insert next element relative to first Repeat for third, fourth, etc. Slide elements over to make space during insert123456789void InsertionSort(Vector&lt;int&gt; &amp;v)&#123; for (int i = 1; i &lt; v.size(); i++) &#123; int cur = v[i]; // slide cur down into position to left for (int j=i-1; j &gt;= 0 &amp;&amp; v[j] &gt; cur; j--) v[j+1] = v[j]; v[j+1] = cur; &#125;&#125; Insertion sort analysisBecause of the nested loops, each of which can take N iterations, insertion sort is O(N2). HeapsortPriority queues can be used to sort in O(N log N) time. The algorithm based on this idea is known as heapsort. Heapsort analysisThe building of the heap, uses less than 2N comparisons. In the second phase, the ith deleteMax uses at most less than 2*log (N − i + 1) comparisons, for a total of at most 2N log N − O(N) comparisons (assuming N ≥ 2). Consequently, in the worst case, at most 2N log N − O(N) comparisons are used by heapsort. Merge sort Inspiration: Algorithm like selection sort is quadratic growth (O(N2)). Double input -&gt; 4X time, halve input -&gt; 1/4 time.Can recursion save the day? If there are two sorted halves, how to produce sorted full result? Divide and conquer algorithm Divide input in half Recursively sort each half Merge two halves together “Easy-split hard-join” No complex decision about which goes where, just divide in middle Merge step preserves ordering from each half Merge depends on the fact that the first element in the complete ordering must be either the first element in v1 or the first element in v2, whichever is smaller. 12345678910111213141516171819202122232425void MergeSort(Vector&lt;int&gt; &amp;v)&#123; if (v.size() &gt; 1) &#123; int n1 = v.size()/2; int n2 = v.size() - n1; Vector&lt;int&gt; left = Copy(v, 0, n1); Vector&lt;int&gt; right = Copy(v, n1, n2); MergeSort(left); MergeSort(right); v.clear(); Merge(v, left, right); &#125;&#125;void Merge(Vector&lt;int&gt; &amp;v,Vector&lt;int&gt; &amp;left,Vector&lt;int&gt; &amp;right) &#123; int l=0, r=0; while(l&lt;left.size() &amp;&amp; r&lt;right.size()) &#123; if (left[l]&lt;right[r]) v.add(left[l++]); else v.add(right[r++]); &#125; while(l&lt;left.size()) v.add(left[l++]); while(r&lt;right.size()) v.add(right[r++]);&#125; Mergesort analysisThe time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge, which is linear. T(N) = N + 2T(N/2). log N levels * N per level= O(NlogN). Mergesort uses the lowest number of comparisons of all the popular sorting algorithms.Theoretical result show that no general sort algorithm could be better than NlogN.But there is still better in practice: The running time of mergesort, when compared with other O(N log N) alternatives, depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array). These costs are language dependent. In Java, when performing a generic sort (using a Comparator), an element comparison can be expensive, but moving elements is cheap (because they are reference assignments, rather than copies of large objects). In C++, in a generic sort, copying objects can be expensive if the objects are large, while comparing objects often is relatively cheap because of the ability of the compiler to aggressively perform inline optimization. QuicksortMost sorting programs in use today are based on an algorithm called Quicksort, which employs a Divide and conquer strategy as merge sort, but instead take a different approach to divide up input vector into low half and high half. Quicksort uses a few more comparisons, in exchange for significantly fewer data movements. The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efficiently. “Hard-split easy-join”, Each element examined and placed in correct half, so that join step become trivial. Choose an element (pivot) to serve as the boundary between the small and large elements. Partitioning: Rearrange the elements in the vector so that all elements to the left of the boundary are less than the pivot and all elements to the right are greater than or possibly equal to the pivot. Sort the elements in each of the partial vectors.12345678void Quicksort(Vector&lt;int&gt; &amp;v, int start, int stop)&#123; if (stop &gt; start) &#123; int pivot = Partition(v, start, stop); Quicksort(v, start, pivot-1); Quicksort(v, pivot+1, stop); &#125;&#125; Quicksort performance analysisThe running time of quicksort is equal to the running time of the two recursive calls plus the linear time spent in the partition (the pivot selection takes only constant time). T(N) = T(i) + T(N − i − 1) + cN, where i = |S1| is the number of elements in S1.There are thre cases Ideal 50/50 split: The pivot is in the middle, T(N) = cN + 2T(N/2) =&gt; O(NlogN) Average bad 90/10 split: N per level, but more levels, solve N*(9/10)k = 1, still k = O(NlogN) Worst N-1/1 split: The pivot is the smallest element, all the time. Then i = 0, T(N) = T(N − 1) + cN, N &gt; 1. With N levels! O(N2) In a vector with randomly chosen elements, Quicksort tends to perform well, with an average-case complexity of O(N log N). In the worst case — which paradoxically consists of a vector that is already sorted — the performance degenerates to O(N2). Despite this inferior behavior in the worst case, Quicksort is so much faster in practice than most other algorithms that it has become the standard. Design StrategyWhen an algorithm is given, the actual data structures need not be specified. It is up to the programmer to choose the appropriate data structure in order to make the running time as small as possible. There are many to be considered: algorithms, data structure, space-time tradeoff, code complexity. Dynamic ProgrammingTo solve optimization problems in which we make a set of choices in order to arrive at an optimal solution. As we make each choice, subproblems of the same form often arise. Dynamic programming is effective when a given subproblem may arise from more than one partial set of choices; the key technique is to store the solution to each such subproblem in case it should reappear. Unlike divide-and-conquer algorithms which partition the problem into disjoint subproblems, dynamic programming applies when the subproblems overlap. “Programming” in this context refers to a tabular method. When should look for a dynamic-programming solution to a problem? Optimal substructure: a problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. Overlapping subproblems: When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problemhas overlapping subproblems. In contrast, a problem for which a divide-andconquer approach is suitable usually generates brand-new problems at each step of the recursion. General setps of Dynamic Programming Characterize the structure of an optimal solution. Recursively define the value of an optimal solution. Compute the value of an optimal solution, typically in a bottom-up fashion. Construct an optimal solution from computed information. Greedy AlgorithmsGreedy algorithms work in phases. In each phase, a decision is made in a locally optimal manner, without regard for future consequences. When the algorithm terminates, we hope that the local optimum is equal to the global optimum. If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution. Huffman Codes A Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations. We merely select the two smallest trees. If we maintain the trees in a priority queue, ordered by weight, then the running time is O(C logC), since there will be one buildHeap, 2C − 2 deleteMins, and C − 2 inserts. A simple implementation of the priority queue, using a list, would give an O(C2) algorithm. The choice of priority queue implementation depends on how large C is. In the typical case of an ASCII character set, C is small enough that the quadratic running time is acceptable. Divide and ConquerTraditionally, routines in which the text contains at least two recursive calls and subproblems be disjoint (that is, essentially nonoverlapping) are called divide-and-conquer algorithms. Divide: Smaller problems are solved recursively (except, of course, base cases). Conquer: The solution to the original problem is then formed from the solutions to the subproblems.We have already seen several divide-and-conquer algorithms: mergesort and quicksort, which have O(N log N) worst-case and averagecase bounds, respectively. Backtracking AlgorithmsSee Recursive BacktrackingIn some cases, the savings over a brute-force exhaustive search can be significant.The elimination of a large group of possibilities in one step is known as pruning. How to evaluate/compare alternatives Often interested in execution performance: Time spent and memory used Should also consider ease of developing, verifying, maintaining codeQuicksort strategy Picking the pivotPicking a good pivot improves performance, but also costs some time. If the algorithm spends more time choosing the pivot than it gets back from making a good choice, you will end up slowing down the implementation rather than speeding it up. The popular, uninformed choice is to use the first element as the pivot. This is acceptable if the input is random, but if the input is presorted or in reverse order, then the pivot provides a poor partition. A safe approach is to choose the pivot element randomly. On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all. A good estimate can be obtained by picking three elements randomly and using the median of these three as pivot. The randomness turns out not to help much, so the common course is to use as pivot the median of the left, right, and center elements. Quicksort partitioning strategyA known method that is very easy to do it wrong or inefficiently. General process: The first step is to get the pivot element out of the way by swapping it with the last element. Two pointers, i point to the first element and j to the next-to-last element. What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part. “Small” and “large” are relative to the pivot. While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot. We move j left, skipping over elements that are larger than the pivot. When i and j have stopped, i is pointing at a large element and j is pointing at a small element. If i is to the left of j (not yet cross), those elements are swapped. Repeat the process until i and j cross The final is to swap the pivot element with present i element One important detail we must consider is how to handle elements that are equal to the pivot? Suppose there are 10,000,000 elements, of which 500,000 are identical (or, more likely, complex elements whose sort keys are identical). To get an idea of what might be good, we consider the case where all the elements in the array are identical. If neither i nor j stops, and code is present to prevent them from running off the end of the array, no swaps will be performed. Although this seems good, a correct implementation would then swap the pivot into the last spot that i touched, which would be the next-to last position (or last, depending on the exact implementation). This would create very uneven subarrays. If all the elements are identical, the running time is O(N2). If both i and j stop, there will be many swaps between identical elements. The partition creates two nearly equal subarrays. The total running time would then be O(N log N). Thus it is better to do the unnecessary swaps and create even subarrays than to risk wildly uneven subarrays. Small arrays For very small arrays (N ≤ 20), quicksort does not perform as well as insertion sort. Furthermore, because quicksort is recursive, these cases will occur frequently. A common solution is not to use quicksort recursively for small arrays, but instead use a sorting algorithm that is efficient for small arrays, such as insertion sort. A good cutoff range is N = 10, although any cutoff between 5 and 20 is likely to produce similar results. This also saves nasty degenerate cases, such as taking the median of three elements when there are only one or two. Text editor case study Buffer requirements Sequence of characters + cursor position Operations to match commands above What to consider? Implementation choices performance implications Buffer class interface 1234567891011121314class Buffer &#123; public: Buffer(); ~Buffer(); void moveCursorForward(); void moveCursorBackward(); void moveCursorToStart(); void moveCursorToEnd(); void insertCharacter(char ch); void deleteCharacter(); void display(); private: // TBD!&#125;; Buffer layered on Vector Need character data + cursor Chars in Vector&lt;char&gt; Represent cursor as integer index Minor detail – is index before/after cursor? Buffer contains: AB|CDE 1234// for Buffer classprivate: Vector&lt;char&gt; chars;int cursor; Performance insertCharacter() and deleteCharacter() is linear, other operation is just O(1) Space used ~1 byte per char Buffer layered on Stack Inspiration: add/remove at end of vector is fast If chars next to cursor were at end… Build on top of stack? Another layered abstraction! How is cursor represented? Buffer contains:AB|CDEThere is no explicit cursor representation, instead using two stack to represent a whole data structure being seperated by the implicit cursor. 123// for Buffer classprivate: Stack&lt;char&gt; before, after; Performance moveCursorToStart(), moveCursorToEnd() operation is linear, other operation is just O(1) Space used ~2 byte per char Buffer as double linked list Inspiration: contiguous memory is constraining Connect chars without locality Add tail pointer to get direct access to last cell Add prev link to speed up moving backwards Buffer contains:AB|CDE 1234567// for Buffer classprivate: struct cellT &#123; char ch; cellT *prev, *next; &#125;; cellT *head, *tail, *cursor; Cursor design To cell before or after? 5 letters, 6 cursor positions… Add “dummy cell” to front of list Performance destruction is linear, other operation is just O(1) Space used ~9 byte per char Compare implementations table th:nth-of-type(1) { width: 200px; } table th:nth-of-type(2) { width: 80px; } table th:nth-of-type(3) { width: 80px; } Operation Vector Stack Single linked list Double linked list Buffer() O(1) O(1) O(1) O(1) ~Buffer() O(1) O(1) O(N) O(N) moveCursorForward() O(1) O(1) O(1) O(1) moveCursorBackward() O(1) O(1) O(N) O(1) moveCursorToStart() O(1) O(N) O(1) O(1) moveCursorToEnd() O(1) O(N) O(N) O(1) insertCharacter() O(N) O(1) O(1) O(1) deleteCharacter() O(N) O(1) O(1) O(1) Space used 1N 2N 5N 9N Space-time tradeoff Doubly-linked list is O(1) on all six operations But, each char uses 1 byte + 8 bytes of pointers =&gt; 89% overhead! Compromise: chunklist Array and linked list hybrid Shares overhead cost among several chars Chunksize can be tuned as appropriate Cost shows up in code complexity Cursor must traverse both within and across chunks Splitting/merging chunks on insert/deletes Implementing MapMap is super-useful, support any kind of dictionary, lookup table, index, database, etc.Map stores key-value pairs, support fast access via key, operations to optimize: add, getValueHow to make it work efficiently? Implement Map as Vector Layer on Vector, provides convenience with low overhead Define pair struct, to olds key and value together, Vector&lt;pair&gt; Vector sorted or unsorted? If sorted, sorted by what? Sorting: Provides fast lookup, but still slow to insert (because of shuffling) How to implement getValue, add? Does a linked list help? Easy to insert, once at a position But hard to find position to insert… Implementing Map as tree Implementatation Each Map entry adds node to tree, node contains: string key, client-type value, pointers to left/right subtrees Tree organized for binary search, Key is used as search field getValue: Searches tree, comparing keys, find existing match or error add: Searches tree, comparing keys, overwrites existing or adds new node Private members for Map 1234567891011121314151617template &lt;typename ValType&gt; class Map &#123; public: // as before private: struct node &#123; string key; ValType value; node *left, *right; &#125;; node *root; node *treeSearch(node * t, string key); void treeEnter(node *&amp;t, string key, ValType val); DISALLOW_COPYING(Map)&#125;; Evaluate Map as tree Space used: Overhead of two pointers per entry (typically 8 bytes total) Runtime performance: Add/getValue take time proportional to tree height(expected to be O(logN)) Degenerate trees The insert order is “sorted”: 2 8 14 15 18 20 21, totally unbalanced with height = 7 The insert order is “alternately sorted”: 21 2 20 8 14 15 18 or 2 8 21 20 18 14 15 Association: What is the relationship between worst-case inputs for tree insertion and Quicksort? What to do about it: AVL tree Compare Map implementations Operation Vector BST Sorted Vector getValue O(N) O(lgN) O(lgN) add O(N) O(lgN) O(N) Space used N 9N N Hashing Hash table ADT Hash table data structure: A list of keys and TableSize Hash function: A mapping that map each key into some number in the range 0 to TableSize-1 and distributes the keys evenly among the appropriate cell HashingThe major problems are choosing a function, deciding what to do when two keys hash to the same value (this is known as acollision), and deciding on the table size RehashingIf the table gets too full, the running time for the operations will start taking too long, and insertions might fail for open addressing hashing with quadratic resolution. A solution is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table. The Big-FiveIn C++11, classes come with five special functions that are already written for you. These are the destructor, copy constructor, move constructor, copy assignment operator, and move assignment operator. Collectively these are the big-five. DestructorThe destructor is called whenever an object goes out of scope or is subjected to a delete. Typically, the only responsibility of the destructor is to free up any resources that were acquired during the use of the object. This includes calling delete for any corresponding news, closing any files that were opened, and so on. The default simply applies the destructor on each data member. ConstructorA constructor is a method that describes how an instance of the class is constructed. If no constructor is explicitly defined, one that initializes the data members using language defaults is automatically generated. Copy Constructor and Move Constructor Copy Assignment and Move Assignment (operator=)By Defaults, if a class consists of data members that are exclusively primitive types and objects for which the defaults make sense, the class defaults will usually make sense.The main problem occurs in a class that contains a data member that is a pointer. The default destructor does nothing to data members that are pointers (for good reason—recall that we must delete ourselves). Furthermore, the copy constructor and copy assignment operator both copy the value of the pointer rather than the objects being pointed at. Thus, we will have two class instances that contain pointers that point to the same object. This is a so-called shallow copy (contrast to deep copy). To avoid shallow copy, ban the copy funtionality by calling DISALLOW_COPYING(ClassType). As a result, when a class contains pointers as data members, and deep semantics are important, we typically must implement the destructor, copy assignment, and copy constructors ourselves. Explicit constructor:All one-parameter constructors should be made explicit to avoid behind-the-scenes type conversions. Otherwise, there are somewhat lenient rules that will allow type conversions without explicit casting operations. Usually, this is unwanted behavior that destroys strong typing and can lead to hard-to-find bugs.The use of explicit means that a one-parameter constructor cannot be used to generate an implicit temporary 1234567891011class IntCell &#123;public: explicit IntCell( int initialValue = 0 ) : storedValue&#123; initialValue &#125; &#123; &#125; int read( ) const &#123; return storedValue; &#125;private: int storedValue; &#125;;IntCell obj; // obj is an IntCellobj = 37; // Should not compile: type mismatch Since IntCell constructor is declared explicit, the compiler will correctly complain that there is a type mismatch TemplateType-independentWhen we write C++ code for a type-independent algorithm or data structure, we would prefer to write the code once rather than recode it for each different type Function template A function template is not an actual function, but instead is a pattern for what could become a function. An expansion for each new type generates additional code; this is known as code bloat when it occurs in large projects.Class template12345678template &lt;typename Object&gt;class MemoryCell &#123; public: explicit MemoryCell( const Object &amp; initialValue = Object&#123; &#125; ) : storedValue&#123; initialValue &#125; &#123; &#125; private: Object storedValue;&#125;; MemoryCell is not a class, it is only a class template. It will be a class if specify the Object type. MemoryCell&lt;int&gt; and MemoryCell&lt;string&gt; are the actual classes. Graph AlgorithmsDefinitions: vertices, edges, arcs, directed arcs = digraphs, weight/cost, path, length, acyclic(no cycles) Topological Sort A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from vi to vj, then vj appears after vi in the ordering. A topological ordering is not possible if the graph has a cycle To find a topological ordering, define the indegree of a vertex v as the number of edges (u, v), then use a queue or stack to keep the present 0 indegree vertexes. At each stage, as long as the queue is not empty, dequeue a 0 indegree vertexes in the queue, enqueue each new generated 0 indegree vertexes into the queue. Sortest-Path Algorithms Breadth-first search Explores equally in all directions To find unweighted shortest paths Operates by processing vertices in layers: The vertices closest to the start are evaluated first, and the most distant vertices are evaluated last. Dijkstra’s Algorithm Also called Uniform Cost Search, cost matters Instead of exploring all possible paths equally, it favors lower cost paths. Dijkstra’s algorithm proceeds in stages. At each stage, while there are still vertices waiting to be known: Selects a vertex v, which has the smallest dv among all the unknown vertices, and declares v as known stage. For each of v’s neighbors, w, if the new path’s cost from v to w is better than previous dw, dw will be updated. But w will not be marked as known, unless at next while-loop stage, dw happens to be the smalles. The above steps could be implemented via a priority queue. A proof by contradiction will show that this algorithm always works as long as no edge has a negative cost. If the graph is sparse, with |E| =θ(|V|), this algorithm is too slow. In this case, the distances would need to be kept in a priority queue. Selection of the vertex v is a deleteMin operation. The update of w’s distance can be implemented two ways. One way treats the update as a decreaseKey operation. An alternate method is to insert w and the new value dw into the priority queue every time w’s distance changes. Greedy Best First Search(Heuristic search) With Breadth First Search and Dijkstra’s Algorithm, the frontier expands in all directions. This is a reasonable choice if you’re trying to find a path to all locations or to many locations. However, a common case is to find a path to only one location. A modification of Dijkstra’s Algorithm, optimized for a single destination. It prioritizes paths that seem to be leading closer to the goal. To make the frontier expand towards the goal more than it expands in other directions. First, define a heuristic function that tells us how close we are to the goal, design a heuristic for each type of graph 123def heuristic(a, b): # Manhattan distance on a square grid return abs(a.x - b.x) + abs(a.y - b.y) Use the estimated distance to the goal for the priority queue ordering. The location closest to the goal will be explored first. This algorithm runs faster when there aren’t a lot of obstacles, but the paths aren’t as good(not always the shortest). A* Algorithm Dijkstra’s Algorithm works well to find the shortest path, but it wastes time exploring in directions that aren’t promising. Greedy Best First Search explores in promising directions but it may not find the shortest path. The A* algorithm uses both the actual distance from the start and the estimated distance to the goal. Compare the algorithms: Dijkstra’s Algorithm calculates the distance from the start point. Greedy Best-First Search estimates the distance to the goal point. A* is using the sum of those two distances. So A* is the best of both worlds. As long as the heuristic does not overestimate distances, A* does not use the heuristic to come up with an approximate answer. It finds an optimal path, like Dijkstra’s Algorithm does. A* uses the heuristic to reorder the nodes so that it’s more likely that the goal node will be encountered sooner. Conclusion: Which algorithm should you use for finding paths on a map? If you want to find paths from or to all all locations, use Breadth First Search or Dijkstra’s Algorithm. Use Breadth First Search if movement costs are all the same; use Dijkstra’s Algorithm if movement costs vary. If you want to find paths to one location, use Greedy Best First Search or A*. Prefer A in most cases. When you’re tempted to use Greedy Best First Search, consider using A with an “inadmissible” heuristic. If you want the optimal paths, Breadth First Search and Dijkstra’s Algorithm are guaranteed to find the shortest path given the input graph. Greedy Best First Search is not. A* is guaranteed to find the shortest path if the heuristic is never larger than the true distance. (As the heuristic becomes smaller, A turns into Dijkstra’s Algorithm. As the heuristic becomes larger, A turns into Greedy Best First Search.) Advanced Data StructuresRed-Black TreesRed-black tree leads to a natural implementation of the insertion algorithm for 2-3 trees RBT definition Red-black tree means encoding 2-3 trees in this way: red links, which bind together two 2-nodes to represent 3-nodes, and black links, which bind together the 2-3 tree. An equivalent definition is to define red-black BSTs as BSTs having red and black links and satisfying the following three restrictions: Red links lean left. No node has two red links connected to it. The tree has perfect black balance : every path from the root to a null link has the same number of black links. A 1-1 correspondence: If we draw the red links horizontally in a red-black BST, all of the null links are the same distance from the root, and if we then collapse together the nodes connected by red links, the result is a 2-3 tree. RBT implementaion Color representation: Each node is pointed to by precisely one link from its parent, Encode the color of links in nodes, by adding a boolean instance variable color to our Node data type, which is true if the link from the parent is red and false if it is black. By convention, null links are black. For clarity, define constants RED and BLACK for use in setting and testing this variable. Rotation To correct right-leaning red links or two red links in a row conditions. takes a link to a red-black BST as argument and, assuming that link to be to a Node h whose right link is red, makes the necessary adjustments and returns a link to a node that is the root of a red-black BST for the same set of keys whose left link is red. Actually it is switching from having the smaller of the two keys at the root to having the larger of the two keys at the root. Flipping colors to split a 4-node In addition to flipping the colors of the children from red to black, we also flip the color of the parent from black to red. Keeping the root black. Insertion Maintain the 1-1 correspondence between 2-3 trees and red-black BSTs during insertion by judicious use of three simple operations: left rotate, right rotate, and color flip. If the right child is red and the left child is black, rotate left. If both the left child and its left child are red, rotate right. If both children are red, flip colors. Deletion Assignments Name Hash Game of Life Serafini Recursion Boggle! Patient Queue Huffman Encoding Trailblazer]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs106b</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 09 双向链表 Doubly Linked List - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-09-java-doubly-linked-list%2F</url>
    <content type="text"><![CDATA[双向链表 Doubly Linked List前面介绍过单向链表，不过单向链表有几个缺点. 第一个就是它的addLast操作非常慢。单向链表只有一个变量保存列表头的地址, 以及每个节点对后面节点的单向引用(链接). 对于很长的列表，addLast方法必须遍历整个列表, 一直到找到列表末尾才能执行插入操作. 那么如何解决呢?最直观的解决方案就是加个’车尾’, 如图 这样我们就可以直接通过last.next引用末尾位置.不过另一个问题并没有解决, 就是删除列表最后一项removeLast这个操作还是很慢。因为在目前的结构设计下, 我们需要先找到倒数第二项，然后将其下一个指针设置为null。而要找到倒数第二节点, 我们就得先找到倒数第三个节点…… 以此类推。也就是说，对于删除末尾的操作，还是要几乎遍历整个列表。 反方向的链接基于前面单向链表构建双向链表, 一个比较有效的方法是额外为每个节点添加一个指向前面节点的链接/指针.12345public class OneNode &#123; public OneNode prev; //指向前 public int item; public OneNode next; //指向后&#125; 增加这些额外的指针会导致额外的代码复杂度, 以及额外的内存开销, 这就是追求时间效率的代价. Sentinel 与尾节点双向链表的一个设计初衷，就是为了解决单向链表针对列表末尾位置的操作效率不高的问题，除了sentinel和反方向的链接还不够，我们还需要一个节点（指针）能够直接帮我们定位到列表末端。可以考虑添加一个的尾节点last， 这样的列表就可以支持O(1)复杂度的addLast,getLast 和 removeLast操作了。 循环双端链表上面的尾节点设计虽然没什么错误，但有点瑕疵：最后一个尾节点指针有时指向前哨节点，有时指向一个真正的节点。更好的方法是使双向链表首尾相连, 构成一个循环，即前后节点共享唯一的一个前哨节点。 这样的设计相对更整洁，更美观(主观上的), sentinel的prev就指向列表最后一个节点, sentinel的next指向列表第一个节点.12345678910111213public class LinkedListDeque&lt;GType&gt; &#123; private class OneNode &#123; public OneNode prev; //sentinel's forward link always points to the last element public GType item; public OneNode next; //sentinel's backward link always points to the first element public OneNode(OneNode p, GType i, OneNode n) &#123; prev = p; item = i; next = n; &#125; &#125;&#125; 然后修改构造函数:123456789101112131415/** Creates an empty deque. */public LinkedListDeque()&#123; sentinel = new OneNode(null,null, null); sentinel.prev = sentinel; sentinel.next = sentinel; size = 0;&#125;/** Creates a deque with x */public LinkedListDeque(GType x)&#123; sentinel = new OneNode(null, null, null); sentinel.next = new OneNode(sentinel, x,sentinel); sentinel.prev = sentinel.next; size = 1;&#125; 如果是初始化空列表, 那么其实就是一个自己指向自己的sentinel节点. 如果是非空列表, 那么sentinel节点和真实的节点就构成了一个最简单的二元循环体. 针对列表末尾位置的操作双端链表结构优雅，虽然某些操作如addFirst等编码复杂度会提高, 但不影响速度. 更重要的是, 相比单向链表, 它反而使得addLast, moveLast等方法的代码实现变得简单了, 而且还进一步提升了运行速度(O(n)到O(c)).1234567891011121314151617181920212223/** Adds an item to the back of the Deque. * O(c) */public void addLast(GType x)&#123; OneNode oldBackNode = sentinel.prev; OneNode newNode = new OneNode(oldBackNode, x, sentinel); sentinel.prev = newNode; oldBackNode.next = newNode; size += 1;&#125;/** Removes and returns the item at the front of the Deque. * If no such item exists, returns null.O(c). */public GType removeFirst()&#123; if (isEmpty())&#123; return null; &#125; OneNode oldFrontNode = sentinel.next; sentinel.next = oldFrontNode.next; oldFrontNode.next.prev = sentinel; size -= 1; return oldFrontNode.item;&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 08 单向链表 Singly Linked List - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-08-java-singly-linked-list%2F</url>
    <content type="text"><![CDATA[链表 Linked List前面有介绍以array为基础搭建的列表，支持自动扩容, 各种插入，删除速度都很快.这里再介绍另一种方案, 链表, 也可以实现列表自动扩容. 带链接的节点链表的核心组成是带链接的节点, 每个节点就像火车车厢, 有钩子连接下一节车厢.以int节点为例:123456789public class IntNode &#123; public int item; public IntNode next; public IntNode(int i, IntNode n) &#123; item = i; next = n; &#125;&#125; next就是这个链接, 每一个节点就是其上一个节点的next. 嵌套类 Nested static class这个节点作为一个相对独立的数据结构, 我们更希望让他单独作为一个类来维护. 再另外创建一个名为LinkedList的class与用户进行交互. 这样还有另一个好处就是提供一个命名为LinkedList的类给用户交互，用户更直观地知道自己是在调用链表。如果直接与node类交互，用户可能会困扰. 但同时考虑到这个node类只有LinkedList会调用，所以我们可以把node类嵌套进LinkedList中，也就是嵌套类，在类中定义类。1234567891011121314151617181920public class LinkedList&lt;XXX&gt; &#123; private class OneNode &#123; public XXX item; public OneNode next; public OneNode(XXX i, OneNode n) &#123; item = i; next = n; &#125; &#125; private OneNode first; private int size; public LinkedList(XXX x) &#123; first = new OneNode(x, null); size = 1; &#125; //下面是各种方法...&#125; 以上定义使用了泛型。声明OneNode实例first为私有变量, 是为了防止用户错误地摆弄链接指向，private和public的使用参考. 静态与非静态嵌套类123456789class OuterClass &#123; ... static class StaticNestedClass &#123; ... &#125; class InnerClass &#123; ... &#125;&#125; 如果嵌套类不需要使用LinkedList的任何实例方法或变量，那可以声明嵌套类为static。像静态类方法一样，静态嵌套类不能直接引用其外部类中定义的实例变量或方法, 只能通过实例对象引用来使用它们。同时外部类不能直接访问静态嵌套类的成员变量，但可以通过静态嵌套类来访问。 非静态嵌套类一般叫做内部类inner class。与实例方法和变量一样，内部类与其外部类的实例关联，并且可以直接访问该对象的方法和变量。另外，因为内部类与一个实例相关联，所以它不能自己定义任何静态成员。一个内部类的实例作为成员存在于其外部类的实例中, InnerClass的一个实例只能存在于OuterClass的一个实例中，并且可以直接访问它的外部实例的方法和变量。 作为OuterClass的成员，嵌套类可以声明为private，public，protected或package private。外部类只能声明为public或package private。更多详情参考官网. 补充必要的实例方法插入的操作核心是改变链接指向， 比如原来是A-&gt;B-&gt;D, 要插入C, 则把C.next指向D,然后把B.next改为指向C, 变为A-&gt;B-&gt;C-&gt;D123456789101112131415161718192021222324252627282930313233/** 在列表开头插入 x. */public void addFirst(XXX x) &#123; first = new OneNode(x, first); size += 1;&#125;/** 返回列表第一个元素. */public XXX getFirst() &#123; return first.item;&#125;/** 在列表末尾插入 x. */public void addLast(XXX x) &#123; size += 1; OneNode p = first; /* 把 p 当做指针顺藤摸瓜一直挪到列表末尾. */ while (p.next != null) &#123; p = p.next; &#125; p.next = new OneNode(x, null);&#125;/** 删除列表末尾的元素. */public void removeLast()&#123; //自行补充...&#125;public int size() &#123; return size;&#125; 可以看到，如果用户不小心把某节点x指回自己x.next=x,那就会进入死循环，所以我们需要把OnoNode实例first声明为私有变量已提供必要的保护。 超载 overloading如果想初始化一个空列表, 可以:12345/** 构造一个空列表. */public LinkedList() &#123; fist = null; size = 0;&#125; 即使原来已经有一个带参数x的构造器了, 这里再加一个同名构造器也没问题. 因为Java允许有不同参数的方法重名, 叫超载 overloading. 程序不变条件 invariants上面超载了一个初始化空列表的构造器,加入初始化一个空列表，然后直接调用addLast，程序会报错, 因为null没有next. 有几种修改方法, 比如用if else这种加特例的方法. 这个方案虽然可以能解决问题，但是必要时应该避免加入特例代码, 毕竟有特例就意味着增加了复杂性和额外的代码特例记忆需求, 而人记忆是有限的. 一个更简洁（尽管不太显而易见）的解决方案是修改数据结构本身，让所有LinkedList，维护起来都没有差别，即使是空的。如果把列表比做拉货的火车，那么货物就是列表承载的数据。一列火车如果只有车厢而没有车头（或者车尾）的话是没有意义的，因为没有动力。所以不管火车有没有拉货，有车厢还是没车厢，要称之为火车我们至少需要一个火车头。我们可以通过创建一个特殊节点, 称为前哨节点 sentinel。前哨节点将保存一个值，具体数值我们不关心，它只是作为火车头，不装货。所以我们要修改LinkedList为：12345678910111213141516171819202122/* 第一个元素 （假如有的话）就是 sentinel.next. */public class LinkedList&lt;XXX&gt; &#123; private class OneNode &#123; //... &#125; private OneNode sentinel; private int size; /** 构造一个空列表. */ public LinkedList() &#123; sentinel = new OneNode(null, null); size = 0; &#125; /** 构造一个初始元素为x的列表. */ public LinkedList(XXX x) &#123; sentinel = new OneNode(null, null); sentinel.next = new OneNode(x, null); size = 1; &#125;&#125; 对于像LinkedList这样简单的数据结构来说，特例不多我们也许可以hold住, 一旦后续遇到像树tree等更复杂的数据结构，控制特例数量就显得极为重要了。所以现在就要培养自己的这方面的习惯，保持程序不变条件成立 Invariants。所谓 invariants 就是指数据结构任何情况下都是不会出错（除非程序有bug）. 具有前哨节点的LinkedList至少具有以下 invariants： 列表默认存在前哨节点。 列表第一个元素（如果非空的话）总是在sentinel.next.item。 size变量始终是已添加的元素总数。 不变条件使得代码的推敲变得更加容易，同时给程序员提供了能够确保代码正常工作的具体目标。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Bash 直接启动 sublime 或 atom 等编辑器以打开或新建文件]]></title>
    <url>%2FLaunch-editor-in-Gitbash%2F</url>
    <content type="text"><![CDATA[程序员或者其他需要码字多的人，经常要使用编辑器如sublime、atom 和 Typora等。如果每次都要用鼠标点击才能用sublime打开文件，或者在编辑器中新建文件，那么就会有点麻烦！但你可以用一句命令解决！ 配置在Git Bash中用各种文本编辑器打开文件或者直接新建文件。这里以atom为例。 常规步骤 打开Git Bash并cd到你的目标文件夹, 或者直接在目标文件中右键打开Git Bash. atom xxx.md 就会在弹出的atom窗口中打开名为xxx.md的markdown文件, 如果没有这个文件, 会自动创建一个. 适用于其他类型文件, 如.java等. 如果想用sublime, 可以用subl xxx.java, 同理notepad++ 可以用 notepad++ xxx.java等。 (若出现错误,看下面) 若系统无法识别命令一般使用sublime或者notepad++的用户, 可能会出现error: 系统无法识别命令...之类的, 可以这么解决: 方法1新建一个文件命名为subl（注意不能有后缀名），内容：12#!/bin/sh&quot;D:\Sublime Text 3\sublime_text.exe&quot; $1 &amp; 第一行指明这是个 shell 脚本.第二行的字符串是sublime的安装目录, 示例只是我电脑的目录, 注意这里要改为你自己的目录,第二行的$1 是取的命令之后输入的参数第二行的&amp;是此命令在后台打开，这样sublime打开之后，就不会阻塞你的git bash 文件保存到 C:\Program Files (x86)\Git\mingW32\bin 目录下(你的git目录可能与我的不一样，注意改成你自己的) 同理适用于其他编辑器，比如用chrome打开.html文件等。如果不想每次都新建一个文件，可以用下面的方法2。 方法2 找到 C:\Users\你的计算机名目录，如果你的计算机名是Administrator，那么你就要去C:\Users\Administrator目录下, 这里一般存放着windows系统的我的文档, 桌面等文件夹. 在该目录下用Git Bash输入notepad .bashrc, 这会用windows记事本新建并打开一个文件.bashrc，这个文件没有名称只有后缀名。.bashrc里面可以给Git Bash设置命令的别名, 设置路径等。 在.bashrc文件加入下面一行文本alias notepad++=&quot;/D/Notepad++/notepad++.exe&quot;, 这里你需要修改为你电脑的安装路径。alias就是别名的意思，当我们执行notepad++的时候，实际执行的是=后面的语句. 重新打开Git Bash, 设置才能生效，如果不想关掉在打开的话，可以直接在bash下输入source ~/.bashrc就可以立刻加载修改后的设置，设置立即生效。现在在bash下输入notepad++ test.py, 就直接打开了notepad++并创建了这个叫test的Python文件。这里的别名不一定非要取notepad++，随你想叫什么都行。 同理也可以扩展到别的文本编辑器，alias atom=&quot;atom的路径&quot;, alias sublime=&quot;sublime的路径&quot;等. 最后还要注意一点，上面所说的路径最好不要有空格，括号等，否则会造成命令无效. .bashrc还有很多有用的配置,可以根据需要进行扩展. 比如很多程序猿会选择修改删除命令rm(此命令不加任何参数的话，会直接删除文件, 可能会造成误删的后果)。这个时候可以给rm加个参数-i，意为在删除的时候给出提示。在文件.bashrc里添加这行代码alias rm=&quot;rm -i&quot;。但这里不建议这么做，因为rm=&quot;rm -i&quot;是一个定时炸弹，在使用它之后，习惯了之后, 你会本能地期望rm在删除文件之前会提示你。但是，总有一天你可能会用一个没有rm alias 别名的系统, 这时若你也直接随手一甩rm, 本以为会有提示, 结果发现数据真的被删除了。 在任何情况下，预防文件丢失或损坏的好方法就是进行备份。 所以如果你想个性化删除命令, 最好不要动rm，而是创建属于你的命令，比如trash, myrm, delete等, 用alias trash=&#39;/bin/rm -irv&#39;会创建一条把文件放入垃圾回收站的命令.]]></content>
      <categories>
        <category>提高效率</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>Git Bash</tag>
        <tag>Sublime</tag>
        <tag>Atom</tag>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 07 用数组构建数据列表 list - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-07-java-array-based-list%2F</url>
    <content type="text"><![CDATA[列表 List前面说到Java的数组无法更改长度，那么也就无法实现插入或者删除数组成员。Java提供了功能更丰富的数据结构 - 列表（list）。所谓列表，即有序的集合（序列），用户可以精确地控制每个元素插入到列表中的哪个位置。用户可以通过整数索引（列表中的位置）来访问元素，并搜索列表中的元素（详细可进一步参考oracle官网）。这里我们尝试以java的array为基础实现一个列表，目标是实现自动扩容 (Java中的ArrayList不仅仅有自动扩容, 也继承了[List]的其他功能)。在探索的过程中, 可以顺带学习很多相关的内容.使用自上而下的设计思想搭建一个框架:先写出最基础的部分, 也就是一个构造器，前面学过了整数数组，我们直接拿来用123456789101112131415161718/** Array based list. */// index 0 1 2 3 4 5 6 7// items: [6 9 -1 2 0 0 0 0 ...]// size: 5public class AList &#123; private int[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() &#123; items = new int[100]; size = 0; &#125; /** 下面添加其他方法 */&#125; 然后思考我们需要什么功能，把功能需求转化为实例方法instance method的形式，先把方法的外壳描绘出来，注释上该方法的功能（目的），输入值，返回值是什么之类的。具体的功能实现可以先空着，之后一步步丰富。 公共 vs 私有 Public vs. Private在上面的代码块中，可以看到 items 和 size 都被声明为 private 私有变量, 这样就只能被所在的java文件内调用. 私有变量和方法的设计初衷是服务于程序的内部功能实现, 而不是用来和外部程序(用户)进行交互的. 设置成私有, 可以避免这些变量和方法被外部程序直接调用, 避免用户通过不恰当/容易出错的方式修改某些变量. 在程序说明文档中, 一般也会明确说明程序提供什么公共变量和方法给用户调用. 因此我们这里也提供几个 public 方法让用户调用, 这样用户就能按照我们设计的方式来访问数据。分别是getLast() - 访问列表最后一个元素，get(int i)访问第i个元素, 和size()访问列表的大小.12345678910111213141516/** 程序内的方法可以访问 private 变量 *//** 返回列表末尾的值. */public int getLast() &#123; return items[size - 1];&#125;/** 返回第 i 个值 (0 是第一个). */public int get(int i) &#123; return items[i];&#125;/** 返回列表元素长度. */public int size() &#123; return size;&#125; 泛型数组我们不仅希望我们的列表可以存整数，也可以存其他类型的数据，可以通过泛型解决，泛型的介绍参考这篇文章. 泛型数组跟前面介绍的泛型示例有一个重要的语法差异：Java不允许我们创建一个通用对象的数组，原因这里不细展开。 假如我们用Item来标识泛型, 那么在上面的列表类中构建泛型数组时, 我们不能用items = new Item[8];, 而要用items = (Item []) new Object[8];, 即使这样也会产生一个编译警告，但先忍着, 后面会更详细地讨论这个问题。12345678910public class AList&lt;Item&gt; &#123; private Item[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() &#123; items = (Item[]) new Object[100]; //会有编译警告, 暂时不管, 后面会解释 size = 0; &#125;&#125; 数组扩容 Resize一个列表应该支持基本的插入和删除数据的操作，但是因为数组本身无法更改长度，所以我们就需要一个方法，在给数组在插入新数据时，先检查长度容量是否足够，如果不够，那么就要增加长度。我们考虑简单的情况, 即需要在数组末尾插入或者删除数据怎么办 插入元素：123456789101112/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; /** 检查长度容量是否足够，如果不够，那么就要增加长度 */ if (size == items.length) &#123; Item[] temp = (Item[]) new Object[size + 1]; System.arraycopy(items, 0, temp, 0, size); items = temp; &#125; items[size] = x; size = size + 1;&#125; 创建新array并把旧数据复制过去的过程通常称为“resizing”。其实用词不当，因为数组实际上并没有改变大小，只是把小数组上的数据复制到大数组上而已。 为了让代码更易于维护，可以把上面的代码中负责大小调整的部分包装在一个独立的method中12345678910111213141516/** 改变列表容量, capacity为改变后的容量. */private void resize(int capacity) &#123; Item[] temp = (Item[]) new Object[capacity]; System.arraycopy(items, 0, temp, 0, size); items = temp;&#125;/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; if (size == items.length) &#123; resize(size + 1); &#125; items[size] = x; size = size + 1;&#125; 删除元素：1234567/** 删去列表最后一个值，并返回该值 */public int removeLast() &#123; Item x = getLast(); items[size - 1] = null; // 曾经引用“删除”的元素的内存地址被清空 size = size - 1; return x;&#125; 事实上即使没有items[size - 1] = null;,也可以达到删除元素的目的.删除对改存储的对象的引用, 是为了避免“loitering”。所谓 loitering，可以理解为占着茅坑不拉屎的对象，它们已经没啥用了，却还是占用着内存。如果这个对象是些几十兆的高清图片，那么就会很消耗内存。这也是为什么安卓手机越用越慢的一个原因。 当引用/内存地址丢失时，Java会销毁对象。如果我们不清空引用，那么Java将不会垃圾回收这些本来预计要删除的对象, 因为它们实际还被列表引用着。 扩容效率分析我们直觉也会感觉到，如果按照现在的设计，即每插入一个新元素，就重新复制一遍数组，这样随着数组越来越大，效率肯定会越来越差。事实上也是这样，如果数组目前长度是100个内存块，那么插入1000次，需要创建并填充大约50万个内存块（等差数列求和N(N+1)/2，101+102+…+1000 ≈ 500000）。但假如我们第一次就扩容到1000，那么就省却了很多运算消耗。可惜我们不知道用户需要插入多少数据，所以要采取其他方法-几何调整。也就是与其按照size + FACTOR这样的速率增加容量, 不如按照size * RFACTOR成倍扩容, 前者的增加速率为1, 后者为 RFACTOR, 只要设置 RFACTOR 大于1, 就能减少扩容的次数.123456789/** 把 X 插入到列表末尾. */public void addLast(Item x) &#123; if (size == items.length) &#123; resize(size * RFACTOR); //用 RFACTOR 作为因子扩容数组, &#125; items[size] = x; size = size + 1;&#125; 目前我们解决了时间效率问题, 但代价是需要更大的内存空间, 也就是空间效率下降了. 假设我们插入了十亿个item，然后再删去九亿九千万个项目。在这种情况下，我们将只使用10,000,000个内存块，剩下99％完全没有使用到。 为了解决这个问题，我们可以在数组容量利用率比较低时把容量降下来. 定义利用率 R 为列表的大小除以items数组的长度。一般当R下降到小于0.25时，我们将数组的大小减半。 其他功能比如排序等, 在后面介绍链表的文章中再讨论.]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 06 array 数组 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-06-java-array%2F</url>
    <content type="text"><![CDATA[数组 Array数组是一种特殊的对象，有一个固定的数组长度参数N，由一连串（N个）连续的带编号的内存块组成，每个都是相同的类型(不像Python可以包含不同类型)，索引从0到N-1编号。A[i]获得数组A的第i个元素。这与普通的类实例不同，类实例有具体变量名命名的内存块。 数组实例化，包含对象的数组 Array Instantiation, Arrays of Objects要创建最简单的整数数组, 有三种方式:123x = new int [3]; //创建一个指定长度的数组，并用默认值（0）填充每个内存块。y = new int [] &#123;1，2，3，4，5&#125;; //创建一个合适大小的数组，以容纳指定的初始值int [] z = &#123;9，10，11，12，13&#125;; //省略了new，只能结合变量声明使用。 创建一组实例化对象:12345678910public class DogArrayDemo &#123; public static void main(String[] args) &#123; /* Create an array of two dogs. */ Dog[] dogs = new Dog[2]; dogs[0] = new Dog(8); dogs[1] = new Dog(20); /* Yipping will result, since dogs[0] has weight 8. */ dogs[0].makeNoise(); &#125;&#125; 注意到new有两种不同的使用方式：一种是创建一个可以容纳两个Dog对象的数组，另外两个创建各个实际的Dog实例。 数组复制123x = new int[]&#123;-1, 2, 5, 4, 99&#125;;int[] b = &#123;9, 10, 11&#125;;System.arraycopy(b, 0, x, 3, 2); //效果类似于Python的`x[3:5] = b[0:2]` System.arraycopy的五个参数分别代表： 待复制的数组(源) 源数组复制起点 目标数组 目标数组粘贴起点 有多少项要复制 2D数组Java的二维数组实质上是一数组的数组, 即每一个数组元素里面也是一个数组。1234567891011121314151617int[][] matrix; //声明一个引用数组的数组matrix = new int[4][]; //创建四个内存块, 用默认null值填充, 之后用于储存对整数数组的引用, 即地址,int[] rowZero = matrix[0];/** 实例化整数数组, 把其地址/引用分别赋值给/储存到 matrix 的第N个内存块*/matrix[0] = new int[]&#123;1&#125;;matrix[1] = new int[]&#123;1, 1&#125;;matrix[2] = new int[]&#123;1, 2, 1&#125;;matrix[3] = new int[]&#123;1, 3, 3, 1&#125;;int[] rowTwo = matrix[2];rowTwo[1] = -5;/** 创建四个内存块, 其中每个被引用的整数数组长度为4,每个元素都是0.*/matrix = new int[4][4];int[][] matrixAgain = new int[][]&#123;&#123;1&#125;, &#123;1, 1&#125;,&#123;1, 2, 1&#125;, &#123;1, 3, 3, 1&#125;&#125;;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 05 数据类型 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-05-java-variable-types%2F</url>
    <content type="text"><![CDATA[数据类型数据类型是程序设计语言描述事物、对象的方法。Java数据类型分为基本类型（内置类型）和引用类型(扩展类型）两大类。基本类型就是Java语言本身提供的基本数据类型，比如，整型数，浮点数，字符，布尔值等等。而引用类型则是Java语言根据基本类型扩展出的其他类型，Java要求所有的引用扩展类型都必须包括在类定义里面，这就是Java为什么是面向对象编程语言的原因…上面的定义有点抽象，要理解数据类型，需要先理解一个问题: 神秘的海象问题12345678/** 尝试预测下面的代码运行时会发生什么。b的变化是否会影响a？提示：类似Python。 */Walrus a = new Walrus(1000, 8.3);Walrus b;b = a;b.weight = 5;System.out.println(a);System.out.println(b); 12345678/** 同样尝试预测下面的代码运行时会发生什么。x的改变是否影响y？ */int x = 5;int y;y = x;x = 2;System.out.println("x is: " + x);System.out.println("y is: " + y); 首先给出答案, b的变化会影响a, 但x的改变不影响y，具体见可视化过程.这里的差别虽然微妙, 但其背后的原理对于数据结构的效率来说是非常重要的，对这个问题的深入理解也将引导我们写出更安全，更可靠的代码。 基本类型 Primative Types计算机中的所有信息都以一系列1和0的形式存储在内存中，这些二进制的0和1就是比特位（bits）。比如72和“H”在内存一般以01001000的形式存储，对他们的形式是一样的。一个引申问题就是：Java代码如何解释01001000，怎么知道应该解释为72还是“H”？ 通过类型types，预先定义好类型即可, 以下代码1234char x = 'H';int y = x;System.out.println(x);System.out.println(y); 会分别得到“H”和72. 在这种情况下，x和y变量都包含几乎相同的bits，但是Java解释器在输出时对它们进行了不同的处理。 Java有8种基本类型：byte，short，int，long，float，double，boolean和char。 变量声明 Declaring Variables计算机的内存可以视为包含大量用于存储信息的内存比特位，每个位都有一个唯一的地址。现代计算机可以使用许多这样的位。 当你声明一个特定类型的变量时，Java会用一串连续的内存位存储它。例如，如果你声明一个int，你会得到一个长度32的内存list，里面有32bits。Java中的每个数据类型都有不同的比特数。 除了留出内存空间外，Java解释器还会在一个内部表中创建一个条目，将每个变量名称映射到内存块中第一个位置（表头list head）。 例如，如果声明了int x和double y，那么Java可能会决定使用计算机内存的352到384位来存储x，而20800到20864位则用来存储y。然后解释器将记录int x从352开始，y从20800开始。 在Java语言里无法知道变量的具体内存位置，例如你不能以某种方式发现x在位置352。不像C++这样的语言，可以获取一段数据的确切地址。Java的这个特性是一个折衷！隐藏内存位置自然意味着程序猿的控制权更少，就无法做某些类型的优化。但是，它也避免了一大类非常棘手的编程错误。在现在计算成本如此低廉的时代，不成熟的优化还不如少点bug。 当声明一个变量时，Java不会在预留的内存位置中写入任何内容, 也即没有默认值。因此，如果没有赋值, Java编译器会阻止你使用变量。 以上只是内存分配的简要说明, 堆和栈的介绍可以参考我的CS106B C++笔记。 引用类型 Reference Types所有基本数据类型之外的类型都是引用类型。引用类型顾名思义，就是对对象的引用。在java中内存位置是不开放给程序员的, 但我们可以通过引用类型访问内存中某处对象。所有引用类型都是 java.lang.Object 类型的子类。 对象实例化 Object Instantiation对象实例化：当我们使用new（例 new Dog）实例化对象时，Java首先为类的每个实例变量分配一串长度合适的bits位，并用缺省值填充它们。然后，构造函数通常（但不总是）用其他值填充每个位置.123456789public static class Walrus &#123; public int weight; public double tuskSize; public Walrus(int w, double ts) &#123; weight = w; tuskSize = ts; &#125;&#125; 用new Walrus(1000, 8.3)创建一个Walrus实例后, 我们得到分别由一个32位(int weight = 1000)和一个64位(double tuskSize = 8.3)的内存块组成的实例：通过程序可视化过程)来更好地理解. 当然在Java编程语言的实际实现中，实例化对象时都有一些额外的内存开销, 这里不展开. 通过 new 实例化对象，new 会返回该对象的内存地址给我们，但假如我们没有用一个变量去接收这个地址，那么我们就无法访问这个对象。之后该对象会被作为垃圾回收。 引用变量声明 Reference Variable Declaration前面有提到，我们需要声明变量来接受实例化的对象在内存中的地址。当声明任何引用类型的变量（比如array, 前面的Dog类等）时，Java都会分配一串64位的内存位置. 这个64位的内存块仅用于记录变量的内存地址, 所谓内存地址, 可以理解为内存(房子)的编号(地址), 一般是内存块的表头位置的64位表达式1234Walrus someWalrus; // 创建一个64位的内存位置someWalrus = new Walrus(1000, 8.3); //创建一个新的实例/** 内存地址由 new 返回, 并被复制/赋值给 someWalrus 对应的内存位置*/ 比如, 假设weight是从内存位5051956592385990207开始存储的，后面连续跟着其他实例变量，那么就可以把5051956592385990207存储在Dog变量中。5051956592385990207由64位的二进制0100011000011100001001111100000100011101110111000001111000111111表达，这样smallDog的内存就可以抽象的理解为一个表smallDog: 0100011000011100001001111100000100011101110111000001111000111111 -&gt; 具体存放实例的内存(Walrus: weight=1000, tuskSize=8.3)‘-&gt;’可以理解为指针. 实例化数组在前面有介绍过，数组array是引用类型，是对象，故数组变量只是存储内存位置。 前面有提到，如果丢失了引用变量存储的内存地址，那么该地址对应的对象就找不回来了。例如，如果一个特定的 Walrus 地址的唯一副本存储在x中，那么x = null这行代码将删去地址，我们则丢失了这个 Walrus 对象。这也不一定是坏事，很多时候在完成了一个对象后就不在需要了，只需简单地丢弃这个参考地址就可以了。 Java 等值规则 Rule of Equals对于y = x，Java解释器会将x的位拷贝到y中,这个规则适用于java中任何使用=赋值的语法, 是理解开头的”神秘的海象”问题的关键. 基本类型变量的位, 存储赋值的值（基本类型）在内存中值(具体位数取决于具体的类型) 1234int x = 5; // 此时是把内存中的某一个地址 p 复制给 xint y;y = x; // y 也指向 px = 2; // 把一个新的内存地址 new p 复制给x, 但y还是指向原来的p x的位存储的是基本类型int 5(32 bits), x = 2是把新的基本类型int 2复制给x, 但y还是指向原来的int 5， 所以y没变化。 引用类型 reference type 变量的位, 存储赋值的值（引用类型）在内存中的地址(固定的64 bits) 1234Dog a = new Dog(5); // 创建一个64位的内存位, 并赋值一个新的实例 pDog b; // 仅创建一个64位的内存位, 没有引用内存地址(null)b = a; // 把a的位（是实例 p 的内存地址）复制给b, 这样 b 也是指向实例 pb.weight = 21; // 此时修改b, 会改写b指向的内存实例 p a和b只存储地址, 而它们的地址都指向相同的实例； 如果对 b 的修改本质是对 p的修改, 那么输出a.weight的时候, 就会变成21. 参数传递 Parameter Passing给函数传递参数，本质上也是赋值操作，参考上面的等值规则，也即复制这些参数的bits给函数，也称之为pass by value。Java的参数传递都是pass by value。至于传递过去的参数会不会因为函数内部的操作而更改，其判断原理在上面的等值规则已经阐明。 通用数据类型 Generic在定义类的时候，有时候我们可能希望这个类能够接受任何类型的数据，而不仅仅是限定了基本类型中的任何一种。比如我们想实现一个类似PPT的类，自然需要这个PPT类能够接收各种类型的字符，数字，并呈现出来。这个时候就需要使用泛型 Generic, 也即通用数据类型。 在2004年，Java的设计者在语言中加入了泛型，使​​我们能够创建包含任何引用类型的数据结构（引用类型和基本类型的解释参考另一篇文章, ）。方法就是在类声明的类名后面，使用一个任意的占位符，并用尖括号括住&lt;随便什么字符&gt;。然后，在任何你想使用泛型的地方，改用占位符。比如1234567public class PPT &#123; public class PPT &#123; public int item; ... &#125; ...&#125; 改为1234567public class PPT&lt;xxx&gt; &#123; public class PPT &#123; public xxx item; ... &#125; ...&#125; &lt;xxx&gt;里面的名称并不重要, 改成其他也行, 只是一个标识符, 用来接受参数, 当用户实例化这个类时, 必须使用特殊的语法PPT&lt;String&gt; d = new PPT&lt;&gt;(&quot;hello&quot;); 由于泛型仅适用于引用类型，因此我们不能将基本类型int等放在尖括号内。相反，我们使用基本类型的引用版本，比如对于int, 用 Integer，PPT&lt;Integer&gt; d = new PPT&lt;&gt;(&quot;10&quot;); 总结使用方法: 在一个实现某数据结构的.java文件中，在类名后面, 只指定泛型类型一次。 在其他使用该数据结构的java文件中，声明实例变量时要指定所需的类型。 如果您需要在基本类型上实例化泛型，请使用Integer, Double, Character, Boolean, Long, Short, Byte, Float，而不是其基本类型。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 04 类 class 02 类与实例 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-04-java-class-02-class-instance%2F</url>
    <content type="text"><![CDATA[Class前面提到，类的方法和变量细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。 类方法 vs 实例方法 Class Methods vs. Instance Methods参考上一篇文章的例子，类方法由类调用Dog.makeNoise();. 实例方法只能由实例调用bigDog.makeNoise();. 同理可推, 类方法无法调用实例变量.可以看到实例方法更具体, 更贴近实体世界, 那我们仍需要类方法, 因为: 有些类不需要实例化, 毕竟我们也经常需要处理抽象的概念, 这些抽象概念在人类认知范畴内是统一的, 比如数学计算, 我们需要计算某个数值的平方根, x = Math.sqrt(100);, 拿来就用, 不需要先实例化. 这点在Python中体现得很好. 有些类有静态方法, 是有实际作用的。例如，若想比较一个类里面的不同实例, 比如两只狗的重量。比较简单的方法就是使用一个比较狗的重量的类方法: 123456789public static Dog maxDog(Dog d1, Dog d2) &#123; if (d1.weight &gt; d2.weight) &#123; return d1; &#125; return d2;&#125;Dog d = new Dog(15);Dog d2 = new Dog(100);Dog.maxDog(d, d2); 这个时候, 若使用实例方法也可以, 但没那么直观：12345678910/** 我们使用关键字this来引用当前对象d。*/public Dog maxDog(Dog d2) &#123; if (this.weight &gt; d2.weight) &#123; return this; &#125; return d2;&#125;Dog d = new Dog(15);Dog d2 = new Dog(100);d.maxDog(d, d2); 类变量 vs 实例变量 Class Variables vs. Instance Variables静态变量的也是有用处的。这些变量一般是类本身固有的属性。例如，我们可能需要用狗类的另一种生物学的统称“犬科”来作为类的说明， 这个时候可以用public static String binomen = &quot;犬科&quot;;，这个变量理论上是由类来访问的。虽然Java在技术上允许使用实例名称来访问静态变量，但是这有时候可能会令人困惑， 所以还是少用为好。 构造器 Constructors in Java与上面的DogLauncher实例化对象的方式相比, 我们更希望实例化可以带参数的，那样可以为我们节省手动给实例变量赋值的麻烦。为了启用这样的语法，我们只需把如下的构造函数直接添加进Dog类中：12345/**注意：构造函数与class类同名 */public Dog(int w) &#123; weight = w; &#125; 然后在DogLauncher里实例化一只狗时, 直接Dog d = new Dog(20);即可. 在以上代码的基础上, 后续当我们想使用new和参数创建一只狗时，可以随时调用public Dog(int w)构造函数。对于熟悉Python的人来说，你可以理解java的构造函数为Python的__init__。 一些术语: 声明(declaration): Dog smalldog;声明一个类作为一个变量在内存中占位 实例化: new Dog(20), 如果没有把它作为值赋给一个类声明变量,那么这个实例化的值会被垃圾回收. 声明, 实例化并赋值: Dog smalldog = new Dog(5)]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 04 类 class 01 变量和方法 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-04-java-class-01-intro%2F</url>
    <content type="text"><![CDATA[ClassJava的语法是为了更容易地模拟真实世界而设计的. 比如用程序实现一只狗, 可以用定义一个类class来描述它. 类class里面包括变量Variable，方法method（可以理解为Python的函数function）。变量可以储存数据，方法可以处理数据。变量必须在类中声明(即不能离开类独立存在)，不像Python或Matlab这样的语言可以在运行时添加新的变量。类的方法和变量又细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。实例instance的概念后面会解释。 类（静态）变量和方法 Class(Static) Variables and Methods静态变量和方法的特征就是有static字符在前面.以下代码定义了一个类来模拟狗，包含一个类变量作为这个类的说明，一个类方法用于发出叫声：12345678public class Dog &#123; public static String instruction = "狗类实例"; //类变量, 说明 public static void makeNoise() &#123; System.out.println("汪!"); &#125;&#125; 这里没有定义main(), 在这种情况下如何直接运行这个类(java Dog), 程序是会报错的123错误: 在类 Dog 中找不到 main 方法, 请将 main 方法定义为: public static void main(String[] args)否则 JavaFX 应用程序类必须扩展javafx.application.Application`. 你可以选择在里面添加一个main()方法. 但这次我们选择不定义具体的main(). 具体要如何运行, 我们可以另写一个类定义一个main()方法来调用这个类.12345public class DogLauncher &#123; public static void main(String[] args) &#123; Dog.makeNoise(); &#125;&#125; 这两种方式(在类A内部定义好main() vs. 在其他类B定义main()来调用A)没有优劣之分, 二者有不同的适用情况. 随着不断深入学习，二者的区分将变得更清晰。 注意到, 类变量和方法是有局限性的。现实世界中, 并不是所有的狗都是一样的特征，仅仅靠类这个概念是无法区分不同个体的狗, 除非你为不同的狗定义不同的类（以及里面的变量和方法）, 那么就会很繁琐痛苦. 也就是说，用类来模拟个体是低效的，我们要使用实例. 实例变量和对象实例化 Instance Variables and Object InstantiationJava的类定义就像定义一张蓝图, 我们可以在这个蓝图的基础上, 生成不同的实例instance. 实例是概念性的说法，本质上在Java里就是对象object。这样的特性提供了一个很自然而然地在java中模拟生成实体世界的方法：定义一个狗的类，在这个类的基础上，通过不同的特征参数实例化不同特征的狗（instances），并使类方法的输出取决于特定实例的狗的属性。1234567891011121314/** 一只狗的类:*/public class Dog &#123; public int weight; public void makeNoise() &#123; if (weight &lt; 10) &#123; System.out.println("嘤嘤嘤!"); &#125; else if (weight &lt; 30) &#123; System.out.println("汪汪汪"); &#125; else &#123; System.out.println("嗷呜!"); &#125; &#125;&#125; 这里的方法和变量没有static, 所以是实例（非静态）方法和变量. 如果直接用 Dog 类来调用这些方法, 会报错:123456public class DogLauncher &#123; public static void main(String[] args) &#123; Dog.weight = 21; Dog.makeNoise(); &#125;&#125; 123456DogLauncher.java:3: 错误: 无法从静态上下文中引用非静态 变量 weight Dog.weight = 21; ^DogLauncher.java:4: 错误: 无法从静态上下文中引用非静态 方法 makeNoise() Dog.makeNoise(); ^ 这个时候, 你需要实例化一只狗, 让这个实例来调用非静态变量和方法:1234567public class DogLauncher &#123; public static void main(String[] args) &#123; Dog biglDog = new Dog(); biglDog.weight = 5; biglDog.makeNoise(); &#125;&#125; 运行时，这个程序将会创建一个重量为5的狗，这个狗就会“嗷呜”叫。 总的来说，之所以需要实例方法和变量，是因为我们需要模拟个体，一只具体的狗，并让它发出声音。这个weight和makeNoise()只能由具体的狗调用。狗类不能调用，也没有调用的意义, 毕竟每只狗的重量和声音都不同的. 在设计程序时, 如果其中一个方法我们只打算让特定的实例来调用它(而不让类去调用它), 那么这个方法应该设计成实例方法。]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 03 代码风格 注释 Javadoc - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-03-java-code-style-comments%2F</url>
    <content type="text"><![CDATA[代码风格与注释 Code style and comments在学习和实践过程中，我们应该努力保持代码可读性。良好的编码风格的一些最重要的特点是： 一致的风格（间距，变量命名，缩进风格等） 大小（线不太宽，源文件不要太长） 描述性命名（变量，函数，类），例如变量或函数名称为年份或getUserName而不是x或f。让代码本身提供可解读性。 避免重复的代码：几乎不会有两个重要的代码块几乎相同，除了一些改变。 适当的评论, 使其他读者也能轻松理解你的代码 行注释: //分隔符开头行被当做注释。 Block（又名多行注释）注释: /*, */, 但我们更推荐javadoc形式的注释。 JavadocJavadoc: / **，*/, 可以（但不总是）包含描述性标签。 借助javadoc工具可以生成HTML格式的API文档。第一段是方法的描述。描述下面是不同的描述性标签, 比如参数 @param， 返回值 @return， 可能抛出的任何异常 @throws123456789/** * @author 名字，邮箱&lt;address @ example.com&gt; * @version 1.6 版本 * @param * @return */public class Test &#123; // class body&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 02 语法基础 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-02-java-basic-syntax%2F</url>
    <content type="text"><![CDATA[Java基本语法12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello world!"); &#125;&#125; 上面的程序由一个类声明组成，该声明使用关键字public class声明。 Java所有的代码都应该包含在class里面。 真正负责运行的代码，是一个名为main的method，它声明为public static void main(String[] args)。 public：公共的，大部分方法都是以这个关键字开始的，后面会进一步解释。 static：这是一个静态方法，不与任何特定的实例关联，后面会解释。 void：它没有返回类型。 main：这是方法的名称。 String [] args：这是传递给main方法的参数。 使用大括号{ }来表示一段代码的开始和结束。 声明必须以分号结尾 静态分类 Static Typing程序语言静态与动态的分类，可以参考oracle的说明文件，它解释了动态和静态类型之间的区别, 帮助你理解由程序的错误提示信息。两个主要区别:1. 动态类型语言在运行时执行类型检查，而静态类型语言在编译时执行类型检查。这意味如果以静态类型语言（如Java）编写的脚本包含错误，则在编译错误之前将无法编译. 而用动态类型语言编写的脚本可以编译，即使它们包含会阻止脚本正常运行（如果有的话）的错误。2. 静态类型语言要求你在使用它们之前声明变量的数据类型，而动态类型语言则不需要。考虑以下两个代码示例：123// Javaint num;num = 5; 12# Pythonnum = 5 这两段代码都创建一个名为num的变量并赋值为5. 不同之处在于Java需要将num的数据类型明确定义为int。因为Java是静态类型的，因此它期望变量在被赋值之前被声明。 Python是动态类型的，不需要定义类型, Python根据变量的值确定其数据类型。动态类型语言更加灵活，在编写脚本时可以节省时间和空间。但是，这可能会导致运行时出现问题。例如：123# pythonnumber = 5numbr = (number + 15) / 2 #注意错字 上面的代码本应创建一个值为5的可变数字，然后将其加上15并除以2以得到10. 但是，number在第二行的开头拼写错误。由于Python不需要声明变量，因此会不由分说直接创建一个名为numbr的新变量，并把本应分配给number的值分配给它。这段代码会很顺利编译，但是如果程序试图用number来做某事，程序员假设它的值是10，那么后续就无法产生期望的结果,而且还很难注意到问题。 Java的compiler其中一个关键作用是进行静态类型检查（static type check）。若前面定义了 int x = 0;, 那么后面若给x赋值其他的类型值x = &#39;horse&#39;;, compiler就会报错. 这样就保证了程序不会出现类型错误. 除了错误检查外, static types 也可以让程序媛/猿知道自己处的是什么对象. 总而言之，静态类型具有以下优点： 编译器确保所有类型都是兼容的，这使得程序员更容易调试他们的代码。 由于代码保证没有类型错误，所以编译后程序的用户将永远不会遇到类型错误。例如，Android应用程序是用Java编写的，通常仅以.class文件的形式分发，即以编译的格式。因此，这样的应用程序不应该由于类型错误而崩溃。 每个变量，参数和函数都有一个声明的类型，使程序员更容易理解和推理代码。 Code Style, Comments, Javadoc在学习和实践过程中，我们应该努力保持代码可读性。良好的编码风格的一些最重要的特点是： 一致的风格（间距，变量命名，缩进风格等） 大小（线不太宽，源文件不要太长） 描述性命名（变量，函数，类），例如变量或函数名称为年份或getUserName而不是x或f。让代码本身提供可解读性。 避免重复的代码：几乎不会有两个重要的代码块几乎相同，除了一些改变。 适当的评论, 使其他读者也能轻松理解你的代码 行注释: //分隔符开头行被当做注释。 Block（又名多行注释）注释: /*, */, 但我们更推荐javadoc形式的注释。 Javadoc: / **，*/, 可以（但不总是）包含描述性标签。 借助javadoc工具可以生成HTML格式的API文档。第一段是方法的描述。描述下面是不同的描述性标签, 比如参数 @param， 返回值 @return， 可能抛出的任何异常 @throws123456789/** * @author 名字，邮箱&lt;address @ example.com&gt; * @version 1.6 版本 * @param * @return */public class Test &#123; // class body&#125;]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构 - Java | 01 安装 - CS61B Berkeley]]></title>
    <url>%2FNOTE-CS61B-data-structures-01-java-install%2F</url>
    <content type="text"><![CDATA[Hello World本系列是伯克利 Josh Hug 的 cs61b spring 2017 和 cs61b spring 2018 的学习笔记. Lab, homework 和 project 代码实现参考https://github.com/ShootingSpace/cs61b-data-structures Java安装与配置安装Java，前往Oracle下载java sdk，我用的是Java SE 8u151/ 8u152 版本。安装sdk时会同时安装sdr。 Windows系统配置: 推荐安装git bash, 一切按照默认安装就好. 更新系统环境变量: 直接在运行中搜索Environment Variables, 选择编辑系统环境变量, 在弹出的框中选择高级-&gt;环境变量, 在弹出的框中系统变量里面 新建变量: 变量名 = JAVA_HOME, 变量值 = 你的jdk路径,如C:\Program Files\Java\jdk1.8.0_151 编辑Path: 在前面加入%JAVA_HOME%\bin;%PYTHON_HOME%;(请注意，不能有空格.) OS X系统配置: 安装Homebrew，一个非常好用的包管理工具。要安装，请在terminal终端输入ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;(注意：在此过程中，可能会提示输入密码。当输入密码时，终端上不会显示任何内容，但计算机还是会记录你的密码的。这是一个安全措施, 让其他人在屏幕上看不到你的密码。只需输入您的密码，然后按回车。) 然后，通过输入以下命令来检查brew系统是否正常工作brew doctor. 如果遇到警告，要求下载命令行工具，则需要执行此操作。请参考这个StackOverflow。 安装git：输入brew install git 安装并配置好java后，测试是否成功:随便在你喜欢的文件夹里新建一个java文件HelloWorld.java12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello world!"); &#125;&#125; 你可以选择用sublime来快速新建文件, 直接在你选择的文件里右键 git bash, 在git bash 里面键入subl HelloWorld.java, 还自动启动sublime并新建一个空白的HelloWorld.java文件, 把上面的代码复制进去并保存即可. (若出现类似提示: 找不到subl command, 解决办法请参考博文在Gitbash中直接启动sublime或atom等编辑器以打开或新建文件 )开始真正的测试。直接在之前打开的git bash中输入: ls, 会看到HelloWorld.java这个文件, ls会列出这个目录中的文件/文件夹 javac HelloWorld.java, 理论上这一步不会有任何输出，有的话可能是设置有问题。现在，如果你继续ls，会看到多了一个HelloWorld.class文件， 这是javac创建的。 java HelloWorld (注意没有.java), 会看到输出Hello World, 表明你的Java设置没有问题]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Java</tag>
        <tag>软件工程</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>cs61b</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能AI入门到进阶]]></title>
    <url>%2Fai%2F</url>
    <content type="text"><![CDATA[简介记录学习AI的学习笔记，内容包含基础知识的总结以及编程实现的整理。 Language:English 目录 人工智能 机器学习 深度学习 自然语言处理 计算机视觉 机器人 大数据 MapReduce 人工智能机器学习 Coursera Machine Learning， 吴恩达的简化版机器学习 Machine Learning, 吴恩达的机器学习课程 这个比较深入 Deep Learning, 吴恩达的深度学习课程 Neural Networks for Machine Learning, Hinton的神经网络课程 深度学习 Deep learning, Coursera Machine Learning Practical: DNN, CNN, RNN 每个lab的答案在下一个lab branch里，即lab1的答案可以在lab2 branch里面看到。这个代码全部用Python class，比coursera的难度高点。 自然语言处理 自然语言处理, 斯坦福 加速自然语言处理, 爱丁堡大学 深度学习处理自然语言，斯坦福 计算机视觉 图像识别：卷积神经网络，李飞飞，斯坦福 机器人 机器人入门，斯坦福 大数据 Hadoop和MapReduce入门，优达学城 MapReduce极限计算，爱丁堡大学 并行计算入门：MPI, openMP, and CUDA, 斯坦福 参考:Guide to technical development from Google educationOS Free Programming Books]]></content>
      <categories>
        <category>学习笔记</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>自然语言处理</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>计算机视觉</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS入门到进阶]]></title>
    <url>%2Fcs%2F</url>
    <content type="text"><![CDATA[简介记录学习CS的学习笔记，内容包含基础知识的总结以及编程实现的整理。 Language:English 目录 CS入门 学习编写(至少)一种面向对象编程语言(C ++，Java®，Python®) 学习其他编程语言 测试你的代码 逻辑推理和离散数学 深入了解算法和数据结构 了解计算机操作系统 CS入门现在的入门课基本都是用Python语言。 计算机科学导论，优达学城 CS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML CMU 15213: Introduction to Computer Systems (ICS) 面向对象编程语言一般而言，建议先学Java，Python，再学C++。 这三种语言都基本掌握后，再根据自身的职业需求，选择其中一个语言（或者其他语言）进一步深入练习。因为学校课程主要以Python为主，所以目前我还是主要深入学习Python，这是我的Python学习笔记。 面向初学者程序员的在线资源： 编程方法学，斯坦福CS106A，Java 伯克利大学CS 61A计算机程序的结构与解读，Python Java编程简介，MIT Google的Python Class Google的C ++类 面向有经验的程序员的在线资源： 数据结构，伯克利大学 CS 61B，Java 计算机程序设计，Udacity，Python 抽象编程，斯坦福 CS106B，C ++最新作业：http://web.stanford.edu/class/cs106b/ 《数据结构与算法分析:C++描述》, Mark A. Weiss 其他编程语言根据实际需要自行选择一种或多种学习： JavaScript® CSS＆HTML Ruby® Lua PHP® Haskell Perl® Go Shell®脚本 Lisp® Scheme® 一些在线资源： CS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML Codecademy JavaScript Bento JavaScript Learning Track(Bento) Egghead.io 学习如何编程：JavaScript - Epicodus Inc. 学习：查询 CSS ＆ HTML Bento CSS Learning Track(Bento) Bento HTML Learning Track(Bento) 用破折号建立个人网站 使用Webflow构建响应式网站 使用骨架构建SaaS着陆页 建立动态网站 在1小时内编写个人启动页面：实用HTML和CSS简介 学习如何编程：CSS - Epicodus Inc. 从头开始学习HTML5编程 Ruby 学习如何编程：Ruby - Epicodus Inc. RubyMonk - 交互式Ruby教程 Haskell C9：功能编程基础知识 - Erik Meijer CIS 194：Haskell简介 - Brent Yorgey CS240h：Haskell的功能系统 - Bryan O’Sullivan edX：功能编程简介 - Erik Meijer 亚琛大学：功能编程 - JürgenGiesl Lua Lua Interactive Crash Course Lua Tutorial PHP 学习如何编程：PHP - Epicodus Inc. GO Go Tutorial 测试你的代码了解如何捕获错误，创建测试和破解软件. 软件测试，Udacity 软件调试，Udacity 逻辑推理和离散数学 数学计算机科学，麻省理工学院 数学思考导论，斯坦福大学，Coursera 概率图形模型，斯坦福大学，Coursera 博弈论，斯坦福大学和不列颠哥伦比亚大学，Coursera 算法和数据结构了解基本数据类型(堆栈，队列和袋子)，排序算法(快速排序，合并，堆栈)，数据结构(二叉搜索树，红黑树，哈希表)和Big O. 算法简介，麻省理工学院，2011秋季 算法，普林斯顿大学，Part 1 ＆ Part2 算法：设计和分析，斯坦福大学 算法，第4版，by Robert Sedgewick and Kevin Wayne 参考:Guide to technical development from Google educationOS Free Programming Books]]></content>
      <categories>
        <category>学习笔记</category>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>软件工程</tag>
        <tag>计算机科学</tag>
      </tags>
  </entry>
</search>
