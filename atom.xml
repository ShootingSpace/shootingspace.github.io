<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Computer Science &amp; AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://shukebeta.me/"/>
  <updated>2020-02-06T09:10:49.542Z</updated>
  <id>http://shukebeta.me/</id>
  
  <author>
    <name>Cong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>利用OCR把纸质登记表电子化归档</title>
    <link href="http://shukebeta.me/application-table-OCR/"/>
    <id>http://shukebeta.me/application-table-OCR/</id>
    <published>2020-02-05T16:00:00.000Z</published>
    <updated>2020-02-06T09:10:49.542Z</updated>
    
    <content type="html"><![CDATA[<p>最近因为新冠肺炎，各地的交通出入口都需要加强人流管理，但是各地各级政府的信息化水平参差不齐，很多都是人工手工纸质登记记录。</p><p>朋友公司最近就收到上海周边一个地方政府的请求，能否把高铁站纸质登记表格电子化归档。临时受托帮朋友写了个程序。<br><a id="more"></a></p><h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><ol><li>分析场景：拿到朋友给的图片，手写很潦草，而且身份证等长数字都超出边界挤在边边角角里面。</li><li>确定目标：优先保证表格结构，不然错乱的的文字没有意义。其次保证文本准确识别。</li><li>寻找现在市面上的开源方案：看到优图实验室的OCR，有两个接口，一个是针对表格的，一个是针对手写的。</li><li>分析方案：我们的情况是表格+手写的结合。在demo页面试用了一下，觉得手写接口对字的识别较好，但是表格接口更好地兼顾了表格结构。因此最后决定使用表格OCR接口。</li></ol><p>程序基本的思路是</p><ol><li>图片预处理</li><li>优图OCR接口处理 </li><li>接口输出解码到本地excel</li></ol><p>开源地址：<br><a href="https://github.com/congchan/OCR-For-Table" target="_blank" rel="noopener">https://github.com/congchan/OCR-For-Table</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近因为新冠肺炎，各地的交通出入口都需要加强人流管理，但是各地各级政府的信息化水平参差不齐，很多都是人工手工纸质登记记录。&lt;/p&gt;
&lt;p&gt;朋友公司最近就收到上海周边一个地方政府的请求，能否把高铁站纸质登记表格电子化归档。临时受托帮朋友写了个程序。&lt;br&gt;
    
    </summary>
    
      <category term="CS" scheme="http://shukebeta.me/categories/CS/"/>
    
    
      <category term="Python" scheme="http://shukebeta.me/tags/Python/"/>
    
      <category term="OCR" scheme="http://shukebeta.me/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>A Lite BERT(AlBERT) 原理和源码解析</title>
    <link href="http://shukebeta.me/NLP-albert/"/>
    <id>http://shukebeta.me/NLP-albert/</id>
    <published>2020-01-10T16:00:00.000Z</published>
    <updated>2020-01-11T02:58:51.256Z</updated>
    
    <content type="html"><![CDATA[<h3 id="A-Lite-BERT"><a href="#A-Lite-BERT" class="headerlink" title="A Lite BERT"></a>A Lite BERT</h3><p>BERT(Devlin et al., 2019)的参数很多, 模型很大, 内存消耗很大, 在分布式计算中的通信开销很大.</p><p>但是BERT的高内存消耗边际收益并不高, 如果继续增大BERT-large这种大模型的隐含层大小, 模型效果不升反降.</p><p>针对这些问题, 启发于mobilenet, Alert使用了两种减少参数的方法来降低模型大小和提高训练速度, 分别是Factorized embedding parameterization和Cross-layer parameter sharing. 这些设计让ALBERT增加参数大小的边界收益远远大于BERT.</p><p>除此之外, 在句子关系任务上抛弃了bert的<code>nsp</code>任务, 改为<code>sop</code>任务.<br><a id="more"></a></p><p>整体而言, ALBERT是当前众多BERT系列模型的集大成者, 其思路值得学习, 代码也写得很清楚. 下面仔细过一遍.</p><h3 id="Factorized-embedding-parameterization"><a href="#Factorized-embedding-parameterization" class="headerlink" title="Factorized embedding parameterization"></a>Factorized embedding parameterization</h3><p>BERT以及后续的XLNet(Yang et al., 2019), RoBERTa(Liu et al., 2019)等, WordPiece embedding的维度<code>E</code>是和隐层维度<code>H</code>绑定的. WordPiece embedding本意是学习context-independent的表达，而hidden-layer旨在学习context-dependent的表达。将WordPiece embedding大小<code>E</code>与隐层大小<code>H</code>解绑，可以更有效地利用建模所需的总模型参数.</p><p>从实用性的角度看, 这样可以减少词汇量对模型大小的影响. 在NLP中词汇量一般都很大, 所以这个解绑收益是很明显的.</p><p>具体的做法就是对embedding进行因式分解, 把非常大的单词embedding分解成两个小的矩阵, <code>O(V × H)</code>变成<code>O(V × E + E × H)</code>, 可以显著减少单词映射embedding的参数量. 这个在topic models一文中的隐变量模型中类似的思路体现.</p><h3 id="Cross-layer-parameter-sharing"><a href="#Cross-layer-parameter-sharing" class="headerlink" title="Cross-layer parameter sharing"></a>Cross-layer parameter sharing</h3><p>各个 transformer blocks 所有参数共享, 这样参数不再随着模型层数加深而增大.</p><h3 id="No-Dropout"><a href="#No-Dropout" class="headerlink" title="No Dropout"></a>No Dropout</h3><p>RoBERTA指出BERT一系列模型都是”欠拟合”的, 所以干脆直接关掉dropout, 那么在ALBERT中也是去掉 Dropout 层可以显著减少临时变量对内存的占用. 同时论文发现, Dropout会损害大型Transformer-based模型的性能。</p><h3 id="Sentence-order-Prediction-SOP"><a href="#Sentence-order-Prediction-SOP" class="headerlink" title="Sentence-order Prediction (SOP)"></a>Sentence-order Prediction (SOP)</h3><p>BERT使用的NSP任务是一种二分类loss，预测原始文本中是否有两个片段连续出现，通过从训练语料库中获取连续片段来创建正样本；通过将不同文档的片段配对作为负样本.</p><p>在RoBERTA等改进型的论文中都指出, NSP的表现不是很稳定, 所以RoBERTa直接就去掉了NSP任务. </p><p>而ALBERT推测, NSP任务对下游任务提升不稳定的原因在于NSP任务学习难度不够高(相对于MLM任务)。NSP本质是融合了topic prediction主题预测和coherence prediction两个任务。Coherence prediction是核心的任务, 可以学习inter-sentence信息. 主题预测, 也就是学习两个句子是否来自同一段原文, 则相对容易得多，并且与使用MLM损失学习的内容重叠更多。</p><p>所以我们需要一个更专注于coherence prediction的sentence level任务, 比如ALBERT中用到的SOP. </p><p>SOP的正样本采样方法和BERT一样, 但负样本改为倒置顺序的两句话, 这迫使模型学习关于discourse-level coherence properties的细粒度区别。</p><h3 id="Transformer实现"><a href="#Transformer实现" class="headerlink" title="Transformer实现"></a>Transformer实现</h3><p>Bert和Albert的核心模型架构都是Transformer encoder, 包括用于编码context的<code>Multi-headed self attention</code>层, 用于计算非线性层间特征的<code>Feed-forward layers</code>, 和用于加深网络深度, 降低训练难度的<code>Layer norm and residuals</code>. 除此之外, 还有<code>Positional embeddings</code>用来编码相对位置信息.<img src="/images/transformer_encoder.png" alt="" title="A Transformer encoder."></p><p>Transformer由一个个结构相同的blocks堆叠而成, 每一个block可以简单理解为一个注意力层+全连接层+残差网络, API是这样:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_ffn_block</span><span class="params">(layer_input,</span></span></span><br><span class="line"><span class="function"><span class="params">                        hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        attention_head_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        intermediate_act_fn=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        hidden_dropout_prob=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">  <span class="string">"""A network with attention-ffn as sub-block.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    layer_input: float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      from_width].</span></span><br><span class="line"><span class="string">    hidden_size: (optional) int, size of hidden layer.</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span></span><br><span class="line"><span class="string">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span></span><br><span class="line"><span class="string">      attention scores will effectively be set to -infinity for any positions in</span></span><br><span class="line"><span class="string">      the mask that are 0, and will be unchanged for positions that are 1.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads.</span></span><br><span class="line"><span class="string">    attention_head_size: int. Size of attention head.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: float. dropout probability for attention_layer</span></span><br><span class="line"><span class="string">    intermediate_size: int. Size of intermediate hidden layer.</span></span><br><span class="line"><span class="string">    intermediate_act_fn: (optional) Activation function for the intermediate</span></span><br><span class="line"><span class="string">      layer.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initializer.</span></span><br><span class="line"><span class="string">    hidden_dropout_prob: (optional) float. Dropout probability of the hidden</span></span><br><span class="line"><span class="string">      layer.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    layer output</span></span><br><span class="line"><span class="string">  """</span></span><br></pre></td></tr></table></figure></p><p>其中最开始是注意力层, 并在输出后面接残差, 最后正则化:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"attention_1"</span>):</span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">    attention_output = attention_layer(</span><br><span class="line">        from_tensor=layer_input,</span><br><span class="line">        to_tensor=layer_input,</span><br><span class="line">        attention_mask=attention_mask,</span><br><span class="line">        num_attention_heads=num_attention_heads,</span><br><span class="line">        attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">        initializer_range=initializer_range)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">  <span class="comment"># with `layer_input`.</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">    attention_output = dense_layer_3d_proj(</span><br><span class="line">        attention_output,</span><br><span class="line">        hidden_size,</span><br><span class="line">        attention_head_size,</span><br><span class="line">        create_initializer(initializer_range),</span><br><span class="line">        <span class="keyword">None</span>,</span><br><span class="line">        name=<span class="string">"dense"</span>)</span><br><span class="line">    attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">attention_output = layer_norm(attention_output + layer_input)</span><br></pre></td></tr></table></figure></p><p>其中用到的点乘注意力和多头注意力直接使用上一篇<a href="/NLP-attention-03-self-attention#Multi-head-Attention">Transformer &amp; Self-Attention (多头)自注意力编码</a>中的方法.</p><p>然后就是feed forward layer, 在输出层之前加入了一个升维的中间层<code>intermediate</code>, 并应用激活函数(在这里是<code>gelu</code>), 末尾的输出网络没有激活函数, 只负责把输出映射回<code>transformer</code>的隐含层维度大小, 最后同样加上残差和正则化. 这种<strong>扩张-变换-压缩</strong>的范式, 是借鉴了mobilenet中的思路, 在需要使用<code>ReLU</code>的卷积层中，将channel数扩张到足够大，再进行激活，被认为可以降低激活层的信息损失。:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"ffn_1"</span>):</span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">    intermediate_output = dense_layer_2d(</span><br><span class="line">        attention_output,</span><br><span class="line">        intermediate_size,</span><br><span class="line">        create_initializer(initializer_range),</span><br><span class="line">        intermediate_act_fn,</span><br><span class="line">        num_attention_heads=num_attention_heads,</span><br><span class="line">        name=<span class="string">"dense"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">      ffn_output = dense_layer_2d(</span><br><span class="line">          intermediate_output,</span><br><span class="line">          hidden_size,</span><br><span class="line">          create_initializer(initializer_range),</span><br><span class="line">          <span class="keyword">None</span>,</span><br><span class="line">          num_attention_heads=num_attention_heads,</span><br><span class="line">          name=<span class="string">"dense"</span>)</span><br><span class="line">    ffn_output = dropout(ffn_output, hidden_dropout_prob)</span><br><span class="line">ffn_output = layer_norm(ffn_output + attention_output)</span><br><span class="line"><span class="keyword">return</span> ffn_output</span><br></pre></td></tr></table></figure></p><p>其中用到的<code>dense_layer_2d</code>就是一个基本的神经网络$y=f(Wx+b)$, 其中$f()$是激活函数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_layer_2d</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                   output_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                   initializer,</span></span></span><br><span class="line"><span class="function"><span class="params">                   activation,</span></span></span><br><span class="line"><span class="function"><span class="params">                   num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   name=None)</span>:</span></span><br><span class="line">  <span class="string">"""A dense layer with 2D kernel.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: Float tensor with rank 3.</span></span><br><span class="line"><span class="string">    output_size: The size of output dimension.</span></span><br><span class="line"><span class="string">    initializer: Kernel initializer.</span></span><br><span class="line"><span class="string">    activation: Activation function.</span></span><br><span class="line"><span class="string">    num_attention_heads: number of attention head in attention layer.</span></span><br><span class="line"><span class="string">    name: The name scope of this layer.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float logits Tensor.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">del</span> num_attention_heads  <span class="comment"># unused</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor)</span><br><span class="line">  hidden_size = input_shape[<span class="number">2</span>]</span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">    w = tf.get_variable(</span><br><span class="line">        name=<span class="string">"kernel"</span>,</span><br><span class="line">        shape=[hidden_size, output_size],</span><br><span class="line">        initializer=initializer)</span><br><span class="line">    b = tf.get_variable(</span><br><span class="line">        name=<span class="string">"bias"</span>, shape=[output_size], initializer=tf.zeros_initializer)</span><br><span class="line">    ret = tf.einsum(<span class="string">"BFH,HO-&gt;BFO"</span>, input_tensor, w)</span><br><span class="line">    ret += b</span><br><span class="line">  <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> activation(ret)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></p><p>一个完整的transformer模块, 核心是由多个attention_ffn_block堆叠而成, 同时注意设定<code>reuse=tf.AUTO_REUSE</code>来实现Cross-layer parameter sharing, 设定<code>num_hidden_groups=1</code>就可以让所有层都共享参数.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_groups=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      inner_group_num=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=<span class="string">"gelu"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False)</span>:</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string">  This is almost an exact implementation of the original Transformer encoder.</span></span><br><span class="line"><span class="string">  See the original paper:</span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string">  Also see:</span></span><br><span class="line"><span class="string">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      seq_length], with 1 for positions that can be attended to and 0 in</span></span><br><span class="line"><span class="string">      positions that should not be.</span></span><br><span class="line"><span class="string">    hidden_size: int. Hidden size of the Transformer.</span></span><br><span class="line"><span class="string">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span></span><br><span class="line"><span class="string">    num_hidden_groups: int. Number of group for the hidden layers, parameters</span></span><br><span class="line"><span class="string">      in the same group are shared.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads in the Transformer.</span></span><br><span class="line"><span class="string">    intermediate_size: int. The size of the "intermediate" (a.k.a., feed</span></span><br><span class="line"><span class="string">      forward) layer.</span></span><br><span class="line"><span class="string">    inner_group_num: int, number of inner repetition of attention and ffn.</span></span><br><span class="line"><span class="string">    intermediate_act_fn: function. The non-linear activation function to apply</span></span><br><span class="line"><span class="string">      to the output of the intermediate/feed-forward layer.</span></span><br><span class="line"><span class="string">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: float. Dropout probability of the attention</span></span><br><span class="line"><span class="string">      probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the initializer (stddev of truncated</span></span><br><span class="line"><span class="string">      normal).</span></span><br><span class="line"><span class="string">    do_return_all_layers: Whether to also return all layers or just the final</span></span><br><span class="line"><span class="string">      layer.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = hidden_size // num_attention_heads</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    prev_output = dense_layer_2d(</span><br><span class="line">        input_tensor, hidden_size, create_initializer(initializer_range),</span><br><span class="line">        <span class="keyword">None</span>, name=<span class="string">"embedding_hidden_mapping_in"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    prev_output = input_tensor</span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"transformer"</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">      group_idx = int(layer_idx / num_hidden_layers * num_hidden_groups)</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"group_%d"</span> % group_idx):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">          layer_output = prev_output</span><br><span class="line">          <span class="keyword">for</span> inner_group_idx <span class="keyword">in</span> range(inner_group_num):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"inner_group_%d"</span> % inner_group_idx):</span><br><span class="line">              layer_output = attention_ffn_block(</span><br><span class="line">                  layer_output, hidden_size, attention_mask,</span><br><span class="line">                  num_attention_heads, attention_head_size,</span><br><span class="line">                  attention_probs_dropout_prob, intermediate_size,</span><br><span class="line">                  intermediate_act_fn, initializer_range, hidden_dropout_prob)</span><br><span class="line">              prev_output = layer_output</span><br><span class="line">              all_layer_outputs.append(layer_output)</span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    <span class="keyword">return</span> all_layer_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> all_layer_outputs[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure></p><h3 id="Factorized-Embedding实现"><a href="#Factorized-Embedding实现" class="headerlink" title="Factorized Embedding实现"></a>Factorized Embedding实现</h3><p>首先是需要embedding的因式分解, <code>embedding_lookup</code>输出的是<code>V x E matrix</code>, 其中<code>E</code>就是<code>embedding_size</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                     vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     word_embedding_name=<span class="string">"word_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings=False)</span>:</span></span><br><span class="line">  <span class="string">"""Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span></span><br><span class="line"><span class="string">      ids.</span></span><br><span class="line"><span class="string">    vocab_size: int. Size of the embedding vocabulary.</span></span><br><span class="line"><span class="string">    embedding_size: int. Width of the word embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Embedding initialization range.</span></span><br><span class="line"><span class="string">    word_embedding_name: string. Name of the embedding table.</span></span><br><span class="line"><span class="string">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span></span><br><span class="line"><span class="string">      embeddings. If False, use `tf.nn.embedding_lookup()`.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    flat_input_ids = tf.reshape(input_ids, [<span class="number">-1</span>])</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    output = tf.nn.embedding_lookup(embedding_table, input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:<span class="number">-1</span>] + [input_shape[<span class="number">-1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure></p><p>把embedding映射回隐层大小<code>E x H</code>, 依靠的是上面定义的<code>transformer_model</code>中的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    prev_output = dense_layer_2d(</span><br><span class="line">        input_tensor, hidden_size, create_initializer(initializer_range),</span><br><span class="line">        <span class="keyword">None</span>, name=<span class="string">"embedding_hidden_mapping_in"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="ALBERT模型搭建"><a href="#ALBERT模型搭建" class="headerlink" title="ALBERT模型搭建"></a>ALBERT模型搭建</h3><p>大体框架就是<code>embeddings</code>+<code>encoder</code>+<code>pooler output</code>, 其中<code>encoder</code>就是<code>transformer</code>blocks的堆叠:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlbertModel</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""BERT model ("Bidirectional Encoder Representations from Transformers").</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Constructor for AlbertModel.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `AlbertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to "bert".</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">"bert"</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"embeddings"</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        (self.word_embedding_output,</span><br><span class="line">         self.output_embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.embedding_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">"word_embeddings"</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.word_embedding_output,</span><br><span class="line">            use_token_type=<span class="keyword">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span><br><span class="line">            use_position_embeddings=<span class="keyword">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">"position_embeddings"</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=input_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_hidden_groups=config.num_hidden_groups,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            inner_group_num=config.inner_group_num,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[<span class="number">-1</span>]</span><br><span class="line">      <span class="comment"># The "pooler" converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"pooler"</span>):</span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure></p><p>再附上官网的API介绍：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Already been converted from strings into ids</span></span><br><span class="line">input_ids = tf.constant([[<span class="number">31</span>, <span class="number">51</span>, <span class="number">99</span>], [<span class="number">15</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br><span class="line">input_mask = tf.constant([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">token_type_ids = tf.constant([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line">config = modeling.AlbertConfig(vocab_size=<span class="number">32000</span>, hidden_size=<span class="number">512</span>,</span><br><span class="line">num_hidden_layers=<span class="number">8</span>, num_attention_heads=<span class="number">6</span>, intermediate_size=<span class="number">1024</span>)</span><br><span class="line">model = modeling.AlbertModel(config=config, is_training=<span class="keyword">True</span>,</span><br><span class="line">input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span><br><span class="line">label_embeddings = tf.get_variable(...)</span><br><span class="line">pooled_output = model.get_pooled_output()</span><br><span class="line">logits = tf.matmul(pooled_output, label_embeddings)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://github.com/google-research/ALBERT" target="_blank" rel="noopener">https://github.com/google-research/ALBERT</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;A-Lite-BERT&quot;&gt;&lt;a href=&quot;#A-Lite-BERT&quot; class=&quot;headerlink&quot; title=&quot;A Lite BERT&quot;&gt;&lt;/a&gt;A Lite BERT&lt;/h3&gt;&lt;p&gt;BERT(Devlin et al., 2019)的参数很多, 模型很大, 内存消耗很大, 在分布式计算中的通信开销很大.&lt;/p&gt;
&lt;p&gt;但是BERT的高内存消耗边际收益并不高, 如果继续增大BERT-large这种大模型的隐含层大小, 模型效果不升反降.&lt;/p&gt;
&lt;p&gt;针对这些问题, 启发于mobilenet, Alert使用了两种减少参数的方法来降低模型大小和提高训练速度, 分别是Factorized embedding parameterization和Cross-layer parameter sharing. 这些设计让ALBERT增加参数大小的边界收益远远大于BERT.&lt;/p&gt;
&lt;p&gt;除此之外, 在句子关系任务上抛弃了bert的&lt;code&gt;nsp&lt;/code&gt;任务, 改为&lt;code&gt;sop&lt;/code&gt;任务.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Transformer &amp; Self-Attention (多头)自注意力编码</title>
    <link href="http://shukebeta.me/NLP-attention-03-self-attention/"/>
    <id>http://shukebeta.me/NLP-attention-03-self-attention/</id>
    <published>2019-11-29T16:00:00.000Z</published>
    <updated>2020-01-02T10:00:08.500Z</updated>
    
    <content type="html"><![CDATA[<p>注意力机制的原理是计算query和每个key之间的相关性$\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。<br><a id="more"></a></p><p>注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is all you need</a>提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。</p><p>近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。</p><p>因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。</p><p>所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用<strong>self-attention 自注意力机制</strong>(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时<code>Query</code>, <code>Key</code>和<code>Value</code>都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。</p><h3 id="Transformer总体架构"><a href="#Transformer总体架构" class="headerlink" title="Transformer总体架构"></a>Transformer总体架构</h3><p>与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:</p><ol><li>Transformer使用注意力来编码(不需要LSTM/CNN之类的)。</li><li>引入自注意力机制</li><li>Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。</li></ol><p>Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。</p><p><img src="/images/transform20fps.gif" alt="" title="image from: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"></p><p>Transformer的编码阶段概括起来就是：</p><ol><li>首先为每个单词生成初始表达或embeddings。这些由空心圆表示。</li><li>然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。</li></ol><p>Encoder的self-attention中, 所有<code>Key</code>, <code>Value</code>和<code>Query</code>都来自同一位置, 即上一层encoder的输出。</p><p>解码器类似，所有<code>Key</code>, <code>Value</code>和<code>Query</code>都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前<code>query</code>位置之前的部分。生成<code>Query</code>时, 不仅关注前一步的输出，还参考编码器的最后一层输出。</p><p><img src="/images/transformer.png" alt="" title="单层编码器（左）和解码器（右），由 N = 6 个相同的层构建。"><br><code>N = 6</code>, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。</p><p>编码器:</p><ol><li>Stage 1 - 输入编码: 序列的顺序信息是非常重要的。由于没有循环，也没有卷积，因此使用“位置编码”表示序列中每个标记的绝对（或相对）位置的信息。<ul><li>positional encodings $\oplus$ embedded input</li></ul></li><li>Stage 2 – Multi-head self-attention 和 Stage 3 – position-wise FFN. 两个阶段都是用来残差连接, 接着正则化输出层</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Stage1_out = Embedding512 + TokenPositionEncoding512</span><br><span class="line">Stage2_out = layer_normalization(multihead_attention(Stage1_out) + Stage1_out)</span><br><span class="line">Stage3_out = layer_normalization(FFN(Stage2_out) + Stage2_out)</span><br><span class="line"></span><br><span class="line">out_enc = Stage3_out</span><br></pre></td></tr></table></figure><p>解码器的架构类似，但它在第3阶段采用了附加层, 在输出层上的 mask multi-head attention:</p><ol><li><p>Stage 1 – 输入解码: 输入 output embedding，偏移一个位置以确保对位置<code>i</code>的预测仅取决于<code>&lt; i</code>的位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_right_3d</span><span class="params">(x, pad_value=None)</span>:</span></span><br><span class="line">  <span class="string">"""Shift the second dimension of x right by one."""</span></span><br><span class="line">  <span class="keyword">if</span> pad_value <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">    shifted_targets = tf.pad(x, [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]])[:, :<span class="number">-1</span>, :]</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    shifted_targets = tf.concat([pad_value, x], axis=<span class="number">1</span>)[:, :<span class="number">-1</span>, :]</span><br><span class="line">  <span class="keyword">return</span> shifted_targets</span><br></pre></td></tr></table></figure></li><li><p>Stage 2 - Masked Multi-head self-attention: 需要有一个mask来防止当前位置<code>i</code>的生成任务看到后续<code>&gt; i</code>位置的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用pytorch版本的教程中提供的范例</span></span><br><span class="line"><span class="comment"># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # The attention mask shows the position each tgt word (row) is allowed to look at (column).</span></span><br><span class="line"><span class="comment"># Words are blocked for attending to future words during training.</span></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li></ol><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_31_0.png" alt=""></p><p>阶段2,3和4同样使用了残差连接，然后在输出使用归一化层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Stage1_out = OutputEmbedding512 + TokenPositionEncoding512</span><br><span class="line"></span><br><span class="line">Stage2_Mask = masked_multihead_attention(Stage1_out)</span><br><span class="line">Stage2_Norm1 = layer_normalization(Stage2_Mask) + Stage1_out</span><br><span class="line">Stage2_Multi = multihead_attention(Stage2_Norm1 + out_enc) +  Stage2_Norm1</span><br><span class="line">Stage2_Norm2 = layer_normalization(Stage2_Multi) + Stage2_Multi</span><br><span class="line"></span><br><span class="line">Stage3_FNN = FNN(Stage2_Norm2)</span><br><span class="line">Stage3_Norm = layer_normalization(Stage3_FNN) + Stage2_Norm2</span><br><span class="line"></span><br><span class="line">out_dec = Stage3_Norm</span><br></pre></td></tr></table></figure></p><p>可以利用开源的<a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py" target="_blank" rel="noopener">Tensor2Tensor</a>，通过调用几个命令来训练Transformer网络进行翻译和解析。</p><p>通过Self Attention对比Attention有什么增益呢？可以看到，自注意力算法可以捕获同一个句子中单词之间的语义特征, 比如共指消解（coreference resolution），例如句子中的单词“it”可以根据上下文引用句子的不同名词。除此之外, 理论上也可以捕捉一些语法特征. <img src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png" alt="" title="Co-reference resolution. 两边的&quot;it&quot;指向不同的词. Adopted from Google Blog."></p><p>其实在LSTM_encoder-LSTM_decoder架构上的Attention也可以做到相同的操作, 但效果却不太好. 问题可能在于此时的Attention处理的不是纯粹的一个个序列编码, 而是经过LSTM(复杂的门控记忆与遗忘)编码后的包含前面时间步输入信息的一个个序列编码,  这个导致Attention的软寻址难度增大. 而现在是2019年, 几乎主流的文本编码方案都转投Transformer了, 可见单纯利用self-attention编码其实效率更高.</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Vaswani, 2017</a>明确定义了使用的注意力算法$$\begin{eqnarray} Attention (Q,K,V) = softmax \Big( \frac{QK^T}{\sqrt{d_k}} \Big) V \end{eqnarray},$$其中$\boldsymbol{Q}\in\mathbb{R}^{n\times d_k}, \boldsymbol{K}\in\mathbb{R}^{m\times d_k}, \boldsymbol{V}\in\mathbb{R}^{m\times d_v}$. 这就是传统的Scaled Dot-Product Attention, 把这个Attention理解为一个神经网络层，将$n\times d_k$的序列$Q$编码成了一个新的$n\times d_v$的序列。因为对于较大的$d_k$，内积会数量级地放大, 太大的话softmax可能会被推到梯度消失区域, softmax后就非0即1(那就是hardmax), 所以$q \cdot  k = \sum_{i=1}^{d_k}q_i k_i$按照比例因子$\sqrt{d_k}$缩放.</p><p>BERT/ALBERT中的点积attention实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dot_product_attention from bert implementation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dot_product_attention</span><span class="params">(q, k, v, bias, dropout_rate=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Dot-product attention.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    q: Tensor with shape [..., length_q, depth_k].</span></span><br><span class="line"><span class="string">    k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must</span></span><br><span class="line"><span class="string">      match with q.</span></span><br><span class="line"><span class="string">    v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must</span></span><br><span class="line"><span class="string">      match with q.</span></span><br><span class="line"><span class="string">    bias: bias Tensor (see attention_bias())</span></span><br><span class="line"><span class="string">    dropout_rate: a float.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    Tensor with shape [..., length_q, depth_v].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  logits = tf.matmul(q, k, transpose_b=<span class="keyword">True</span>)  <span class="comment"># [..., length_q, length_kv]</span></span><br><span class="line">  logits = tf.multiply(logits, <span class="number">1.0</span> / math.sqrt(float(get_shape_list(q)[<span class="number">-1</span>])))</span><br><span class="line">  <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, T]</span></span><br><span class="line">    from_shape = get_shape_list(q)</span><br><span class="line">    <span class="keyword">if</span> len(from_shape) == <span class="number">4</span>:</span><br><span class="line">      broadcast_ones = tf.ones([from_shape[<span class="number">0</span>], <span class="number">1</span>, from_shape[<span class="number">2</span>], <span class="number">1</span>], tf.float32)</span><br><span class="line">    <span class="keyword">elif</span> len(from_shape) == <span class="number">5</span>:</span><br><span class="line">      <span class="comment"># from_shape = [B, N, Block_num, block_size, depth]#</span></span><br><span class="line">      broadcast_ones = tf.ones([from_shape[<span class="number">0</span>], <span class="number">1</span>, from_shape[<span class="number">2</span>], from_shape[<span class="number">3</span>],</span><br><span class="line">                                <span class="number">1</span>], tf.float32)</span><br><span class="line"></span><br><span class="line">    bias = tf.matmul(broadcast_ones,</span><br><span class="line">                     tf.cast(bias, tf.float32), transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - bias) * <span class="number">-10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">    logits += adder</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    adder = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">  attention_probs = tf.nn.softmax(logits, name=<span class="string">"attention_probs"</span>)</span><br><span class="line">  attention_probs = dropout(attention_probs, dropout_rate)</span><br><span class="line">  <span class="keyword">return</span> tf.matmul(attention_probs, v)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用pytorch版本的教程中提供的范例</span></span><br><span class="line"><span class="comment"># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">  <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">  d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">  scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">           / math.sqrt(d_k)</span><br><span class="line">  <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">      scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">  p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">  <span class="comment"># (Dropout described below)</span></span><br><span class="line">  p_attn = F.dropout(p_attn, p=dropout)</span><br><span class="line">  <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p>这只是注意力的一种形式，还有其他比如query跟key的运算方式是拼接后再内积一个参数向量，权重也不一定要归一化，等等。</p><h3 id="Self-Attention-SA"><a href="#Self-Attention-SA" class="headerlink" title="Self-Attention (SA)"></a>Self-Attention (SA)</h3><p>在实际的应用中, 不同的场景的$Q,K,V$是不一样的, 如果是SQuAD的话，$Q$是文章的向量序列，$K=V$为问题的向量序列，输出就是Aligned Question Embedding。</p><p>Google所说的自注意力(SA), 就是$Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$, 通过在序列自身做Attention，寻找序列自身内部的联系。Google论文的主要贡献之一是它表明了SA在序列编码部分是相当重要的，甚至可以替代传统的RNN(LSTM), CNN, 而之前关于Seq2Seq的研究基本都是关注如何把注意力机制用在解码部分。</p><p>编码时，自注意力层处理来自相同位置的输入$queries, keys, value$，即编码器前一层的输出。编码器中的每个位置都可以关注前一层的所有位置.</p><p>在解码器中，SA层使每个位置能够关注解码器中当前及之前的所有位置。为了保持 auto-regressive 属性，需要阻止解码器中的向左信息流, 所以要在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值.</p><p>作者使用SA层而不是CNN或RNN层的动机是:</p><ol><li>最小化每层的总计算复杂度: SA层通过$O(1)$数量的序列操作连接所有位置. ($O(n)$  in RNN)</li><li>最大化可并行化计算：对于序列长度$n$ &lt; representation dimensionality $d$（对于SOTA序列表达模型，如word-piece, byte-pair）。对于非常长的序列$n &gt; d$, SA可以仅考虑以相应输出位置为中心的输入序列中的某个大小$r$的邻域，从而将最大路径长度增加到$O(n/r)$</li><li>最小化由不同类型层组成的网络中任意两个输入和输出位置之间的最大路径长度。任何输入和输出序列中的位置组合之间的路径越短，越容易学习长距离依赖。</li></ol><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p>Transformer的SA将关联输入和输出序列中的（特别是远程）位置的计算量减少到$O(1)$。然而，这是以降低有效分辨率为代价的，因为注意力加权位置被平均了。为了弥补这种损失, 文章提出了 Multi-head Attention:<img src="/images/multi_head_attention.png" alt="" title="Multi-Head Attention consists of h attention layers running in parallel. image from https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#positional-encoding-pe"></p><ul><li>$h=8$ attention layers (“heads”): 将key $K$ 和 query $Q$ 线性投影到 $d_k$ 维度, 将value $V$ 投影到$d_v$维度, (线性投影的目的是减少维度) $$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\dots,h$$ 投影是参数矩阵$W^Q_i, W^K_i\in\mathbb{R}^{d_{model}\times d_k}, W^V_i\in\mathbb{R}^{d_{model}\times d_v}$ $d_k=d_v=d_{model}/h = 64$</li><li>每层并行地应用 scaled-dot attention(用不同的线性变换), 得到$d_v$维度的输出</li><li>把每一层的输出拼接在一起 $Concat(head_1,\dots,head_h)$</li><li>再线性变换上一步的拼接向量$MultiHeadAttention(Q,K,V) = Concat(head_1,\dots,head_h) W^O$, where $W^0\in\mathbb{R}^{d_{hd_v}\times d_{model}}$</li></ul><p>因为Transformer只是把原来$d_{model}$维度的注意力函数计算并行分割为$h$个独立的$d_{model}/h$维度的head, 所以计算量相差不大.</p><p>BERT/ALBERT中的multi-head attention层实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_layer</span><span class="params">(from_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    key_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    value_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    from_seq_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_seq_length=None)</span>:</span></span><br><span class="line">  <span class="string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      from_width].</span></span><br><span class="line"><span class="string">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span></span><br><span class="line"><span class="string">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span></span><br><span class="line"><span class="string">      attention scores will effectively be set to -infinity for any positions in</span></span><br><span class="line"><span class="string">      the mask that are 0, and will be unchanged for positions that are 1.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads.</span></span><br><span class="line"><span class="string">    query_act: (optional) Activation function for the query transform.</span></span><br><span class="line"><span class="string">    key_act: (optional) Activation function for the key transform.</span></span><br><span class="line"><span class="string">    value_act: (optional) Activation function for the value transform.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span></span><br><span class="line"><span class="string">      attention probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initializer.</span></span><br><span class="line"><span class="string">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor` and `to_tensor`.</span></span><br><span class="line"><span class="string">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor`.</span></span><br><span class="line"><span class="string">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `to_tensor`.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length, num_attention_heads,</span></span><br><span class="line"><span class="string">      size_per_head].</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  size_per_head = int(from_shape[<span class="number">2</span>]/num_attention_heads)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) != len(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> len(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">"When passing in rank 2 tensors to attention_layer, the values "</span></span><br><span class="line">          <span class="string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span></span><br><span class="line">          <span class="string">"must all be specified."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B, F, N, H]</span></span><br><span class="line">  q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head,</span><br><span class="line">                     create_initializer(initializer_range), query_act, <span class="string">"query"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, T, N, H]</span></span><br><span class="line">  k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,</span><br><span class="line">                     create_initializer(initializer_range), key_act, <span class="string">"key"</span>)</span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,</span><br><span class="line">                     create_initializer(initializer_range), value_act, <span class="string">"value"</span>)</span><br><span class="line">  q = tf.transpose(q, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">  k = tf.transpose(k, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">  v = tf.transpose(v, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    attention_mask = tf.reshape(</span><br><span class="line">        attention_mask, [batch_size, <span class="number">1</span>, to_seq_length, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 'new_embeddings = [B, N, F, H]'</span></span><br><span class="line">  new_embeddings = dot_product_attention(q, k, v, attention_mask,</span><br><span class="line">                                         attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.transpose(new_embeddings, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><p>可以看到<code>k</code>和<code>v</code>都是<code>to_tensor</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用pytorch版本的教程中提供的范例</span></span><br><span class="line"><span class="comment"># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">    super(MultiHeadedAttention, self).__init__()</span><br><span class="line">    <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">    <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">    self.d_k = d_model // h</span><br><span class="line">    self.h = h</span><br><span class="line">    self.p = dropout</span><br><span class="line">    self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">    self.attn = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">      <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">      mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">    query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                         <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">    x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">    x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">    <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><p>NMT中Transformer以三种不同的方式使用Multi-head Attention：</p><ol><li>在<code>encoder-decoder attention</code>层中，<code>queries</code>来自前一层decoder层，并且 memory keys and values 来自encoder的输出。这让decoder的每个位置都可以注意到输入序列的所有位置。这其实还原了典型的seq2seq模型里常用的编码器 - 解码器注意力机制（例如<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Bahdanau et al., 2014</a>或Conv2S2）。</li><li><p>编码器本身也包含了self-attention layers。在self-attention layers中，所有 keys, values and queries 来自相同的位置，在这里是编码器中前一层的输出。这样，编码器的每个位置都可以注意到前一层的所有位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"attention_1"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">      attention_output = attention_layer(</span><br><span class="line">          from_tensor=layer_input,</span><br><span class="line">          to_tensor=layer_input,</span><br><span class="line">          attention_mask=attention_mask,</span><br><span class="line">          num_attention_heads=num_attention_heads,</span><br><span class="line">          attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">          initializer_range=initializer_range)</span><br></pre></td></tr></table></figure></li><li><p>类似地，解码器中的 self-attention layers 允许解码器的每个位置注意到解码器中包括该位置在内的所有前面的位置（有mask屏蔽了后面的位置）。需要阻止解码器中的向左信息流以保持<code>自回归</code>属性(auto-regressive 可以简单理解为时序序列的特性, 只能从左到右, 从过去到未来)。我们通过在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值来维持该特性。</p></li></ol><h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>在编码器和解码器中，每个层都包含一个全连接的前馈网络(FFN)，FFN 分别应用于每个位置，使用相同的两个线性变换和一个ReLU $$FFN(x) = max(0, xW_1+b_1) W_2 + b_2$$<br>虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。它的工作方式类似于两个内核大小为1的卷积层. 输入/输出维度是$d_{model}=512$, 内层的维度$d_{ff} = 2048$.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用pytorch版本的教程中提供的范例</span></span><br><span class="line"><span class="comment"># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="string">"Implements FFN equation."</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">    <span class="comment"># Torch linears have a `b` by default.</span></span><br><span class="line">    self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">    self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">    self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure></p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>在解码时序信息时，LSTM模型通过时间步的概念以输入/输出流一次一个的形式编码的. 而Transformer选择把时序编码为正弦波。这些信号作为额外的信息加入到输入和输出中以表达时序信息.</p><p>这种编码使模型能够感知到当前正在处理的是输入（或输出）序列的哪个部分。位置编码可以学习或者使用固定参数。作者进行了测试（PPL，BLEU），显示两种方式表现相似。文中作者选择使用固定的位置编码参数:$$ \begin{eqnarray} PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \end{eqnarray} $$<br>$$ \begin{eqnarray} PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\end{eqnarray} $$ 其中$pos$是位置，$i$是维度。</p><p>也就是说，位置编码的每个维度对应于正弦余弦曲线的拼接。波长形成从2π到10000⋅2π的几何级数。选择这个函数，是因为假设它能让模型容易地学习相对位置，因为对于任意固定偏移$k$，$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用pytorch版本的教程中提供的范例</span></span><br><span class="line"><span class="comment"># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="string">"Implement the PE function."</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">    super(PositionalEncoding, self).__init__()</span><br><span class="line">    self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">    pe = torch.zeros(max_len, d_model)</span><br><span class="line">    position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                         -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">    pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">    pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">    pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure></p><p>位置编码将根据位置添加正弦余弦波。每个维度的波的频率和偏移是不同的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)))</span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure></p><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_49_0.png" alt=""><br>直观的理解是，将这些值添加到embedding中，一旦它们被投影到$Q / K / V$向量和dot product attention中，就给embedding向量之间提供了有意义的相对距离。</p><p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png" alt="" title="A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That&#39;s because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They&#39;re then concatenated to form each of the positional encoding vectors. image from: https://jalammar.github.io/illustrated-transformer/"><br><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png" alt="" title="A real example of positional encoding with a toy embedding size of 4, image from: https://jalammar.github.io/illustrated-transformer/"></p><h3 id="Shared-Weight-Embeddings-and-Softmax"><a href="#Shared-Weight-Embeddings-and-Softmax" class="headerlink" title="Shared-Weight Embeddings and Softmax"></a>Shared-Weight Embeddings and Softmax</h3><p>与其他序列转导模型类似，使用可学习的Embeddings将 input tokens and output tokens 转换为维度$d_{model}$的向量。通过线性变换和softmax函数将解码器的输出向量转换为预测的token概率。在Transformer模型中，两个嵌入层和pre-softmax线性变换之间共享相同的权重矩阵，在Embeddings层中，将权重乘以$\sqrt{d_{\text{model}}}$. 这些都是当前主流的操作。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用pytorch版本的教程中提供的范例</span></span><br><span class="line"><span class="comment"># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">    super(Embeddings, self).__init__()</span><br><span class="line">    self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">    self.d_model = d_model</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure></p><h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>作者已经进行了一系列测试（论文表3），其中他们讨论N = 6层的建议，模型大小为512，基于h = 8个heads，键值维度为64，使用100K步。</p><p>还指出，由于模型质量随着$d_k$（行B）的减小而降低，因此可以进一步优化点积兼容性功能。</p><p>其声称提出的固定正弦位置编码，与学习到的位置编码相比，产生几乎相等的分数。</p><h3 id="算法适合哪些类型的问题？"><a href="#算法适合哪些类型的问题？" class="headerlink" title="算法适合哪些类型的问题？"></a>算法适合哪些类型的问题？</h3><ul><li>序列转导（语言翻译）</li><li>语法选区解析的经典语言分析任务 syntactic constituency parsing</li><li>共指消解 coreference resolution</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">https://research.googleblog.com/2017/08/transformer-novel-neural-network.html</a><br><a href="https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html" target="_blank" rel="noopener">https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html</a><br><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/" target="_blank" rel="noopener">https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/</a><br><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;注意力机制的原理是计算query和每个key之间的相关性$\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Attention" scheme="http://shukebeta.me/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>利用bert进行迁移学习</title>
    <link href="http://shukebeta.me/NLP-transfer-learning-with-bert/"/>
    <id>http://shukebeta.me/NLP-transfer-learning-with-bert/</id>
    <published>2019-08-11T16:00:00.000Z</published>
    <updated>2019-12-23T08:59:26.207Z</updated>
    
    <content type="html"><![CDATA[<h3 id="NLP任务的难点"><a href="#NLP任务的难点" class="headerlink" title="NLP任务的难点"></a>NLP任务的难点</h3><p>不像图像的普适性, 语言本身有其多样性, 如语境的偏移, 背景的变化, 人与人间的分歧, 这导致以下问题:</p><ol><li>有标注数据的通用性低</li><li>标注数据质量不稳定</li><li>现实世界的语言和使用场景不断更新, 导致模型的维护更新换代成本极高</li><li>…<a id="more"></a></li></ol><p>为了应对NLP的难点, 需要充分利用各种可用的监督信号，包括但不限于传统监督学习（supervision），自监督学习（self-supervised），弱监督(weak supervision)，迁移学习（transfer learning），多任务学习（multi-task learning, MTL）。</p><blockquote><p>Near-term improvements in NLP will be mostly about making clever use of “free” data.</p></blockquote><h3 id="语言模型-经典的自监督学习模型"><a href="#语言模型-经典的自监督学习模型" class="headerlink" title="语言模型 - 经典的自监督学习模型"></a>语言模型 - 经典的自监督学习模型</h3><p>Lecun有给自监督学习下定义，但我个人对自监督的理解是，基于数据本身进行学习，让模型学习到数据隐含的特征。</p><p>比如语言模型的根据前文预测下一个单词。</p><p>最近的BERT丰富了玩法，提出了Mask language model，就是通过上下文预测掩码位置的单词，作为其核心学习任务；BERT的训练过程还应用了多任务学习，把 next sentence prediction 也作为任务之一一起学习。</p><p>目前除了语言模型和句模型(<code>next sentence</code>)，是否还有其他任务?</p><blockquote><p>Baidu ERNIE: 引入了论坛对话类数据，利用 DLM（Dialogue Language Model）建模 Query-Response 对话结构，将对话 Pair 对作为输入，引入 Dialogue Embedding 标识对话的角色，利用 Dialogue Response Loss 学习对话的隐式关系。</p></blockquote><h3 id="ELMo-vs-GPT-vs-BERT"><a href="#ELMo-vs-GPT-vs-BERT" class="headerlink" title="ELMo vs GPT vs BERT"></a>ELMo vs GPT vs BERT</h3><p>经典Word2vec表达是context free的，<code>open a bank account</code>和<code>on the river bank</code>的<code>bank</code>共用一个向量值<code>[0.3, 0.2, -0.8, …]</code>. 如指公司的<code>苹果</code>和指水果的<code>苹果</code>共用一个向量.</p><p>解决方案：在文本语料中训练上下文表达<code>contextual representations</code></p><p>而 ELMo, GPT, 和 BERT 都着眼于<code>contextual representations</code></p><ul><li>ELMo : Deep Contextual Word Embeddings, 训练独立的<code>left-to-right</code>和<code>right-to-left</code>的LMs, 外加一个<code>Word Embedding</code>层, 作为预训练的词向量使用</li><li>OpenAI GPT : Improving Language Understanding by Generative Pre-Training. 使用 <code>left-to-right</code> Transformer LM, 然后在下游任务中fine-tune</li><li>BERT : Bidirectional Encoder Representations from Transformers<br><img src="https://user-images.githubusercontent.com/7529838/47401354-f1a6f480-d77b-11e8-8f3d-94ed277de43f.png" alt=""></li></ul><p>但是, 为何2018年之前类似ELMo的<code>contextual representations</code>并不流行？</p><p>因为好的预训练结果比有监督训练代价高1000倍甚至100,000倍。2013年微调好的二层512维度的LSTM sentiment analysis 有 80% 准确度, 训练时间 8 小时. 同期的相同结构的预训练模型需要训练一周, 准确率稍微好点, 80.5%.</p><h3 id="迁移学习的两种思路-Feature-based-和-Fine-tune-based"><a href="#迁移学习的两种思路-Feature-based-和-Fine-tune-based" class="headerlink" title="迁移学习的两种思路: Feature based 和 Fine-tune  based"></a>迁移学习的两种思路: Feature based 和 Fine-tune  based</h3><p>ELMO区分不同同词不同意的方法是通过它的三层向量的加权组合(concat or sum whatever), 权重可以在下游任务中学习调整.</p><p>而GPT和BERT是通过在下游任务中fine-tune模型参数, 同时也利用了transformer的self-attention机制解决共指消解 <img src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png" alt=""></p><p>而这三个模型是一个不断进化的过程:</p><ol><li>ELMO 独立地训练前后向LSTM, 每一个位置只能直接接收其左右相邻位置的信息, 而且因为实践上LSTM cell 能够记忆的距离很有限(能够记忆的信息也很有限), 这导致ELMO的全局语境理解能力很有限.</li><li>GPT 是从左到右</li><li>BERT 放弃了”预测下一个词”的传统LM任务, 改用Mask-LM任务.</li></ol><p><code>BERT</code>模型学习的应该不仅仅是<code>contextual embeddings</code>：<br>预测缺失的单词（或下一个单词）需要学习许多类型的语义理解features: 语法，语义，语用，共指等.<br>这说明预训练模型其实远远大于它所需要解决的任何特定下游任务</p><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>迁移学习的主流思路是知识共享, 让模型在一种较为通用的数据上<strong>预训练</strong>, 然后把预训练的模型迁移到下游的具体任务中.</p><p>迁移学习在图像领域大获成功（ImageNet + <code>resnet</code>），解决了<strong>分类</strong>这一图像领域的瓶颈。</p><p>近年来涌现出<code>ULMFit</code>, <code>ELMO</code>，<code>GPT</code>, <code>BERT</code>这些优秀的预训练模型，但没有CV领域那么耀眼。主要原因是NLP目前没有单个明确的瓶颈，</p><ol><li>NLP需要多种多样的推理：逻辑，语言语义，情感，和视觉。</li><li>NLP要求长期短期结合的记忆</li></ol><p>比较综合的语言理解任务是GLUE。</p><p>BERT，GPT-2 等算法指明了一条可行的 NLP 在实际工业应用的可行路径：</p><ol><li>预训练：利用超大规模无监督数据预训练神经网络模型</li><li>(可选) 知识注入, 加入知识图谱的结构化信息, 如基于BERT的ERNIE</li><li>知识迁移，二个思路：<ul><li>微调 Fine-tune</li><li>单任务/多任务学习</li></ul></li></ol><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>预训练阶段的核心是什么？</p><ol><li><code>resnet</code>, <code>BERT</code> 和 <code>GPT-2</code> 告诉我们: <strong>更大的数据规模，更多样性的数据，更高的数据质量</strong>。这三点的尺度上限都接近无穷大，所以天花板很高，未来模型的性能还有提升空间。</li><li>针对数据量大和多样性，我们有两种解决思路,<ul><li>预训练阶段需要自监督或者无监督的任务，显而易见的任务是<strong>语言模型</strong>, ELMo, GPT, 和 BERT 都用到了这个任务.</li><li>使用弱监督(远程监督)</li></ul></li></ol><h3 id="知识注入"><a href="#知识注入" class="headerlink" title="知识注入"></a>知识注入</h3><p>百度的<code>ERNIE</code>的做法是: 基于百度的词库, 把<code>BERT</code>中对token level 的 mask 改进为 对 word level 的 mask.</p><ol><li>对于每一个序列, 我们需要进行 word level 的标记, 来区分各个 token 是否属于同一个词.</li><li>对序列进行掩码时, 不再是随机选择 token, 而是选择词</li></ol><h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>与单独训练模型相比，多任务学习在使用shared representation的同时并行地学习任务。通过shared representation在任务之间传递知识，可以提高特定任务模型的学习效率和预测准确性。</p><p>有两种MT思路：</p><ol><li>Hard parameter sharing：不同任务共享底层的神经网络层，但各个任务有自己特定任务的output layer。同时学习的任务越多，模型底层就越能找到捕捉所有任务的表达，而对单个任务过度拟合的可能性就越小。</li><li>Soft parameter sharing：每个任务有自己的模型自己的参数，然后对各个模型的参数之间的距离进行正则化，以鼓励参数趋近。</li></ol><p>Hard parameter sharing的训练，目前至少有两种方式。</p><ol><li>交替地优化每个任务特定的<code>task_loss[k]</code>. 这种方法不需要各个任务的训练数据有任何对齐关联</li><li>联合优化<code>total_loss=Σ(task_loss[k])</code>。 这个方法要求各个任务的batch训练数据相同或者有key来对齐</li></ol><p>除此之外, 第二种方法还方便我们为每个任务添加自适应权重(adaptive weight)，以获得更多task-sensitive learning。<br><a href="https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/3f9ea6de.png" target="_blank" rel="noopener"></a></p><h3 id="弱（远程）监督"><a href="#弱（远程）监督" class="headerlink" title="弱（远程）监督"></a>弱（远程）监督</h3><p>Snorkel MeTaL<br><img src="https://cdn-images-1.medium.com/max/800/0*IuDR-YEFctSuyUB0" alt=""><br>In Snorkel, the heuristics are called Labeling Functions (LFs). Here are some common types of LFs:</p><ul><li>Hard-coded heuristics: usually regular expressions (regexes)</li><li>Syntactics: for instance, Spacy’s dependency trees</li><li>Distant supervision: external knowledge bases</li><li>Noisy manual labels: crowdsourcing</li><li>External models: other models with useful signals</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://towardsdatascience.com/a-technique-for-building-nlp-classifiers-efficiently-with-transfer-learning-and-weak-supervision-a8e2f21ca9c8" target="_blank" rel="noopener">Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision</a></li><li>Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale</li><li>Data Programming: Creating Large Training Sets, Quickly</li><li>Improving Language Understanding by Generative Pre-Training</li><li><a href="https://github.com/kweonwooj/papers/issues/114" target="_blank" rel="noopener">https://github.com/kweonwooj/papers/issues/114</a></li><li><a href="https://dawn.cs.stanford.edu/2019/03/22/glue/#fn:practitioners" target="_blank" rel="noopener">Massive Multi-Task Learning with Snorkel MeTaL: Bringing More Supervision to Bear</a></li><li><a href="http://ruder.io/multi-task/index.html" target="_blank" rel="noopener">An Overview of Multi-Task Learning in Deep Neural Networks</a></li><li>Training Complex Models with Multi-Task Weak Supervision</li><li><a href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" target="_blank" rel="noopener">https://nlp.stanford.edu/seminar/details/jdevlin.pdf</a></li><li><a href="https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/" target="_blank" rel="noopener">Get 10x Speedup in Tensorflow Multi-Task Learning using Python Multiprocessing</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;NLP任务的难点&quot;&gt;&lt;a href=&quot;#NLP任务的难点&quot; class=&quot;headerlink&quot; title=&quot;NLP任务的难点&quot;&gt;&lt;/a&gt;NLP任务的难点&lt;/h3&gt;&lt;p&gt;不像图像的普适性, 语言本身有其多样性, 如语境的偏移, 背景的变化, 人与人间的分歧, 这导致以下问题:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有标注数据的通用性低&lt;/li&gt;
&lt;li&gt;标注数据质量不稳定&lt;/li&gt;
&lt;li&gt;现实世界的语言和使用场景不断更新, 导致模型的维护更新换代成本极高&lt;/li&gt;
&lt;li&gt;…
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>从头理解注意力机制</title>
    <link href="http://shukebeta.me/NLP-attention-01/"/>
    <id>http://shukebeta.me/NLP-attention-01/</id>
    <published>2019-07-29T16:00:00.000Z</published>
    <updated>2019-11-19T06:46:02.318Z</updated>
    
    <content type="html"><![CDATA[<h3 id="注意力机制如何起源的"><a href="#注意力机制如何起源的" class="headerlink" title="注意力机制如何起源的"></a>注意力机制如何起源的</h3><p>神经网络中的注意力机制启发自人类的<strong>视觉注意力机制</strong>，能够（高分辨率地）聚焦于图像中需要重点关注的目标区域（节省大脑资源），同时（低分辨率地）感知周围的图像，然后随着时间的推移调整焦点（状态调整）。</p><p>在神经网路中，注意力机制是为了解决什么问题？<br><a id="more"></a><br>在深度学习还没流行的时候, 传统的算法早已应用了注意力机制的思想. </p><p>比如一个非线性回归问题，对于代表位置的输入变量${x_1, …, x_m}$ 和 代表位置对应的输出值${y_1, …, y_m}$, 如何预测新的$x_n$对应的输出? Baseline 就是求均值, $$\frac{1}{m} \sum_{i=1}^{m} y_i$$ 当然更好的方案(Watson, Nadaraya, 1964)是根据不同的输入$x_i$给与$y_i$不同的权重, $$y = \sum_{i=1}^{m} \alpha(x, x_i) y_i $$</p><p>这里$x$代表一个新的输入(作为<strong>query</strong>), 根据$x$和已有的位置$x_i$(作为<strong>key</strong>)进行某种运算, 得到$x_i$对应的输出$y_i$(作为<strong>value</strong>)的权重. 如果每一个权重都是一个Guassians分布, 并正则化, 则一个<strong>加权的回归预测模型</strong>就是:$$f(x) = \sum_i y_i \frac{k(x_i, x)}{\sum_j k(x_j, x)}$$</p><p>这个算法的”深度学习”版本, 就是其权重是通过优化器(如sgd)学习得来, 并且把平均运算改为<strong>加权池化(weighted pooling)</strong>.</p><h3 id="如何简单直观地理解注意力机制"><a href="#如何简单直观地理解注意力机制" class="headerlink" title="如何简单直观地理解注意力机制"></a>如何简单直观地理解注意力机制</h3><p>虽然注意力机制一开始被应用于图像识别领域，但是后来推广到神经机器翻译(NMT)中(<code>Seq2Seq for Machine Translation, Sutskever, Vinyals, Le ‘14</code>). NMT也是注意力机制在NLP领域最早最成功的应用之一. </p><p><img src="http://www.wildml.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-10.39.06-AM.png" alt="" title="一个典型的seq2seq(encoder-decoder)翻译模型, 向量h表示编码器的内部状态"><br>在上图中，<code>Echt</code>，<code>Dicke</code>和<code>Kiste</code>词被送到编码器中，并且在特殊信号（未显示）之后，解码器开始生成翻译后的句子。解码器不断生成单词，直到产生特殊的句子结尾标记(如<code>&lt;eos&gt;</code>)。也就是说解码器仅根据最后一个隐含状态$h_3$来生成序列. 假如这个句子很短, 那么效果其实是很好的. </p><p>不过对于比较长的句子, 那么这个架构的弱点就暴露无疑了. </p><ol><li>首先, 编码器能否把句子的所有信息(语言学上的和常识等知识)都理解/捕捉到?</li><li>其次, 受限于目前的实现技术(主要是硬件), 单个隐含状态(如$h_3$这个向量)的维度大小是有限的, 而句子长度以及语言的组合情况是无限的, 单靠$h_3$自身是存储信息能力是有限的.</li><li>再者, 解码器是否有足够的解码能力从一个隐含状态中解码出所有的信息?</li></ol><p>虽然大部分句子是相对紧凑的, 但语言有个特点, 就是一个词有可能和前面好几步之外的词有联系, 比如一些指代词用于指代文本最开头出现的名词; 语义上, 某个句子的理解, 可能依赖于前面多个句子; 当然往大了说, 要理解一篇文章或一本书, 我们通常需要理解并联系多个段落, 多个章节. 这种现象称之为语言的长距离依赖(<strong>long-term dependency</strong>), 在一般性的序列数据中, 这个现象称之为的Long-range dependence(LRD). 即使是使用了LSTM这种理论上可以克服长距离依赖问题地网络, 也无法很好的克服语言的长距离依赖问题, 究其原因, 除了LSTM自身的局限性之外, 更主要是深度学习的梯度学习方法的局限性(在梯度反向传播中, 会出现梯度消失).</p><p>在没有更好地参数学习方法替代, 以及隐含层容量有限的前提下, 注意力机制通过为各个时间步的词分配注意力, 从理论上赋予了模型回望源头任意时间步的能力. 注意力机制自身包含的参数是一般神经网络的重要补充, 而它的机能也一定程度上解决了梯度消失的问题. </p><p>注意力机制在NMT的具体作用过程是这样, 训练过程中, 给定一对输入序列<code>知识就是力量&lt;end&gt;</code>和输出序列<code>Knowledge is power &lt;end&gt;</code>。解码器可以在输出序列的时间步<code>1</code>(当前时间步就是一个<code>query</code>), 使用集中编码了<code>知识</code>信息的背景变量来生成<code>Knowledge</code>，在时间步<code>2</code>使用更集中编码了<code>就是</code>的信息的背景变量来生成<code>is</code>，依此类推。这看上去就像是在解码器的每一时间步对输入序列中不同时间步编码的信息分配不同的注意力。这样注意力矩阵参数就编码了这种”注意力”, 同时也更好的协助其他网络部件学习参数. 在预测阶段的每一个时间步, 注意力也参与其中.</p><p><img src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/EncDecAttention.gif" alt="" title="注意力机制; 蓝色连线透明度表示解码器关注编码字的程度。透明度越低，注意力越强. image from https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis"></p><p>一个经典的(目前也还在不断发展的)NLP问题是文本编码, 即把非结构化的文本, 映射为结构化的数字/向量. 较早有纯统计的<strong>Bag of words</strong>(Salton &amp; McGill, 1986), 后期发展出了经典的<strong>Word2Vec</strong>(Mikolov et al., 2013). 现在主流的神经网络文本编码方法是Word2Vec, fasttext, rnn(lstm/gru)等, 核心思想是把文本中的每个字符都映射到一个embedding向量空间中, 全部加在一起得到整个文本的向量表示, $f(x)=\rho \bigg( \sum_{i=1}^n \phi(x_i) \bigg)$, 再拿去给后续的网络做分类等任务. 这种算法的缺陷是, 最终编码出来的向量, 会偏向统计频率高的元素, 这导致其在很多实际应用中表现不好, 比如情感分析中, 很多转折句, 前后态度是反转的, 但核心是转折后的部分. </p><blockquote><p>They respect you, they really do, but you have to… Why are you laughing?</p></blockquote><p><img src="/images/everything_before_the_word_but_is_horse_shit.gif" alt=""></p><p>如何让编码模型重点关注句子的关键部分呢? 这得分情况, 一种如这样 <code>They respect you, they really do,</code><strong><code>but you have to... Why are you laughing?</code></strong>, 整个句子的意思, 是着重于<code>but</code>后面的部分. 亦或者如<code>Wang et al, ’16</code>中提到的<code>“The appetizers are ok, but the service is slow.”</code>, 一个句子中其实分为两个意思, <code>对于外观口味，评价为积极，而对于服务，评价为消极</code>。</p><p>那么这个时候就需要用注意力机制来给句子的编码分配权重了, $$f(x)=\rho \bigg( \sum_{i=1}^{n} \alpha(w_i, X) \phi(x_i) \bigg)$$<br>通过注意力机制，我们不再需要竭尽全力把完整的句子输入编码为固定长度的向量，二十允许解码器在输出生成的每个步骤中“关注”源语句的不同部分。</p><p><strong>所以 Attention 在神经网络模型中的作用就是改进池化(pooling):</strong><br>没有Attention的池化:$$f(X)=\rho \bigg( \sum_{x \in X} \phi(x) \bigg)$$<br>有Attention后: $$f(X)=\rho \bigg( \sum_{x \in X} \alpha(x, w) \phi(x) \bigg)$$</p><h3 id="如何表达注意力机制"><a href="#如何表达注意力机制" class="headerlink" title="如何表达注意力机制"></a>如何表达注意力机制</h3><p>把Attention机制从encoder-decoder架构中抽象出来理解, 如下图: <img src="/images/attention_mechanism.png" alt=""></p><p>注意力三个核心组件是: </p><ol><li><code>Query</code>: decoder当前待解码的输出. 如果是seq2seq模型, 那就是当前解码器待生成的时间步(用前一时间步的解码器隐含状态来表达).</li><li><code>Key-Value</code>: 每个<code>key</code>(输入序列经过编码后的隐含状态)都对应一个<code>value</code>. 在文本任务中, 一般<code>Key</code>和<code>Value</code>一样, 都是输入序列的编码。</li><li><code>Query</code>和<code>Key</code>的相关性: $\alpha(q, k)$, 告诉模型如何根据<code>Query</code>和各个<code>Key</code>的相关性来分配权重.</li></ol><p>计算注意力的主要步骤:</p><ol><li>计算<code>Query</code>和每个<code>key</code>之间的相关性$\alpha_c(q,k_i)$, 常用的相关性函数包括点积(Scaled Dot-Product Attention)，拼接，神经网路等</li><li>归一化(如softmax)后获得分配权重${\theta_1, …, \theta_k}$</li><li>计算<code>Value</code>的加权平均值, 作为Attention输出值.</li></ol><p>$$\begin{eqnarray} A(q, {(k,v)}) \xrightarrow[\text{output}]{\text{maps as}} \sum_{i=1}^k{\overbrace{\alpha_c(q,k_i)}^{\theta_i}}v_i, q \in Q, k \in K, v \in V \end{eqnarray}$$</p><p>在编码器-解码器架构中，<code>Query</code>通常是解码器的隐含状态。而<code>Key</code>和<code>Value</code>，都是编码器的隐含状态。加权求和作为输出: $$\begin{eqnarray} out = \sum_{i=1}^k \theta_i v_i = \sum_{i=1}^k \theta_i h(x_i) \end{eqnarray}$$</p><h3 id="Attention和Memory对比"><a href="#Attention和Memory对比" class="headerlink" title="Attention和Memory对比"></a>Attention和Memory对比</h3><p>从上面的描述看Attention更像是对神经网络(如LSTM等)的<strong>记忆功能</strong>的改进. 也就是说, 注意力机制只是让网络能够访问其内存，比如编码器的隐含状态. 网络选择从内存中检索什么，并给与不同程度的“关注度”. 换句话说, 何种内存访问机制是soft的，网络将检索所有内存并加权组合。使soft的内存访问好处是可以使用反向传播轻松地进行端对端网络训练（当然也有其他办法-采样方法来计算梯度）。</p><p>而另一方面, 更复杂的Memory机制的研究也发展地如火如荼。比如End-To-End Memory Networks(Sainbayar 2015)中提到的网络结构, 允许网络在输出之前多次读取相同的输入序列，并在每个步骤中更新内存。可以应用于, 例如通过对输入故事进行多个推理步骤来回答问题。<br><code>Joe went to the kitchen.</code><br><code>Fred went to the kitchen.</code><br><strong><code>Joe picked up the milk.</code></strong><br><code>Joe travelled to the office.</code><br><strong><code>Joe left the milk.</code></strong><br><code>Joe went to the bathroom.</code></p><p><code>Where is the milk?</code></p><p>此时, 当网络参数以某种方式绑定在一起时，这个内存结构就和上面介绍的注意力机制一样了，只是它在内存上进行了多次跃迁（因为它试图集成来自多个句子的信息）。</p><p>在这种情境下, 注意力机制也可以很灵活地应用, 比如分别在字符级使用注意力机制来编码单词, 在单词级上编码句子, 在句子级上编码段落, 即 Hierarchical attention. <img src="/images/hierarchical_attention_network.png" alt=""></p><p>Neural Turing Machines(Graves et al., ‘14)的思想也是在内存机制上, 通过将神经网络和外部存储资源耦合来扩展神经网络的功能，这些资源可以通过注意力机制与之交互。组合后的系统类似于图灵机或冯·诺依曼架构，具有端到端的可微分性(因此可以通过梯度下降来进行训练)。除此之外, 神经图灵机但具有更复杂的寻址类型，既可以使用基于内容的寻址（如上下文），也可以使用基于位置的寻址，从而使网络可以学习寻址模式以执行简单的计算机程序，例如排序算法。</p><p>这里并不是要给出Attention和Memory机制的确切的定义区别(也给不了, 有的人觉得二者就是一个东西, 比如有人就称Attention其实软寻址, 应该称为Soft Attention), 而是从主流角度给出类比和解读.</p><h3 id="实战案例-注意力机制应用到机器翻译中"><a href="#实战案例-注意力机制应用到机器翻译中" class="headerlink" title="实战案例: 注意力机制应用到机器翻译中"></a>实战案例: 注意力机制应用到机器翻译中</h3><p>还是以机器翻译为例: 对于解码器的每一个时间步$t’$, 生成一个背景向量$c_{t’}$来捕捉相关的解码器信息, 以用于预测输出目标值$y_{t’}$. 解码器在时间步 $t’$ 的隐藏状态 $$s_{t’} = g(y_{t’-1}, c_{t’}, s_{t’-1}).$$ 令编码器在时间 $t$ 的隐藏状态为 $h_t$，且总时间步数为 $T$。解码器在时间步 $t’$ 的背景变量为 $$c_{t’} = \sum_{t=1}^T A{t’ t} h_t,$$ 其中 $A{t’ t}$ 是注意力分配的权重，用于给定解码器的当前隐藏状态 $s_{t’}$，对编码器中不同时间步的隐藏状态$h_t$求加权平均。$$A{t’ t} = align(s_{t’}, h_t) = \frac{\exp(score(t’ t))}{ \sum_{t=1}^T \exp(score(t’ t)) },$$ 其中 $score(t’ t) \in \mathbb{R}$ 的计算为 $$score(t’ t) = \alpha(s_{t’ - 1}, h_t).$$</p><p>上式中的score打分函数 $score(t’ t)$ 有多种设计方法。Bahanau 等使用了MLP感知机：</p><p>$$e_{t’t} = v^\top \tanh(W_s s_{t’ - 1} + W_h h_t),$$</p><p>其中 $v$、$W_s$、$W_h$ 以及编码器与解码器中的各个权重和偏差都是模型参数。</p><p>Bahanau 等在编码器和解码器中分别使用了门控循环单元GRU。在解码器中，我们需要对门控循环单元的设计稍作修改。解码器在 $t’ $ 时间步的隐藏状态为</p><p>$$s_{t’} = z_{t’} \odot s_{t’-1}  + (1 - z_{t’}) \odot \tilde{s}_{t’},$$</p><p>其中的重置门、更新门和候选隐含状态分别为</p><p>$$<br>\begin{aligned}<br>r_{t’} &amp;= \sigma(W_{yr} y_{t’-1} + W_{sr} s_{t’ - 1} + W_{cr} c_{t’} + b_r), \\<br>z_{t’} &amp;= \sigma(W_{yz} y_{t’-1} + W_{sz} s_{t’ - 1} + W_{cz} c_{t’} + b_z),\\<br>\tilde{s_{t’}} &amp;= \text{tanh}(W_{ys} y_{t’-1} + W_{ss} (s_{t’ - 1} \odot r_{t’}) + W_{cs} c_{t’} + b_s).<br>\end{aligned}<br>$$</p><p>然后，给定目标(解码器)隐藏状态$h_{t’}$, 以及背景向量$c_{t’}$, 通过使用简单的并联层合并这两个向量的信息, 来生成所谓的注意力隐藏状态:</p><p>$$\tilde{h_{t’}} = \tanh(W_c[c_{t’} : h_{t’}]) $$</p><p>这个注意力向量 $\tilde{h_t}$ 之后会通过一个softmax层来生成预测的概率分布.</p><h3 id="延伸阅读-全局注意力机制"><a href="#延伸阅读-全局注意力机制" class="headerlink" title="延伸阅读:全局注意力机制"></a>延伸阅读:全局注意力机制</h3><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation(Luong  et  al. 2015)</a>对应用于NMT的注意力机制进行了一定的总结：分为全局（global）和局部（local）注意力机制。区别在于“注意力”是放在所有源位置或仅放置在少数源位置。</p><blockquote><p>The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector $c_t$.</p></blockquote><p><img src="/images/global_attention.png" alt="" title="image from: lLuong  et  al. (2015)"><br>两种注意力机制区别就在于如何生成背景向量$c_{t’}$.</p><p>Luong  et  al. (2015) 给出了几种打分函数的计算</p><p>$$<br>\begin{aligned}<br>    score_{dot}(t’ t) &amp;= s^\top_{t’}h_t \\<br>    score_{general}(t’ t) &amp;= s^\top_{t’} W_\alpha h_t, \\<br>    score_{concat}(t’ t) &amp;= v^\top_\alpha \tanh (W_\alpha[s_{t’} : h_t])<br>\end{aligned}<br>$$</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473</li><li>Wang, Yequan, Minlie Huang, and Li Zhao. “Attention-based LSTM for aspect-level sentiment classification.” Proceedings of the 2016 conference on empirical methods in natural language processing. 2016.</li><li>Yang, Zichao, et al. “Hierarchical attention networks for document classification.” Proceedings of the 2016 Conference of the North<br>American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.</li><li><a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="noopener">http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/</a></li><li>目前主流的attention方法都有哪些？ - 张俊林的回答 - 知乎 <a href="https://www.zhihu.com/question/68482809/answer/264632289" target="_blank" rel="noopener">https://www.zhihu.com/question/68482809/answer/264632289</a></li><li><a href="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis" target="_blank" rel="noopener">https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis</a></li><li><a href="alex.smola.org/talks/ICML19-attention.pdf">Attention in Deep Learning, Elex Smola, ICML 2019, Long Beach, CA</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;注意力机制如何起源的&quot;&gt;&lt;a href=&quot;#注意力机制如何起源的&quot; class=&quot;headerlink&quot; title=&quot;注意力机制如何起源的&quot;&gt;&lt;/a&gt;注意力机制如何起源的&lt;/h3&gt;&lt;p&gt;神经网络中的注意力机制启发自人类的&lt;strong&gt;视觉注意力机制&lt;/strong&gt;，能够（高分辨率地）聚焦于图像中需要重点关注的目标区域（节省大脑资源），同时（低分辨率地）感知周围的图像，然后随着时间的推移调整焦点（状态调整）。&lt;/p&gt;
&lt;p&gt;在神经网路中，注意力机制是为了解决什么问题？&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Attention" scheme="http://shukebeta.me/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>深入理解word2vec细节以及TensorFlow实现</title>
    <link href="http://shukebeta.me/word2vec/"/>
    <id>http://shukebeta.me/word2vec/</id>
    <published>2019-07-21T16:00:00.000Z</published>
    <updated>2019-07-29T13:11:08.220Z</updated>
    
    <content type="html"><![CDATA[<p>Word2vec <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Mikolov et al.</a></p><a id="more"></a><h2 id="How-to-represent-meanings"><a href="#How-to-represent-meanings" class="headerlink" title="How to represent meanings?"></a>How to represent meanings?</h2><p>如何在数学上表达词义？</p><p>Vector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上<strong>语义相似</strong>的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于<a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis" target="_blank" rel="noopener">Distributional Hypothesis</a>, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">Latent Semantic Analysis</a>))和Predictive models(例如 <a href="http://www.scholarpedia.org/article/Neural_net_language_models" target="_blank" rel="noopener">neural net language models (NNLM)</a>)。</p><p>具体的区别详见<a href="http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf" target="_blank" rel="noopener">Baroni et al.</a>. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。</p><p>Word2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Mikolov et al.</a>).</p><p>从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words (<code>&#39;the cat sits on the&#39;</code>)预测目标单词(例如<code>&quot;mat&quot;</code>); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。</p><h3 id="优化目标函数"><a href="#优化目标函数" class="headerlink" title="优化目标函数"></a>优化目标函数</h3><p>NNLM 的训练是利用 <a href="https://en.wikipedia.org/wiki/Maximum_likelihood" target="_blank" rel="noopener">最大似然 maximum likelihood</a> (ML) 原则来最大化给定上文单词\(h\) (for “history”) 预测下一个词的概率 \(w_t\) (for “target”)。</p><p>$$<br>\begin{align}<br>P(w_t | h) &amp;= \text{softmax}(\text{score}(w_t, h)) \<br>           &amp;= \frac{\exp { \text{score}(w_t, h) } }<br>             {\sum_\text{Word w’ in Vocab} \exp { \text{score}(w’, h) } }<br>\end{align}<br>$$</p><p>其中 \(\text{score}(w_t, h)\) 计算 word \(w_t\) 和 context \(h\) 的相关性 (一般用点乘). 训练时，最大化</p><p>$$<br>\begin{align}<br> J_\text{ML} &amp;= \log P(w_t | h) \<br>  &amp;= \text{score}(w_t, h) -<br>     \log \left( \sum_\text{Word w’ in Vocab} \exp { \text{score}(w’, h) } \right).<br>\end{align}<br>$$</p><p>这么计算成本很高， 因为在每一训练步，需要为词汇表 \(V\) 中的每一个词汇 \(w’\) 计算在当前上下文 \(h\) 的分数概率。</p><p><img src="https://tensorflow.google.cn/images/softmax-nplm.png" alt=""></p><h3 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h3><p>但是，word2vec的目的是特征学习，而不是学习完整的概率语言模型。所以word2vec（CBOW和Skip gram一样）的训练目标函数其实是一个二分类模型(<a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">logistic regression</a>)，给定一个上下文，在 \(k\) 个噪声词（根据算法选出）和一个真正的目标词汇\(w_t\)中识别出目标词\(w_t\)。如下图(以CBOW为例, Skip gram方向反过来)</p><p><img src="https://tensorflow.google.cn/images/nce-nplm.png" alt=""></p><p>目标函数变为最大化:<br>$$<br>J_\text{NEG} = \log Q_\theta(D=1 |w_t, h) + k \mathop{\mathbb{E}}_{\tilde w \sim P_n}<br>\left[ \log Q_\theta(D = 0 |\tilde w, h) \right]<br>$$</p><p>where \(Q_\theta(D=1 | w, h)\) is the binary logistic regression probability<br>under the model of seeing the word \(w\) in the context \(h\) in the dataset<br>\(D\), calculated in terms of the learned embedding vectors \(\theta\). In<br>practice we approximate the expectation by drawing \(k\) contrastive words<br>from the noise distribution (i.e. we compute a<br><a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration" target="_blank" rel="noopener">Monte Carlo average</a>).</p><p>负采样是指每个训练样本仅更新模型权重的一小部分：</p><p>负采样的选择是基于 unigram 分布 $f(w_i)$: 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。<br>$$<br>P(w_i) = \frac{  {f(w_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(w_j)}^{3/4} \right) }<br>$$<br>负采样带来的好处是</p><ol><li>训练速度不再受限于 vocabulary size</li><li>能够并行实现</li><li>模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。<br>模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。</li></ol><p>每个词由两个向量表示：</p><ol><li>$v_w$, 表示这个词作为中心词 (Focus Word) 时的样子。</li><li>$u_w$, 表示它作为另一个中心词的上下文 (Context Word) 时的样子。</li></ol><p>这样, 对于一个中心词 $c$ 和外围词$o$:<br>$$<br>P(o|c) = \frac{exp(u^T_o v_c)}{\sum_{w \in V} \left( {exp(u^T_w v_c)} \right)}<br>$$</p><p>在C语言的源代码里，这些向量由两个数组 (Array) 分别负责：<br><code>syn0</code>数组，负责某个词作为中心词时的向量。是<strong>随机初始化</strong>的。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369</span></span><br><span class="line"><span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++) &#123;</span><br><span class="line">     next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>)<span class="number">25214903917</span> + <span class="number">11</span>;</span><br><span class="line">     syn0[a * layer1_size + b] =</span><br><span class="line">        (((next_random &amp; <span class="number">0xFFFF</span>) / (real)<span class="number">65536</span>) - <span class="number">0.5</span>) / layer1_size;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p><p><code>syn1neg</code>数组，负责这个词作为上下文时的向量。是<strong>零初始化</strong>的。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L365</span></span><br><span class="line"><span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</span><br><span class="line">   syn1neg[a * layer1_size + b] = <span class="number">0</span>;</span><br></pre></td></tr></table></figure></p><p>训练时，先选出一个中心词。在正、负样本训练的时候，这个中心词就保持不变 (Constant) 了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L226</span></span><br><span class="line"><span class="comment"># Negative sampling.</span></span><br><span class="line">sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(</span><br><span class="line">    true_classes=labels_matrix,</span><br><span class="line">    num_true=<span class="number">1</span>,</span><br><span class="line">    num_sampled=opts.num_samples,</span><br><span class="line">    unique=<span class="keyword">True</span>,</span><br><span class="line">    range_max=opts.vocab_size,</span><br><span class="line">    distortion=<span class="number">0.75</span>,</span><br><span class="line">    unigrams=opts.vocab_counts.tolist()))</span><br></pre></td></tr></table></figure><h2 id="用TensorFlow实现word2vec"><a href="#用TensorFlow实现word2vec" class="headerlink" title="用TensorFlow实现word2vec"></a>用TensorFlow实现word2vec</h2><p>Negative Sampling 可以利用原理近似的 <a href="https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf" target="_blank" rel="noopener">noise-contrastive estimation (NCE) loss</a>, 已经在TF的<a href="https://tensorflow.google.cn/api_docs/python/tf/nn/nce_loss" target="_blank" rel="noopener">tf.nn.nce_loss()</a>实现了.</p><h3 id="Building-the-graph"><a href="#Building-the-graph" class="headerlink" title="Building the graph"></a>Building the graph</h3><p>初始化一个在<code>-1: 1</code>之间随机均匀分布的矩阵<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">embeddings = tf.Variable(</span><br><span class="line">    tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br></pre></td></tr></table></figure></p><p>NCE loss 依附于 logistic regression 模型。为此, 我们需要定义词汇中每个单词的weights和bias。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nce_weights = tf.Variable(</span><br><span class="line">  tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                      stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">nce_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br></pre></td></tr></table></figure></p><p>参数已经就位, 接下来定义模型图.</p><p>The skip-gram model 有两个输入. 一个是以word indice表达的一个batch的context words, 另一个是目标单词。为这些输入创建placeholder节点, 以便后续馈送数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Placeholders for inputs</span></span><br><span class="line">train_inputs = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br></pre></td></tr></table></figure></p><p>利用<code>embedding_lookup</code>来高效查找word indice对应的vector.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br></pre></td></tr></table></figure></p><p>使用NCE作为训练目标函数来预测target word:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute the NCE loss, using a sample of the negative labels each time.</span></span><br><span class="line">loss = tf.reduce_mean(</span><br><span class="line">  tf.nn.nce_loss(weights=nce_weights,</span><br><span class="line">                 biases=nce_biases,</span><br><span class="line">                 labels=train_labels,</span><br><span class="line">                 inputs=embed,</span><br><span class="line">                 num_sampled=num_sampled,</span><br><span class="line">                 num_classes=vocabulary_size))</span><br></pre></td></tr></table></figure></p><p>然后添加计算梯度和更新参数等所需的节点。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We use the SGD optimizer.</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">1.0</span>).minimize(loss)</span><br></pre></td></tr></table></figure></p><h3 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h3><p>使用<code>feed_dict</code>推送数据到<code>placeholders</code>, 调用<code>tf.Session.run</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> generate_batch(...):</span><br><span class="line">  feed_dict = &#123;train_inputs: inputs, train_labels: labels&#125;</span><br><span class="line">  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)</span><br></pre></td></tr></table></figure></p><h3 id="Evaluating-the-model"><a href="#Evaluating-the-model" class="headerlink" title="Evaluating the model"></a>Evaluating the model</h3><p>Embeddings 对于 NLP 中的各种下游预测任务非常有用。可以利用analogical reasoning, 也就是预测句法和语义关系来简单而直观地评估embeddings, 如<code>king is to queen as father is to ?</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L292</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_eval_graph</span><span class="params">(self)</span>:</span></span><br><span class="line">  <span class="string">"""Build the eval graph."""</span></span><br><span class="line">  <span class="comment"># Eval graph</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Each analogy task is to predict the 4th word (d) given three</span></span><br><span class="line">  <span class="comment"># words: a, b, c.  E.g., a=italy, b=rome, c=france, we should</span></span><br><span class="line">  <span class="comment"># predict d=paris.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># The eval feeds three vectors of word ids for a, b, c, each of</span></span><br><span class="line">  <span class="comment"># which is of size N, where N is the number of analogies we want to</span></span><br><span class="line">  <span class="comment"># evaluate in one batch.</span></span><br><span class="line">  analogy_a = tf.placeholder(dtype=tf.int32)  <span class="comment"># [N]</span></span><br><span class="line">  analogy_b = tf.placeholder(dtype=tf.int32)  <span class="comment"># [N]</span></span><br><span class="line">  analogy_c = tf.placeholder(dtype=tf.int32)  <span class="comment"># [N]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalized word embeddings of shape [vocab_size, emb_dim].</span></span><br><span class="line">  nemb = tf.nn.l2_normalize(self._emb, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Each row of a_emb, b_emb, c_emb is a word's embedding vector.</span></span><br><span class="line">  <span class="comment"># They all have the shape [N, emb_dim]</span></span><br><span class="line">  a_emb = tf.gather(nemb, analogy_a)  <span class="comment"># a's embs</span></span><br><span class="line">  b_emb = tf.gather(nemb, analogy_b)  <span class="comment"># b's embs</span></span><br><span class="line">  c_emb = tf.gather(nemb, analogy_c)  <span class="comment"># c's embs</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># We expect that d's embedding vectors on the unit hyper-sphere is</span></span><br><span class="line">  <span class="comment"># near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].</span></span><br><span class="line">  target = c_emb + (b_emb - a_emb)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute cosine distance between each pair of target and vocab.</span></span><br><span class="line">  <span class="comment"># dist has shape [N, vocab_size].</span></span><br><span class="line">  dist = tf.matmul(target, nemb, transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># For each question (row in dist), find the top 4 words.</span></span><br><span class="line">  _, pred_idx = tf.nn.top_k(dist, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Nodes for computing neighbors for a given word according to</span></span><br><span class="line">  <span class="comment"># their cosine distance.</span></span><br><span class="line">  nearby_word = tf.placeholder(dtype=tf.int32)  <span class="comment"># word id</span></span><br><span class="line">  nearby_emb = tf.gather(nemb, nearby_word)</span><br><span class="line">  nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=<span class="keyword">True</span>)</span><br><span class="line">  nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,</span><br><span class="line">                                       min(<span class="number">1000</span>, self._options.vocab_size))</span><br></pre></td></tr></table></figure></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://tensorflow.google.cn/tutorials/representation/word2vec" target="_blank" rel="noopener">https://tensorflow.google.cn/tutorials/representation/word2vec</a></li><li><a href="https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf" target="_blank" rel="noopener">Learning word embeddings efficiently with noise-contrastive estimation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Word2vec &lt;a href=&quot;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mikolov et al.&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CS" scheme="http://shukebeta.me/categories/CS/"/>
    
    
      <category term="Python" scheme="http://shukebeta.me/tags/Python/"/>
    
      <category term="Programming Language" scheme="http://shukebeta.me/tags/Programming-Language/"/>
    
  </entry>
  
  <entry>
    <title>Topic Modelling - 主题建模以及隐变量模型</title>
    <link href="http://shukebeta.me/NLP-topic-modeling/"/>
    <id>http://shukebeta.me/NLP-topic-modeling/</id>
    <published>2018-12-22T16:00:00.000Z</published>
    <updated>2019-12-31T09:41:01.112Z</updated>
    
    <content type="html"><![CDATA[<p>本篇介绍 topic modeling, 以及一个经典的算法Latent Dirichlet allocation, 文本挖掘与语义理解的集大成者(至少在深度学习统治之前). 当然LDA不仅仅局限于文本, 还可应用于涉及大量数据集的各种问题，包括协同过滤，基于内容的图像检索和生物信息学等领域的数据。<br><a id="more"></a></p><h2 id="Topic-Modelling"><a href="#Topic-Modelling" class="headerlink" title="Topic Modelling"></a>Topic Modelling</h2><p>大规模文本挖掘的核心问题, 就是用数学模型代替人力来理解文本语义，目标是找到对集合成员（如一堆文本）的数学/统计描述，以便能够对这些大型集合进行高效处理，同时保留对基本任务（如分类，检测，摘要以及相似性和相关性判断）有用的基本统计关系。</p><p>在这方面的研究方法很多，特别是信息检索(IR)领域. 一个基本方法是将语料库中的每个文档向量化，向量中的每个实数代表计数率。比如经典的tf-idf方法，用<strong>Document-Term Matrix</strong>来表达不同词在不同文档出现的情况差异, 一般term就是word作为features, 所以在这里我们表示document-word matrix(DWM), 就是<code>DWM[i][j] = The number of occurrences of word_j in document_i</code>.<br>Doc 1: I have a fluffy cat.<br>Doc 2: I see a fluffy dog. </p><table><thead><tr><th>DWM</th><th>I</th><th>have</th><th>a</th><th>fluffy</th><th>cat</th><th>see</th><th>dog</th></tr></thead><tbody><tr><td>doc1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr><td>doc2</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr></tbody></table><p>然后进行normalization, 去和 inverse document frequency count(IDF)进行比较. IDF统计每个词在整个文档集合中出现的总次数, 通常转化为log scale, 并进行适当的normalization. </p><p><img src="/images/document_term.png" alt="" title="image from https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"></p><p>这个矩阵把文档表示为向量，使得不同文档之间可以从几何上衡量相似性，根据相似性<strong>聚类文本</strong>.</p><p>虽然tf-idf有很多很好的特性, 但是它的降维程度非常有限, 而且无法揭示文档间或文档内的统计结构.</p><p>比如我们需要知道文本包含什么<strong>信息</strong>, 却又不清楚什么信息是重要的, 所以我们希望能把信息也归纳成几类。我们称这种信息为<strong>主题</strong>, 一种粗粒度的信息. 那么就有了一个很重要的任务, 就是挖掘出这一堆文本包含的主题都有哪几大类. 每个文本都可能包含多种不同主题, 而且包含的侧重也不一样, 所以进一步的, 我们希望能够挖掘出每个文本的主题分布, 也就是主题类别在各个文本中的权重. 这种对文本信息的挖掘和理解方法, 称之为<strong>主题建模(Topic Modelling)</strong>.</p><p>此时主题数量就是一个超参数, 通过主题建模，构建了单词的clusters而不是文本的clusters。因此，文本被表达为多个主题的混合，每个主题都有一定的权重。</p><p>因为主题建模不再是用词频来表达, 而是用主题权重<code>{Topic_i: weight(Topic_i, T) for Topic_i in Topics}</code>, 所以主题建模也是一种 Dimensionality Reduction. </p><p>主题建模也可以理解为文本主题的tagging任务, 只是无监督罢了.</p><h3 id="Latent-Semantic-Analysis-LSA"><a href="#Latent-Semantic-Analysis-LSA" class="headerlink" title="Latent Semantic Analysis (LSA)"></a>Latent Semantic Analysis (LSA)</h3><p>通过引入Latent的概念，把主题表达为隐藏的信息, 也就是假设主题已经存在, 只是我们看不到. LSA使用DWM矩阵的SVD奇异值分解来确定tf-idf特征空间中的线性子空间，该子空间捕获了集合中的大部分variance。</p><ol><li>假设单词使用中存在一些latent的结构, 由于单词选择的多样性而被掩盖了.</li><li>与其将文本表示为单词的t维空间中的向量，不如将相似词有效地组合在一起的”概念”(topic), 作为维度, 将文本（以及词本身）表示为低维空间中的向量. 这些低维的轴就是通过PCA得出的Principal Components</li><li>然后就可以在 latent semantic 空间中为下游任务服务, 如计算文本相似度(通过内积等cosine相似度计算).</li></ol><p><img src="/images/ducument_topic_term.png" alt="" title="image from https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"></p><p>把DWM矩阵表达为 DTM(Document Topic Matrix) 和 TWM(Topic Word Matrix) 两个矩阵, 它们维度更小，相乘的结果应该尽可能近似原始的DWM矩阵。</p><p>假设词汇$V$有$1024$个, 文档$W$有$64$篇, 用 $DWM = W \times V$来表达Document-Word Matrix, 需要$64 \times 1024 = 65,536$的参数量. 假如我们设定topic参数为$8$, 那么就可以用$DWM = DTM \times TWM $来近似表达Document-Term Matrix, 参数量减少为$64 \times 8 + 8 \times 1024 = 8,704$, 缩减了将近$90\%$</p><p>所以LSA核心思想是构造 Document-Term Matrix 的低阶近似.</p><ol><li>用 tf-idf 计算加权DWM. tf-idf(term frequency–inverse document frequency)是DWM矩阵的一种经典加权表达。</li><li>然后对DWM矩阵进行 Singular Value Decomposition (SVD).</li></ol><p><img src="/images/lsa_process.png" alt="" title="https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf"></p><p>降维的意义不仅仅是减少下游任务的计算负担：</p><ol><li>tf-idf向量一般很长很庞大。因此降维操作对于聚类或分类等进一步计算是能节省很多资源。</li><li>原始DTM矩阵被认为是有噪声的：近似矩阵被解释为去噪矩阵（比原始矩阵更好的矩阵）。</li><li>假定原始的DTM矩阵相对于“真实的”DTM矩阵过于稀疏，降维也可以看作一种泛化。也就是说，原始矩阵仅列出每个文档中实际的单词，而我们可能会对与每个文档相关的所有单词感兴趣-如同义词等。</li></ol><p><code>(car), (truck), (flower)} --&gt; {(1.3452 * car + 0.2828 * truck), (flower)}</code></p><h3 id="Probabilistic-Latent-Semantic-Analysis-PLSA"><a href="#Probabilistic-Latent-Semantic-Analysis-PLSA" class="headerlink" title="Probabilistic Latent Semantic Analysis (PLSA)"></a>Probabilistic Latent Semantic Analysis (PLSA)</h3><p>也称之为aspect model, 尝试从统计学的角度改进LSA. 将文档中的每个单词建模为混合模型中的样本. 其中这个混合模型混合的成分是multinomial随机变量，可以视为“主题”的表示形式。因此，每个单词都是由单个主题生成的，每一个文档中的不同单词可能从不同的主题生成。每个文档都表示为这些混合成分根据不同比例混合的列表，从而简化为固定主题集的概率分布.</p><p>把潜在的topics视作 Latent variable 隐变量z, 而文本Documents和词汇Words就是观察变量 observed variables. 共现(co‐occurrence)的数据都关联有隐含的话题类别, 做出条件独立假设, D和W是基于隐变量z的条件独立变量,<br>$$ P(w|d) = \sum_{z\in Z} P(w|z)P(z|d)$$</p><p>$$ P(d, w) = P(d)P(w|d) = P(d) \sum_{z\in Z} P(w|z)P(z|d) \\<br>           = \sum_{z\in Z} P(d) P(w|z)P(z|d) \\<br>           = \sum_{z\in Z} P(z) P(w|z)P(d|z)<br>$$</p><p><img src="/images/plsa_illustrations.png" alt="" title="https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf"></p><p>利用隐变量可以解决稀疏性问题，也就是避免文档中未出现的word的概率为零, 可以理解为一种平滑.</p><p>使用EM算法:<br>E-Step: 计算隐变量的posterior probabilities, </p><ul><li>$P(z|d, w) = \frac{ P(z) P(w|z)P(d|z) }{ \sum_{z’\in Z} P(z’) P(w|z’)P(d|z’) }$</li></ul><p>M-Step: 更新参数</p><ul><li>$P(w|z) \propto \sum_{d \in D} n(d, w) P(z|d, w) $ </li><li>$P(d|z) \propto \sum_{w \in W} n(d, w) P(z|d, w) $ </li><li>$P(z) \propto \sum_{d \in D}\sum_{w \in W} n(d, w) P(z|d, w) $ </li></ul><p>PLSA是一种生成式的概率模型. PLSA的$P(w, d)$可以解释为LSA中的$P = U Σ V^T$, 其中$U$包含$P(d|z)$, $Σ$作为$P(z)$的对角矩阵, $V$包含$P(w|z)$ </p><p>PLSA有助于处理多义词(Polysemous words), 通过$P(w|z)$排序比较, 比如<code>SEGMENT</code>在topic1中更靠近<code>image</code>, 意味着<code>Image region</code>; 在topic2中更靠近<code>sound</code>, 意味着<code>Phonetic segment</code>.</p><p>虽然PPCA也是概率模型, 但是PPCA假设了正态分布(normal distribution), 局限性很大. PLSA将每个共现的概率建模为条件独立的多项式分布(multinomial distributions)的混合. 多项式分布在此领域是更好的选择。</p><p>因为有了$p(z|d)$充当特定文档的主题混合权重, pLSA可以捕获文档包含多个主题的可能性. 但是, $d$是训练集文档列表的虚拟索引, 因此$d$是一个多项式随机变量，其值可能与训练文档一样多, 这样pLSA仅针对训练集的文档学习主题混合$p(z|d)$, 对于训练集之外的document而言, 不知道如何分配概率. 等于说pLSA并不是一个定义明确的文档级别的概率生成模型。</p><p>除此之外, 因为使用训练集文档索引的分布，另一个困难就是需要估计的参数数量随着训练集数量增加而线性增加. 具体地说, 一个k-topic pLSA的参数是$k$个latent topic上大小为$V$的多项式分布, 以及$M$个mixtures, 参数量是$kV + kM$，随着$M$线性增长。所以pLSA容易过拟合(虽然可以用Tempered EM算法来稍微缓解). </p><h3 id="Latent-Dirichlet-allocation-LDA"><a href="#Latent-Dirichlet-allocation-LDA" class="headerlink" title="Latent Dirichlet allocation(LDA)"></a>Latent Dirichlet allocation(LDA)</h3><p>回顾LSA和pLSA, 都基于“词袋”的假设。从概率论的角度来说，是对文档中单词有exchangeability的假设（Aldous，1985）。此外，尽管很少正式地陈述这些方法，但这些方法还假定文档是可交换的。语料库中文档的特定顺序也可以忽略。Finetti（1990）提出的经典表示定理认为任何可交换随机变量的集合都具有表示为混合分布的形式 - 通常是无限混合。因此，如果我们希望考虑文档和单词的可交换表示形式，则需要考虑能够同时捕获单词和文档的可交换性的混合模型。这就是<a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" target="_blank" rel="noopener">Latent Dirichlet allocation. David Blei, Andrew Ng, and Michael Jordan. 2003.</a>这篇文章的动机.</p><p>LDA对主题分布的基本设定是, 每个文档被表达为latent variables(topics)的随机混合, 其中各个topics可以由单词的概率分布来描述.</p><p>对于语料库$D$中的每个文档$\boldsymbol{w}$, LDA假设如下的生成过程:</p><ol><li>选择参数$N ∼ Poisson(ξ)$,</li><li>用Dirichlet分布$Dir(\alpha)$生成一个多项式分布参数$θ$, 即$p(θ|\alpha)$</li><li>对于文档中的每一个词$w_n$:<ol><li>基于多项式概率分布$Multinomial(θ)$选择一个topic$z_n$, 即$p(z_n |θ)$</li><li>基于$p(w_n | z_n, β)$, 即以topic $z_n$为条件的multinomial概率, 选择一个词$w_n$</li></ol></li></ol><p>这个过程做了几个假设. 一个是, $\beta$作为单词概率的参数, 是一个$k \times V$的矩阵, $\beta_{ij} = p(w^j = 1 | z^i = 1)$, 是需要估计的固定变量. 这里要注意，$N$是独立于所有其他数据生成变量（$θ$和$z$）, 因此是一个辅助变量，通常会忽略它的随机性。其余的假设有兴趣可以去读论文.</p><p>给定了参数$\alpha$和$\beta$, 可以估计topic mixture θ，一组$N$个主题$\boldsymbol{z}$和一组$N$个单词$\boldsymbol{w}$的联合分布: $$ p(θ,\boldsymbol{z}, \boldsymbol{w}|α,β) = p(θ|α) \prod^N_{n=1} p(z_n | \theta) p(w_n|z_n, \beta) $$</p><p>$p(z_n |θ)$在这里就是第$i$个$\theta_i$, 这个独特的$i$使得$z^i_n = 1$. 沿着$θ$求积分并在$z$上求和，得到文档的 marginal distribution $$p(\boldsymbol{w}|α,β) = \int p(θ|α) \Bigg( \prod\limits^N_{n=1} \sum\limits_{z_n} p(z_n | \theta) p(w_n|z_n, \beta)  \Bigg) d\theta$$</p><p>最后，取各个文档的marginal distribution的乘积，得到整个语料库(corpus)的概率 $$ p(D|α,β)  = \prod\limits^M_{d=1} p(\boldsymbol{w}|α,β) $$</p><p>参数$\alpha$和$\beta$是语料库级别的参数，假定在生成语料库的过程中只采样一次。$\theta_d$是文档级别的变量, 每个文档采样一次. $z_{dn}$和$w_{dn}$是词级别的变量, 每个文档的每个词采样一次.</p><p>LDA通过将主题混合权重视为k-parameter隐随机变量，而不是与训练集显式关联的一大套参数, 解决pLSA的缺陷。而且k-topic的LDA模型参数量是$k + kV$, 不会随着训练语料库的增加而增长.</p><p>如果在几何上比较和理解pLSA和LDA, 模型都可以视为在words的分布空间上操作, 每个这样的分布都可以看作<code>(V-1)-simplex</code>(称之为word simplex)上的一个点. 如图, 假设有<code>3</code>个单词, 假设选择<code>k=3</code>的topic simplex包含在三个单词的word simplex中。word simplex的三个角对应于三个特殊的分布(<code>[1, 0, 0], [0, 1, 0], [0, 0, 1]</code>)，即其中各有一个单词的概率为<code>1</code>。topic simplex的三个点对应于三个不同的单词分布(比如类似<code>[0.7, 0.2, 0.1], [0.05, 0.9, 0.05], [0.3, 0.05, 0.65]</code>)。<img src="/images/topic_models_geometric_interpretation.png" alt="" title="image from Blei 2003"></p><p>最简单的unigram模型在word simplex上找到一个点，并假设语料库中的所有单词都来自相应的分布。而隐变量模型考虑(选择)word simplex上的<code>k</code>个点(在图中是<code>k=3</code>个)，并基于这些点形成sub-simplex，即topic simplex。</p><ul><li>pLSA模型假定训练集文档的每个单词各来自一个随机选择的主题。主题本身来自document-specific的主题分布，即topic simplex上的一个个点<code>x</code>。每个文档都有一个这样的分布；因此，文档训练集定义了关于topic simplex的经验分布。</li><li>LDA假定，不管是训练集还是测试集的文档, 每个单词都是由随机选择的主题生成的，该主题是从一个以随机选择的$θ_d$为参数的分布中得出的。参数$θ_d$的采样方法是每个文档采样一个topic simplex的平滑分布, 就是图中的等高线。</li></ul><h3 id="LDA推理和参数估计"><a href="#LDA推理和参数估计" class="headerlink" title="LDA推理和参数估计"></a>LDA推理和参数估计</h3><p>LDA推理关键的一步是计算给定的一个文档的隐变量的后验分布(posterior distribution): $$<br>p(θ, \boldsymbol{z} | \boldsymbol{w}, α, β) = \frac{p(θ, \boldsymbol{z}, \boldsymbol{w} | α, β)}{p( \boldsymbol{w} | α, β)} $$</p><p>其中的$p(\boldsymbol{w}|α,β)$由于latent topics的求和中$θ$和$β$之间的耦合而变得很难求解(Dickey, 1983). 尽管因为后验分布导致精确的推理是很难，但对于LDA，可以考虑使用各种近似算法，包括Laplace逼近，变分(variational)逼近和Markov chain Monte Carlo(Jordan, 1999)。</p><p>论文中介绍了一种convexity-based variational inference方法, 基本思想是利用Jensen’s inequality获得log likelihood的可调下限(Jordan, et al., 1999)</p><p>使用迭代逼近来计算DLA模型：</p><ol><li>初始化：每个单词随机分配给一个主题。</li><li>循环遍历每个单词，基于以下信息将单词重新分配给一个主题：</li></ol><ul><li>training: repeat until converge<ol><li>assign each word in each document to one of T topics.</li><li>For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t)</li><li>Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d)</li></ol></li></ul><h3 id="LDA的实际应用"><a href="#LDA的实际应用" class="headerlink" title="LDA的实际应用"></a>LDA的实际应用</h3><p><a href="https://github.com/congchan/Chinese-nlp/blob/master/latent-dirichlet-allocation-topic-model.ipynb" target="_blank" rel="noopener">LDA模型实战案例</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>主题建模的算法:</p><ol><li>(p)LSA: (Probabilistic) Latent Semantic Analysis – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra. SVD假设了Gaussian distributed. </li><li>LDA: latent Dirichlet allocation, 假设了multinomial distribution。<blockquote><p>LDA是pLSA的generalization, LDA的hyperparameter设为特定值的时候，就specialize成 pLSA 了。从工程应用价值的角度看，这个数学方法的generalization，允许我们用一个训练好的模型解释任何一段文本中的语义。而pLSA只能理解训练文本中的语义。（虽然也有ad hoc的方法让pLSA理解新文本的语义，但是大都效率低，并且并不符合pLSA的数学定义。）这就让继续研究pLSA价值不明显了。</p></blockquote></li><li>NMF – Non-Negative Matrix Factorization</li><li>Generalized matrix decomposition 实际上是 collaborative filtering 的 generalization，是用户行为分析和文本语义理解的共同基础.</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://nlpforhackers.io/topic-Modelling/" target="_blank" rel="noopener">https://nlpforhackers.io/topic-Modelling/</a></li><li><a href="http://cocosci.berkeley.edu/tom/papers/SteyversGriffiths.pdf" target="_blank" rel="noopener">Steyvers and Griffiths (2007)</a>. Probabilistic topic models. Distributional semantic models and topic models have been extensively investigated not just in NLP, but also as models of human cognition. This paper provides a brief introduction to topic models as cognitive models. A much more thorough investigation can be found in Griffiths, Steyvers, and Tenenbaum (2007).</li><li><a href="http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/" target="_blank" rel="noopener">Latent Semantic Analysis (LSA) for Text Classification Tutorial</a></li><li><a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158" target="_blank" rel="noopener">Intuitive Guide to Latent Dirichlet Allocation</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇介绍 topic modeling, 以及一个经典的算法Latent Dirichlet allocation, 文本挖掘与语义理解的集大成者(至少在深度学习统治之前). 当然LDA不仅仅局限于文本, 还可应用于涉及大量数据集的各种问题，包括协同过滤，基于内容的图像检索和生物信息学等领域的数据。&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归</title>
    <link href="http://shukebeta.me/NLP-HMM-CRF/"/>
    <id>http://shukebeta.me/NLP-HMM-CRF/</id>
    <published>2018-12-15T16:00:00.000Z</published>
    <updated>2019-01-01T10:43:51.580Z</updated>
    
    <content type="html"><![CDATA[<h2 id="词性标注（Part-of-Speech-Tagging）"><a href="#词性标注（Part-of-Speech-Tagging）" class="headerlink" title="词性标注（Part-of-Speech-Tagging）"></a>词性标注（Part-of-Speech-Tagging）</h2><p>POS任务是指根据观察得到的序列, 推断出对应的词性标注, 比如<code>Bob drank coffee at Starbucks</code>, 标注可能为<code>Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)</code>.</p><p>除此之外, 还有命名实体识别, 以及其他涉及到需要根据观察序列推断隐含状态的问题, 都可以考虑用隐马尔可夫模型(Hidden Markov Model, HMM)或者条件随机场(conditional random fields, CRF)来处理.</p><a id="more"></a><h2 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h2><p>Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。<br>G = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立.<br><img src="/images/probabilistic_graphical_models.png" alt="" title="image from: Probabilistic Graphical Models Principles and Techniques"><br>从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为<code>P(当前节点 | 父节点)</code>.</p><p>而马尔可夫网络则是无向图, 包含了一组具有马尔可夫性质的随机变量. 马尔可夫随机场(Markov Random Fields, MRF)是由参数$λ=(S, π, A)$表示, 其中S是状态的集合，π是初始状态的概率, A是状态间的转移概率。一阶马尔可夫链就是假设t时刻的状态只依赖于前一时刻的状态，与其他时刻的状态和观测无关。这个性质可以用于简化概率链的计算。使用类似性质作为假设的模型还有Bi-gram语言模型等.</p><h3 id="朴素贝叶斯分类器与隐马尔可夫模型"><a href="#朴素贝叶斯分类器与隐马尔可夫模型" class="headerlink" title="朴素贝叶斯分类器与隐马尔可夫模型"></a>朴素贝叶斯分类器与隐马尔可夫模型</h3><p>朴素贝叶斯分类器(NBs)假设条件独立性(朴素贝叶斯假设, Hand and Yu, 2001)：$p(x_i | y, x_j) = p(x_i | y)$, 在给定目标值 y 时，x的属性值之间相互条件独立。这样, 计算可以简化为 $$p(y | \overrightarrow{x}) \propto p(y, \overrightarrow{x}) = p(y) \prod_{i=1} p(x_i | y).$$</p><p>朴素贝叶斯模型只考虑了单个输出变量y。如果要为一个观察序列$\overrightarrow{x} =(x_1, …, x_n)$预测对应的分类序列$\overrightarrow{y} =（y_1, …, y_n)$ ，一个简单的序列模型可以表示为多个NBs的乘积。此时不考虑序列单个位置之间的相互依赖。$$p(\overrightarrow{y}, \overrightarrow{x}) = \prod^n_{i=1} p(y_i) p(x_i | y_i).$$<br>此时每个观察值$x_i$仅取决于对应序列位置的类变量$y_i$。由于这种独立性假设，从一个步骤到另一个步骤的转换概率不包括在该模型中。然而这种假设在实践中几乎不会符合，这导致这种模型的性能很有限。</p><p>因此，比较合理的假设是观测序列在连续相邻位置间的观测值存在依赖。要模拟这种依赖关系, 就要引入状态转移概率$p(y_i | y_{i-1})$, 由此引出著名的隐马尔可夫模型 Hidden Markov model, HMM, Rabiner (1989):<br>$$p(\overrightarrow{y}, \overrightarrow{x}) = \prod^n_{i=1} p(y_i | y_{i-1}) p(x_i | y_i).$$ $$p(\overrightarrow{x}) = \sum_{y\in \mathcal{Y}} \prod^n_{i=1} p(y_i | y_{i-1}) p(x_i | y_i).$$</p><p>HMM参数$λ = (Y, X, π, A, B)$ ，其中Y是隐状态（输出变量）的集合，X是观察值（输入）集合，π是初始状态的概率，A是状态转移概率矩阵$p(y_i | y_{i-1})$，B是输出观察值概率矩阵$p(x_i | y_{i})$。在POS任务中, X就是观察到的句子, Y就是待推导的标注序列, 因为词性待求的, 所以人们称之为<strong>隐含状态</strong>.</p><p>HMM的缺陷是其基于观察序列中的每个元素都相互条件独立的假设。即在任何时刻观察值仅仅与状态（即要标注的标签）有关。对于简单的数据集，这个假设倒是合理。但大多数现实世界中的真实观察序列是由多个相互作用的特征和观察序列中较长范围内的元素之间的依赖而形成的。而条件随机场(conditional random fiel, CRF)恰恰就弥补了这个缺陷.</p><h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p>随机场, 可以看成是一组随机变量的集合（这组随机变量对应同一个样本空间）。当给每一个位置按照某种分布随机赋予一个值之后，其全体就叫做随机场。这些随机变量之间可能有依赖关系，一般来说，也只有当这些变<br>量之间有依赖关系的时候，我们将其单独拿出来看成一个随机场才有实际意义。</p><p>如果给定的MRF中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个MRF的分布，也就是条件分布，那么这个MRF就称为 conditional random fields (CRF)。它的条件分布形式完全类似于MRF的分布形式，只不过多了一个观察集合X。所以, CRF本质上是给定了条件(观察值observations)集合的MRF</p><p>1.特征函数的选择: 特征函数的选取直接关系模型的性能。<br>2.参数估计: 从已经标注好的训练数据集学习条件随机场模型的参数，即各特征函数的权重向量λ。<br>3.模型推断: 在给定条件随机场模型参数λ下，预测出最可能的状态序列。</p><h3 id="CRF特征函数"><a href="#CRF特征函数" class="headerlink" title="CRF特征函数"></a>CRF特征函数</h3><p>在CRF中，首先需要定义特征函数. 特征函数的定义非常灵活, 可以是：</p><ol><li>一个句子 s</li><li>句子中单词的位置 i</li><li>当前单词的标签$l_i$</li><li>前一个单词的标签$l_{i-1}$ (如果仅限于相邻位置的单词, 那么就是 linear-chain CRF).</li><li>任意其他单词的标签$l_{j}$</li></ol><p>然后为每个特征函数$f_{j}$分配权重$\lambda_j$, 权重是从数据中学习而来. 对$j$个特征方程求和, 对序列每个位置$i$求和:<br>$$ score(l | s) = \sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})$$<br>CRF的每个特征函数都是一个输入的函数, 对应的输出是一个实数值（只是0或1）。例如, 选择特征函数$f_1(s, i, l_i, l_{i-1}) = 1$, 当且仅当$l_i = ADVERB$, 且第i个单词以“<code>-ly</code>”结尾; 否则为0. 如果与此特征相关的权重$\lambda_j$很大且为正，那么这个特征等同于说模型倾向于把以<code>-ly</code>结尾的单词标记为ADVERB。</p><p>通过指数化和归一化把这些得分转换为概率值:<br>$$p(l | s) = \frac{exp[score(l|s)]}{\sum_{l^\prime} exp[score(l^\prime|s)]} = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})]}{\sum_{l’} exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l^\prime_i, l^\prime_{i-1})]} $$</p><h3 id="CRF与HMM"><a href="#CRF与HMM" class="headerlink" title="CRF与HMM"></a>CRF与HMM</h3><p>只需要在CRF的对数线性形式中, 设置权重为对应HMM取对数后的二元转换和发射概率: $\log p(l,s) = \log p(l_0) + \sum_i \log p(l_i | l_{i-1}) + \sum_i \log p(w_i | l_i)$</p><ul><li>对于HMM的每个状态转换概率$p(l_i = y | l_{i-1} = x)$, CRF定义一组特征函数为$f_{x,y}(s, i, l_i, l_{i-1}) = 1$ 如果 $l_i = y$ 且 $l_{i-1} = x$, 为这些特征赋予权重$w_{x,y} = \log p(l_i = y | l_{i-1} = x)$</li><li>类似的, 对于HMM的每个发射概率$p(w_i = z | l_{i} = x)$, CRF定义一组特征函数为$g_{x,y}(s, i, l_i, l_{i-1}) = 1$ 如果 $w_i = z$ 且 $l_i = x$, 赋予权重$w_{x,z} = \log p(w_i = z | l_i = x)$.</li></ul><p>如此, CRF计算的分值$p(l|s)$就精确地正比于对应的HMM, 也就是说, 任意的HMM都可以由CRF表达出来.</p><p>CRF比HMM更强大, 更广泛</p><ol><li>CRF可以定义更广泛的特征函数：HMM受限于相邻位置的状态转换（二元转换）和发射概率函数，迫使每个单词仅依赖于当前标签，并且每个标签仅依赖于前一个标签。而CRF可以使用更多样的全局特征。例如，如果句子的结尾包含问号，则可以给给CRF模型增加一个特征函数，记录此时将句子的第一个单词标记为VERB的概率。这使得CRF可以使用长距离依赖的特征。</li><li>CRF可以有任意的权重值：HMM的概率值必须满足特定的约束， $0 &lt;= p(w_i | l_i) &lt;= 1, \sum_w p(w_i = w | l_1) = 1)$, 而CRF的权重值是不受限制的。</li></ol><p>CRF既具有判别式模型的优点，又考虑到长距离上下文标记间的转移概率，以序列化形式进行全局参数优化和解码的特点，解决了其他判别式模型(如MEMM)难以避免的标记偏见问题。</p><h3 id="CRF与Logistic-Regression"><a href="#CRF与Logistic-Regression" class="headerlink" title="CRF与Logistic Regression"></a>CRF与Logistic Regression</h3><p>CRF的概率计算与Logistic Regression (LR)的形式类似，<br>$$CRF: p(l | s) = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})]}{\sum_{l’} exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l^\prime_i, l^\prime_{i-1})]} $$<br>$$LR: P(y|x) = \frac{\exp \bigg( \sum\limits_{i=1}^{N} w_{i} \cdot f_{i}(x,y) \bigg)} {\sum\limits_{y’ \in Y} \exp \bigg( \sum\limits_{i=1}^{N} w_{i} \cdot f_{i}(x,y’) \bigg)}$$<br>在LR中, $f_i(y, x)$是一个特征，$w_i$是与该特征相关的权重。提取的特征是二元特征，取值0或1，通常称为指示函数。这些特征中的每一个都由与输入$x$和分类$y$相关联的函数计算。</p><p>实际上，CRF基本上就是逻辑回归的序列化：与逻辑回归是用于分类的对数线性模型不同，CRF是标签序列的对数线性模型。</p><h3 id="CRF模型训练"><a href="#CRF模型训练" class="headerlink" title="CRF模型训练"></a>CRF模型训练</h3><p>如何通过数据训练CRF模型, 估计特征函数的权重? 利用极大似然估计（Maximum Likelihood Estimation，MLE)和梯度优化(gradient descent).</p><p>$\log p(l | s)$相对于参数$λ_i$的梯度为:$$\frac{\partial}{\partial w_j} \log p(l | s) = \sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l^\prime_j, l^\prime_{j-1})$$<br>导数的第一项是真实标签下的特征$f_i$的贡献，第二项是当前模型下特征$f_i$的期望贡献。</p><p>对于一堆训练样例（句子和相关的词性标签）。随机初始化CRF模型的权重。要将这些随机初始化的权重转移到正确的权重，对于每个训练示例:</p><ul><li>遍历每个特征函数$f_i$，计算训练示例相对于$λ_i$的对数概率的梯度</li><li>以learning rate $\alpha$的速率沿梯度方向不断修正$λ_i$: $\lambda_i = \lambda_i + \alpha [\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l^\prime_j, l^\prime_{j-1})]$</li><li>重复这些训练步骤，直到满足停止条件（例如，更新低于某个阈值）。</li></ul><p>CRF的缺点是模型训练时收敛速度比较慢.</p><p>训练后的CRF模型, 可以用于预测一个未标记序列的最大可能标记. 我们需要每个标记的概率$p(l | s)$, 对于大小为k的标签集和长度为m的句子, 需要比较的$p(l | s)$组合有$k^m$种. 但是计算时, 可以利用动态规划的方法, 原理类似于Viterbi算法.</p><h3 id="CRF中文命名实体识别"><a href="#CRF中文命名实体识别" class="headerlink" title="CRF中文命名实体识别"></a>CRF中文命名实体识别</h3><p>比如中文命名实体识别任务, 假如需要判断人名、地名、组织名三类命名实体.</p><p>对于人名, 通过一些模板来筛选特征。模板是对上下文的特定位置和特定信息的考虑, 适用于人名的特征模板:</p><ul><li>人名的指界词：主要包括称谓词、动词和副词等，句首位置和标点符号也可。根据指界词与人名共现的概率的大小，将人名的左右指界词各分为两级，生成4个人名指界词列表：<img src="/images/人名指界词.png" alt=""></li><li>人名识别特征的原子模板，每个模板都只考虑了一种因素：<img src="/images/人名识别特征原子模板.png" alt=""></li></ul><p>当特征函数取特定值时，特征模板被实例化, 就可以得到具体的特征。比如当前词的前一个词 $w_{i-1}$ 在人名1级左指界词列表中出现, $f_i(x, y) = 1, if: PBW1(w_{i-1}) = true, y = PERSON$</p><p>类似的，做地名、组织名的特征提取和选择，并将其实例化，得到所有的特征函数。</p><p>评测指标:<br>召回 recall = $ \frac{正确识别的命名实体首部（尾部）的个数}{标准结果中命名实体首部（尾部）的的总数} \times 100\%$</p><p>精确率 precision = $ \frac{正确识别的命名实体首部（尾部）的个数}{识别出的命名实体首部（尾部）的总数} \times 100\%$</p><p>F1 =  $ \frac{2 \times precision \times recall}{precision + recall}$</p><h2 id="生成式模型和判别式模型"><a href="#生成式模型和判别式模型" class="headerlink" title="生成式模型和判别式模型"></a>生成式模型和判别式模型</h2><p>从朴素贝叶斯, 到HMM; 从Logistic Regression到CRF, 这些概率图模型有如下转换关系:<br><img src="/images/relationship_nbs_hmm_lr_crf.png" alt="" title="Diagram of the relationship between naive Bayes, logistic regression, HMMs, linear-chain CRFs, generative models, and general CRFs. image from: An Introduction to Conditional Random Fields, by Charles Sutton and Andrew McCallum"></p><p>而在朴素贝叶斯与Logistic Regression, 以及HMM和CRF之间, 又有生成式和判别式的区别.</p><ul><li>生成式模型描述标签向量y如何有概率地<strong>生成</strong>特征向量x, 即尝试构建x和y的联合分布$p(y, x)$, 典型的模型有HMM，贝叶斯模型，MRF。生成式模型</li><li>而判别模型直接描述如何根据特征向量x判断其标签y, 即尝试构建$p(y | x)$的条件概率分布, 典型模型如如LR, SVM，CRF，MEMM等.</li></ul><p>原则上，任何类型的模型都可以使用贝叶斯规则转换为另一种类型，但实际上这些方法是不同的. 生成模型和判别模型都描述了$p(y, x)$的概率分布，但努力的方向不同。生成模型，例如朴素贝叶斯分类器和HMM，是一类可以因式分解为$p(y, x) = p(y)p(x|y)$的联合分布, 也就是说，它们描述了如何为给定标签的特征采样或“生成”值。生成式模型从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。生成式模型的优点是:<br>• 实际上带的信息要比判别模型丰富， 研究单类问题比判别模型灵活性强<br>• 能更充分的利用先验知识<br>• 模型可以通过增量学习得到<br>缺点也很明显: • 学习过程比较复杂; • 在目标分类问题中准确度不高</p><p>而判别式模型, 比如 LR, 是一系列条件分布$p(y | x)$. 也就是说，分类规则是直接建模的。原则上，判别模型也可通过为输入提供边际分布$p(x)$来获得联合分布$p(y, x)$，但很少需要这样。条件分布$p(y | x)$不包括$p(x)$的信息，在分类任务中其实无论如何也用不到。其次，对$p(x)$建模的困难之处在于它通常包含很多建模难度较高的有高度依赖性的特征。判别式模型寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。优点是:<br>• 分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。<br>• 能清晰的分辨出多类或某一类与其他类之间的差异特征<br>• 在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好<br>•适用于较多类别的识别<br>缺点是：• 不能反映训练数据本身的特性。• 能力有限，可以分类, 但无法把整个场景描述出来。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/" target="_blank" rel="noopener">http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/</a><br>Classical probabilistic models and conditional random fields<br><a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf" target="_blank" rel="noopener">An Introduction to Conditional Random Fields</a>, by Charles Sutton and Andrew McCallum</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;词性标注（Part-of-Speech-Tagging）&quot;&gt;&lt;a href=&quot;#词性标注（Part-of-Speech-Tagging）&quot; class=&quot;headerlink&quot; title=&quot;词性标注（Part-of-Speech-Tagging）&quot;&gt;&lt;/a&gt;词性标注（Part-of-Speech-Tagging）&lt;/h2&gt;&lt;p&gt;POS任务是指根据观察得到的序列, 推断出对应的词性标注, 比如&lt;code&gt;Bob drank coffee at Starbucks&lt;/code&gt;, 标注可能为&lt;code&gt;Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;除此之外, 还有命名实体识别, 以及其他涉及到需要根据观察序列推断隐含状态的问题, 都可以考虑用隐马尔可夫模型(Hidden Markov Model, HMM)或者条件随机场(conditional random fields, CRF)来处理.&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="http://shukebeta.me/NLP-recurrent-neural-networks/"/>
    <id>http://shukebeta.me/NLP-recurrent-neural-networks/</id>
    <published>2018-12-14T16:00:00.000Z</published>
    <updated>2019-01-01T06:46:51.583Z</updated>
    
    <content type="html"><![CDATA[<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>当人类阅读时，会根据对之前单词的理解和记忆来辅助理解当前看到的每个单词。也就是人能够很好地处理语言的长距离依赖特性（long-term dependency）。在自然语言处理任务中，很多传统的模型无法做到这一点，比如前馈神经网络；而传统的n-gram模型固然可以通过把把n系数增大来捕捉长距离依赖，但带来的非常巨大的内存消耗。<br><a id="more"></a></p><p>循环神经网络（Recurrent Neural Networks, RNNs)可以看做是多个<strong>共享参数</strong>的前馈神经网络不断叠加的结果<br><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="" title="A recurrent neural network and the unfolding in time of the computation involved in its forward computation. &quot;image from: http://colah.github.io"></p><p>这里的核心是想办法解码历史信息, 即通过递归方程$s_i = R(x_i, s_{i−1})$让$s_i$解码序列$x_{1:n}$. 比如把所有历史信息累加就是一种非常简单粗暴的方式, 这样得到的是连续词袋模型(continuous-bag-of-words model)$s_i = R_{CBOW}(x_i, s_{i-1}) = x_i + s_{i−1}$, 虽然简单，但这种RNN其实忽略了数据的时序性质。</p><p>一般意义上的RNN是指Elman Network or Simple-RNN (S-RNN)(<code>Elman [1990]</code>), $s_i = R_{SRNN}(x_i, s_{i-1}) = g(x_iW^x + s_{i−1}W^s + b)$, 也就是把历史信息先进行线性变换(乘以矩阵), 再和bias加起来, 再通过一个非线性激活函数(tanh或ReLU). 添加了线性变换再进行非线性激活, 使网络对输入的顺序变得敏感。</p><p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg" alt="" title="image from: Nature"><br>在使用时, 给定输入序列（单词序列或语音）得出输出序列的过程如下：</p><ul><li>把每个词$x_{t}$(以向量表示)逐个输入RNN</li><li>每一时间步$t$都有对应的隐含状态$s_t$，用于解码历史信息: $s_t = g(Ux_t + Ws_{t-1} + b)$.</li><li>每一时间步都可以有一个输出（虽然大部分应用只用到最后一时间步）$o(t)$： 例如，语言模型想要预测下一个单词，那么输出就是在词汇表上的概率分布向量，$o_t = softmax(Vs_t)$.</li><li>其中，各个时间步共享几个参数矩阵（$U, V, W$）</li></ul><p>In addition to the above normal many to many structure RNNs, there are other non-sequence input or output: Many to one, e.g. when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. One to many: Music generation.<br><img src="http://karpathy.github.io/assets/rnn/diags.jpeg" alt="" title="source from http://karpathy.github.io/2015/05/21/rnn-effectiveness/"></p><p>除了应用于语言模型, RNNs 还可以应用于<br>· tagging, e.g. part-of-speech tagging, named entity recognition (many to many RNNs)<br>· sentence classification, e.g. sentiment classification (many to one RNNs)<br>· generate text, e.g. speech recognition, machine translation, summarization</p><h3 id="RNNs-Backpropagation"><a href="#RNNs-Backpropagation" class="headerlink" title="RNNs Backpropagation"></a>RNNs Backpropagation</h3><p>Backpropagation Through Time (BPTT): Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps.</p><p>RNNs trained with BPTT have difficulties learning long-term dependencies (e.g. dependencies between steps that are far apart) due to what is called the vanishing/exploding gradient problem.</p><h3 id="梯度消失与爆炸"><a href="#梯度消失与爆炸" class="headerlink" title="梯度消失与爆炸"></a>梯度消失与爆炸</h3><p>The Vanishing/Exploding Gradient problem。</p><p>RNNs shares the same matrix (w, u, etc.) at each time step during forward prop and backprop. 求导数时, 根据链式法则, loss对各参数的导数会转换为loss对输出y的导数, 乘以y对隐含层的导数, 乘以隐含层相对隐含层之间的导数, 再乘以隐含层对参数的导数.<img src="/images/vanish_gradient.png" alt=""></p><p>不同隐含层（举例如$h_t$和$h_k$）之间如果相隔太远, $h_t$对$h_k$的导数就变成多个jacobian矩阵的相乘， 对各个jacobian范数（norms）进行分析后，发现$h_t$对$h_k$的导数值在训练过程中会很快变得很极端（非常小或者非常大）。</p><p>Gradient作为传导误差以帮助系统纠正参数的关键角色，如果本身变得接近于<code>0</code>或者<code>nan</code>，那么我们就无法判断t和t+n的数据的依赖性（是没有依赖？还是因为vanish of gradient？还是因为参数设置错误？）。梯度衰减会直接降低模型学习长距离依赖关系的能力，给定一个时间序列，例如文本序列，循环神经网络较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。</p><p>在使用RNN学习language model的时候，非常容易出现梯度爆炸，解决办法是使用 gradient clipping 梯度裁剪，就是通过把梯度映射到另一个大小的空间，以限制梯度范数的最大值<a href="https://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">On the difficulty of training Recurrent Neural Networks</a>。</p><p>虽然梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。一个缓解梯度衰减的方案是使用更好的参数初始化方案和激活函数（ReLUs）<a href="https://arxiv.org/abs/1504.00941" target="_blank" rel="noopener">A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</a>.</p><p>不过更主流的解决梯度衰减的方案是使用更复杂的rnn隐含单元: Gated Recurrent Units (GRU) introduced by <a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a> and LSTMs.</p><h2 id="门控循环网络"><a href="#门控循环网络" class="headerlink" title="门控循环网络"></a>门控循环网络</h2><p>因为梯度消失的问题，RNN的解码能力是很有限的。S-RNN架构的一个明显缺陷是对历史信息的记忆是不受控制，在每一时间步的计算，读写整个记忆状态$s_t$。而门控循环网络，比如Long Short-Term Memory（LSTMs），Gated Recurring Unit（GRUs），使用<strong>门</strong>的概念，让网络拥有控制哪些信息需要记录, 哪些需要丢弃的能力。如何实现这种门呢? 考虑一种介于<code>[0, 1]</code>中间的因子, 让这种因子与各种状态信息相乘, 可以为每个状态信息独自训练一个因子, 也就是由简单的神经网络(非线性激活函数Sigmoid)来控制.</p><p>是否允许信息通过（打开）或不通过（关闭）取决于其门控单元内部Sigmoid激活层的点乘运算。Sigmoid函数值介于0和1之间，可用于描述允许通过单元格的信息量。</p><p>LSTM架构将状态向量$s_i$分成两半，其中一半被视为“记忆单元”$C$, 而另一半被视为一般的工作存储单元-隐含状态$h$。</p><p>1, LSTM用<strong>遗忘门</strong>来决定从前一时间步的记忆单元中丢弃哪些信息，控制当前记忆单元应该忘记多少来自前一步状态$h_{t-1}$的信息量，标记为<strong>遗忘信息</strong>。遗忘门由一个sigmoid层学习而来 <img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="" title="image from: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></p><p>2, 用<strong>输入门</strong> Input gate (a sigmoid hidden layer) 来决定有多少新信息是值得储存的（当前时间步$t$）。输入门控制哪些信息需要更新. 再通过一个隐含层(tanh/relu)生成新的<strong>候选信息</strong>向量$\widetilde{C}_t$. <img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="" title="image from: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></p><p>输入门和遗忘门一起，控制每一步的信息存储和改写, 将遗忘信息和候选信息组合在一起作为<strong>更新信息</strong>，作为当前时间步的新记忆单元，$C_{t}$.<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="" title="image from: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></p><p>3, 最后，用一个<strong>输出门</strong> Output gate (a sigmoid layer) 来控制多少记忆单元作为当前步的工作隐含状态$h_t$。先通过一个tanh激活层把当前记忆单元$C_t$推送为<code>[-1, 1]</code>之间的值, 再乘以输出门.<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="" title="image from: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></p><p>总的来说, LSTM有遗忘门, 输入门和输出门这<strong>三个门</strong>. 加上其中的更新信息, 形式上LSTM有<strong>四个神经网络</strong>, 输入都是上一步隐含状态和当前步的输入向量的.</p><h3 id="GRUs"><a href="#GRUs" class="headerlink" title="GRUs"></a>GRUs</h3><p>LSTMs有两种隐含状态, 但GRUs (<a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho, et al. (2014)</a>)抛弃了这种设计, 只用一种隐含状态$h$. 这导致了一系列的变化.</p><p>GRUs首先根据当前的输入词向量和隐含状态计算<strong>更新门</strong>$z_t$和一个<strong>重置门</strong>$r_t$。重置门用于协助计算隐含层的更新信息$\widetilde{h}_t$. 输出的隐含状态$h_t$是由更新门$z_t$来控制.<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" alt="" title="image from: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></p><p>LSTMs的遗忘门和输入门的合作核心是<strong>更新信息</strong>. 而GRUs将LSTMs的遗忘门和输入门合并成一个更新门, 抛弃了输出门的概念, 让更新门负责计算新的隐含状态. 这样GRUs内部总共只有两个门, 三个神经网络, 某种程度上简化了LSTMs模型。</p><p>GRU intuition</p><ul><li>重置门赋予了模型丢弃与未来无关的信息的能力。若重置门接近于0，则忽略之前的记忆，仅储存新加入的信息.</li><li>更新门控制过去的状态对现在的影响程度（即决定更新多少），如果接近于1，则 $h_t=z_t\cdot h_{t-1}$, 等同于把过去的信息完整复制到未来，相应地缓解梯度衰减。</li><li>短距离依赖的单元，过去的信息仅保留很短的时间，重置门一般很活跃，也就是数值在0和1之间频繁变动。</li><li>长距离依赖的单元，重置门较稳定（保留过去的记忆较长时间），而更新门较活跃。</li></ul><h3 id="不同RNNs变种的比较"><a href="#不同RNNs变种的比较" class="headerlink" title="不同RNNs变种的比较"></a>不同RNNs变种的比较</h3><p>Vanilla RNNs Execution:</p><ol><li>Read the whole register h</li><li>Update the whole register</li></ol><p>GRU Execution:</p><ol><li>Select a readable subset</li><li>Read the subset</li><li>Select a writable subset</li><li>Update the subset</li></ol><p><img src="/images/gru.vs.lstm.png" alt="" title="image from: http://web.stanford.edu/class/cs224n"></p><h3 id="门控循环神经网络的训练"><a href="#门控循环神经网络的训练" class="headerlink" title="门控循环神经网络的训练"></a>门控循环神经网络的训练</h3><ol><li>把参数矩阵初始化为正交</li><li>把遗忘门的bias初始化为1，默认不遗忘</li><li>别忘了梯度裁剪</li><li>注意dropout在RNNs中的应用不同于DNN和CNN</li></ol><h2 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h2><p>Bidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs.<br><img src="/images/bidirectional_rnn.png" alt="" title="image from: http://web.stanford.edu/class/cs224n"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>斯坦福cs224n <a href="http://web.stanford.edu/class/cs224n" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n</a></p><p><a href="http://colah.github.io" target="_blank" rel="noopener">http://colah.github.io</a></p><p>Neural Network Methods in Natural Language Processing, by Yoav Goldberg</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;循环神经网络&quot;&gt;&lt;a href=&quot;#循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络&quot;&gt;&lt;/a&gt;循环神经网络&lt;/h2&gt;&lt;p&gt;当人类阅读时，会根据对之前单词的理解和记忆来辅助理解当前看到的每个单词。也就是人能够很好地处理语言的长距离依赖特性（long-term dependency）。在自然语言处理任务中，很多传统的模型无法做到这一点，比如前馈神经网络；而传统的n-gram模型固然可以通过把把n系数增大来捕捉长距离依赖，但带来的非常巨大的内存消耗。&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>语言模型</title>
    <link href="http://shukebeta.me/NLP-language-model/"/>
    <id>http://shukebeta.me/NLP-language-model/</id>
    <published>2018-11-11T16:00:00.000Z</published>
    <updated>2019-01-03T08:55:39.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>语言模型Language modeling（LM）最初是针对语音识别问题而开发的, 现在广泛用于其他NLP应用中, 比如机器翻译需要利用LM来给翻译出的句子打分.<br><a id="more"></a><br>假设我们有一个语料库 - 某种语言的句子的无限集合$\mathcal{V^+}$（这些句子是由有限的词$\mathcal{V}$组成的）。例如，我们可能从网上获得大量文本。给定了此语料库，我们想估计LM的参数。这些参数包含语料库中所有单词的有限集合$\mathcal{V}$, 以及句子的概率分布函数$p(x_1, x_2, …, x_n)$，必须满足</p><ol><li>For any $\langle x_1…x_n \rangle \in \mathcal{V^+}$, $p(x_1, x_2, …, x_n) ≥ 0$</li><li>$\sum_{\langle x_1…x_n \rangle \in \mathcal{V^+}}p(x_1, x_2, …, x_n) = 1$</li></ol><p>比如，当$\mathcal{V}$只有<code>cat, eat, fish</code>, 那么它组合成的句子按照人类的评价标准, 通顺程度从高到低是: <code>cat eat fish</code>, <code>fish eat cat</code>, <code>cat fish eat</code>, <code>eat cat fish</code>, <code>eat fish cat</code>, <code>fish cat eat</code>. 这些是可能出现的句子(还没出现的不代表未来不会出现), 从概率分布的角度看待, 这些句子的概率之和是<code>1</code>, 因为这三个词只能组成这几个句子. 而LM的意义就在于能够赋予<code>cat eat fish</code>最大的概率, 代替人来判断句子是否准确, 通俗的说是一个句子通顺打分机器.</p><p>广义的语言模型, 可以计算任何连续的单词或者任何其他序列数据（比如语音）出现的概率, 当然是以参数的训练样本的角度来看待。除了为每个词序列指定概率之外，语言模型还指定给定的单词（或单词序列）跟随前面的单词序列的似然概率。</p><p>语言模型本身即是一种概率模型. 概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟一个事件发生的概率。</p><p>LM的任务就是为单词序列$w_{1:n}$分配概率$P(w_{1:n})$, 等同于给序列的每个位置预测可能出现的单词，给定前面的单词（作为条件），预测下一个单词出现的概率 <code>P(w|w1, w2, w3...)</code>。听起来有点像词性标注(Tagging)… 事实上最初为语言建模开发的参数估计技术也给词性标注做了不少贡献.</p><p>利用链式法则, $$P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})P(w_4|w_{1:3})…P(w_n|w_{1:n-1}),$$ 最后一项基于<code>n-1</code>个词的条件概率计算难度非常大。为了简化LM参数的训练，利用<code>k</code>阶马尔可夫假设，声明序列的下一个词仅依赖于前<code>k</code>个词。如利用一阶马尔可夫假设得到<code>P(transparent | the water is so ) ≈ P(transparent | so)</code>.</p><p>使用马尔可夫假设简化前面的乘链:<br>$$\begin{align}<br>P(w_{1:n}) &amp;= \prod_{i=1}^n P(w_i | w_1, …, w_{i-1}) \\<br>&amp;\propto \prod_{i=1}^n P(w_i | w_{i-k}, …, w_{i-1}) \end{align}$$<br>在语料处理时，开头的句子前面需要相应的加上<code>k</code>个补丁符号<code>&lt;s&gt;</code>，才能计算第一个词的条件概率。LM也是一种生成模型, 一般是在句子末尾加上特殊符号<code>&lt;/s&gt;</code>表示句子结束, 以方便生成任务时判断句子的生成结束.</p><p>固然Markov假设对于任意<code>k</code>阶都是有偏差的（毕竟句子可以有任意长的依赖性），但仍可以使用较小的k建模出较强的LM，并且几十年来一直是语言建模的主要方法。</p><p>对于LM参数中每一项似然概率的估算，可以使用<strong>最大似然估计（MLE）</strong>：$$P(w_{i}=m|w_{i-k:i-1}) = \frac{Count(w_{i-k:i})}{Count(w_{i-k:i-1})}$$</p><p>这个就是经典的N-gram模型。</p><h2 id="N-Gram语言模型"><a href="#N-Gram语言模型" class="headerlink" title="N-Gram语言模型"></a>N-Gram语言模型</h2><p><code>N-Gram</code>语言模型是基于<code>N-1</code>阶马尔可夫假设且由MLE估算出的LM。<code>N-Gram</code>LM 预测下一个单词出现概率仅条件于前面的<code>(N-1)</code>个单词, 以<code>The students opened their books</code>为例:</p><ul><li><code>Bi-gram</code>: 统计$P(w_{i}=m|w_{i-1})$, <code>P(students | the)</code>, <code>P(opened | students)</code>, …, 属于<code>马尔可夫一阶模型</code>, 即当前<code>t</code>时间步的状态仅跟<code>t-1</code>相关.</li><li><code>Tri-gram</code>: <code>P(students | &lt;/s&gt; The)</code>, <code>P(opened | The students)</code>, <code>马尔可夫二阶模型</code></li><li><code>Four-gram</code>: 依此类推</li></ul><p>特殊的<code>Uni-gram</code>: 统计$P(w_i)$, <code>P(the)</code>, <code>P(students)</code>, …, 此时整个模型退化为词袋模型, 不再属于马尔可夫模型, 而是基于贝叶斯假设, 即各个单词是条件独立的. 所以一般<code>N-gram</code>是指<code>N&gt;1</code>的.</p><p>N-Gram模型因为使用MLE估算参数，缺点很明显：</p><ul><li>无法很好地解决NLP中的长距离依赖现象, 比如一般表现比较好的Trigram语言模型，没有考虑到两步之外的词</li><li>没有考虑词的相似性，泛化能力差。比如在训练集出现了<code>The cat is walking in the bedroom</code>,理论上应该泛化到给<code>A dog was running in a room</code>, 因为<code>dog</code>和<code>cat</code>(resp. “the” and “a”, “room” and “bedroom”, etc…)有类似的语义和语法定位.</li><li>N-gram只是在测试语料库与训练语料库比较相似时表现才比较好。否则基于训练语料训练出来的参数肯定无法很好地评估测试语料，就像人无法对其不认识的语言做任何语法句法上的评价。</li><li>稀疏问题1：大多数高阶Gram几乎不会出现，虽然<code>u v w</code>在训练语料中从来没有出现过, 但我们不能简单地把<code>P(w | u, v)</code>定义为0，因为语言是千变万化的，有些词组虽然少见但不代表不存在。句子的概率是由各个gram似然概率相乘而来，如果仅仅因为一个词组出现次数为0就导致整个句子概率变为0, 那显然是不合理的.</li><li>稀疏问题2：部分低阶gram没有出现过，低阶gram的次数作为MLE公式中分母变为0，那计算就没法进行下去了.</li><li>一般而言，N越高，模型表现越好，但是更大的N使稀疏问题变得更糟。通常人们不会取大于5的N。</li><li>需要存储所有可能的N-Gram，所以模型的大小是 <code>O(exp(n))</code>, 需要大量的内存，而其实大部分都是出现次数为0.</li></ul><h3 id="平滑"><a href="#平滑" class="headerlink" title="平滑"></a>平滑</h3><p>针对数据稀疏问题（0概率的问题）, 可以使用各种平滑处理（Smoothing）.</p><p>加一（Laplace）平滑：最简单的平滑法，为所有事件（不管有没出现过）的频次加一，这样保证了没有0概率事件出现。这种平滑效果很差，因为齐夫定律<code>Zipf&#39;s law</code>的关系</p><blockquote><p><code>Zipf&#39;s law</code>：在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。</p></blockquote><p>会有很多长尾单词很少甚至几乎没有出现过, 所以在总数为1的概率池子里, 为了给这些长尾单词分配至少频次1的概率, 需要从真正出现的单词(所谓真实发生的事件)中分走很多概率.</p><p>因此可以给Laplace平滑加入控制因子，变为 Add alpha smoothing。更多平滑方案参考<a href="/UoE-anlp#平滑Smoothing">UoE-anlp</a></p><h2 id="语言模型评估方法"><a href="#语言模型评估方法" class="headerlink" title="语言模型评估方法"></a>语言模型评估方法</h2><p>既然LM是用于评估句子是否准确的模型，那么在评价LM好坏时，就要看它在测试集上的表现如何。给定测试集包含$m$个句子$x^{(1)}, x^{(2)}, …, x^{(m)}$, 各个句子的长度分别为$n_i$. LM给这些测试集句子评估的概率大小为$$\prod_{i=1}^m p(x^{(i)})$$ 这个数值越高，说明LM评估测试集句子的质量越好。<strong>注意, 测试集必须是完全没有参与模型训练, 且是在人类标准中是好的句子.</strong></p><p>但在实际使用中, 我们往往使用上面这个概率的一种变换 - <code>困惑度</code>（<code>Perplexity</code>）来评价LM的质量. 首先取整个测试语料库的对数概率除以测试语料库中的单词总数$M$: $$l = \frac{1}{M} \log_2 \prod_{i=1}^m p(x^{(i)}) = \frac{1}{M} \sum_{i=1}^m \log_2 p(x^{(i)})$$<br>然后得到<br>$$\begin{align}<br>Perplexity &amp;= 2^{-l} \\<br>&amp;= 2^{-\frac{1}{M} \sum_{i=1}^m \log_2 p(x^{(i)})}\\<br>\\<br>&amp;= t^{-1}<br>\end{align}$$<br>其中，$t = \sqrt[\leftroot{-2}\uproot{2}M]{\prod_{i=1}^m p(x^{(i)})}$, 作为测试集概率的几何平均. 例如，如果困惑等于100，则$t = 0.01$，表明几何平均值为0.01. 可以看到, Perplexity的值越小，语言模型建模测试集的能力就越好.</p><p>概率取对数转换可以避免数值下溢，可以把乘法转换为加法, 计算也更快.</p><p>困惑度为何就是一种好的衡量标准呢？对于任何一个任务，我们需要定义Baseline模型作为基准，如果后续有一个新的模型，但无法超过此baseline，那么我们认为这个新的模型是没有进步的。对于语言建模这一个任务，最无脑最简单的baseline，就是假设每一个位置的每个单词出现概率相等，这就是最大熵分布，即假设此baseline对这个任务一无所知，所有位置所有单词在它眼里都是没区别的(均匀分布)。如果词汇集(包含<code>&lt;/s&gt;</code>)大小为<code>N</code>, 那么$$P_{i \in T}(w_i | w_{1:i-1}) = \frac{1}{N},$$ 此时的困惑度等于<code>N</code>, 即在均匀概率分布模型下，困惑度等于词汇量的大小。显而易见任何一个有效模型的困惑度必须小于类别个数. 此时困惑度可以理解为模型的<strong>有效词汇量</strong>：例如，词汇量大小为10,000, 而模型的困惑度为120，那么这大致说明有效的词汇量只有大概120个。最佳情况下，模型总是把测试集的概率预测为 1, 此时困惑度为 1。最坏情况下，概率预测为 0, 此时困惑度为正无穷。Baseline模型总是预测所有类别的概率都相同, 此时困惑度为词汇量大小（类别个数）。</p><p>目前很多神经网络框架计算语言模型的损失函数都是用交叉熵损失函数并取对数,<br>要得到perplexity，只需要把这个loss取指数运算。</p><p>那么困惑度一般都是多大呢？Goodman (“A bit of progress in language modeling”, figure 2) 评估了在英语数据上的unigram，bigram和trigram语言模型，词汇量为50,000。Goodman的报告结果显示，trigram模型的困惑度约为74，bigram模型为137，unigram模型为955。相比于Baseline模型困惑度50,000，trigram模型显然有了巨大的改进，且比bigram和unigram模型也有很大的改进。而更强大的SOTA神经语言模型，可以在wikitext-2数据集上跑出40以下的困惑度。</p><h2 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h2><p>神经网络模型解决了传统语言模型的一些缺点：它们允许越来越长的距离依赖，而参数数量仅线性增加，它们减少了手动设计backoff顺序的需要，并且它们支持跨不同上下文的泛化。</p><p><code>Bengio et al. [2003]</code>提出的神经网络语言模型(NNLM, 确切的说是前馈神经网络语言模型), 把文本处理成n个k-gram词窗口$w_{i:i+k-1}$,  每个词转换为词镶嵌的形式$\mathcal{v}(w) \in \mathcal{R}^{d_w}$, 一整个窗口的词向量拼接为矩阵向量$x = [\mathcal{v}(w_0); …; \mathcal{v}(w_{k-1})]$, 作为输入数据输入到一个1到2层的感知机.</p><p>训练数据的处理一般这么操作, 每个句子的开头加上<code>&lt;s&gt;</code>, 末尾加上<code>&lt;/s&gt;</code>, 然后按照k大小的长度一段段截断成k-gram词窗口$w_{i:i+k-1}$. 每一段k-gram的词拼接为一个向量$x = (C(w_{i}), C(w_{i+1}), ···, C(w_{i+k-1}))$, 作为一个训练样本, 其末尾的下一个词$w_{i+k}$作为样本对应的预测标签$y_i = \mathcal{v}(w_{i+k})$. 训练时，以输出的词向量概率分布向量和对应正确标签的 one-hot-vector 间的 cross-entropy loss 为损失函数.</p><p>神经网络的参数数量比传统的N-gram少，因为其每增加一个词，参数就多$d_w$, 也就是线性增加, 而N-gram是多项式增加速率. 并且NNLM的参数矩阵对所有输入都是共享的, 这进一步减少了参数量. 虽然如此, NNLM的训练时间还是比N-gram LM长.</p><p>神经网络语言模型的泛化能力更好，因为相似的词具有相似的特征向量，并且因为概率函数（模型参数）是这些特征值的平滑函数，所以特征的微小变化相应地引起概率的微小变化。</p><p>真正影响NNLM计算效率的是输出层的softmax计算, 因为训练样本的词汇量$\mathcal{V}$往往很大. 输出层的softmax需要与隐含层参数矩阵$W^2 \in \mathcal{R}^{d_{hid} \times \mathcal{V}}$进行昂贵的矩阵向量乘法, 然后进行$\mathcal{V}$次对数操作. 这部分计算占据了大部分运行时间，使得大词汇量的NNLM建模令人望而却步。</p><p>后续发展的NNLM普遍使用循环神经网络（RNN, LSTM）来代替简单的前馈神经网络。循环神经网络可以理解为多层前馈神经网络叠加, 但各神经网络隐含层的参数是共享的. 句子逐词输入循环神经网络, 也就是循环神经网络使用同样参数方程来处理每一个词, 因此循环神经网络的参数量比前馈神经网络更少. 使用循环神经网络作为LM模型时, 同样最后一层还是使用softmax输出层。不同的是输入不再局限于定长的kgram词窗口，LSTM理论上可以接受无限长序列, 但事实上LSTM的记忆能力也是有限的, 太长就会遗忘掉前面的信息.</p><h3 id="对大词汇量语言模型的尝试"><a href="#对大词汇量语言模型的尝试" class="headerlink" title="对大词汇量语言模型的尝试"></a>对大词汇量语言模型的尝试</h3><p>Hierarchical softmax [Morin and Bengio, 2005]</p><p>Self-normalizing aproaches, 比如 noise-contrastive estimation (NCE) <code>[Mnih and Teh, 2012, Vaswani et al., 2013]</code> 或者在训练目标函数中加入正则化项 <code>[Devlin et al., 2014]</code>.</p><p>有关处理大输出词汇表的这些和其他技术的良好评论和比较，请参阅 <code>Chen et al. [2016]</code>.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>class notes by Michael Collins: <a href="http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf</a><br>Neural Network Methods in Natural Language Processing, by Yoav Goldberg</p><p>A Neural Probabilistic Language Model, Yoshua Bengio, 2003</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;语言模型&quot;&gt;&lt;a href=&quot;#语言模型&quot; class=&quot;headerlink&quot; title=&quot;语言模型&quot;&gt;&lt;/a&gt;语言模型&lt;/h2&gt;&lt;p&gt;语言模型Language modeling（LM）最初是针对语音识别问题而开发的, 现在广泛用于其他NLP应用中, 比如机器翻译需要利用LM来给翻译出的句子打分.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 08 - 数据压缩 - 哈夫曼编码</title>
    <link href="http://shukebeta.me/NLP-08-data-compression-huffman-compression/"/>
    <id>http://shukebeta.me/NLP-08-data-compression-huffman-compression/</id>
    <published>2018-10-11T16:00:00.000Z</published>
    <updated>2018-12-17T13:06:05.733Z</updated>
    
    <content type="html"><![CDATA[<h2 id="避免歧义的编码"><a href="#避免歧义的编码" class="headerlink" title="避免歧义的编码"></a>避免歧义的编码</h2><p>在构建压缩编码的对应关系时，我们使用不同的数量的位来编码不同的字符.<br><a id="more"></a><br>比如摩斯密码<img src="/images/Morse_Code.png" alt="" title="Chart of the Morse code letters and numerals.">. 如果单纯使用这种对应关系，会出现一些问题， 如<code>•••−−−•••</code>会产生歧义: <code>SOS</code>? <code>V7</code>? <code>IAMIE</code>? <code>EEWNI</code>? 所以在实际使用中, 密码使用一些间隔来分隔代码字。</p><p>那么对于不同的压缩编码, 有什么常用方法来避免歧义？<br>方法是确保没有一个编码是另一个编码的前缀。比如</p><ul><li>使用固定长度编码。</li><li>为每个编码添加特殊的stop char。</li><li><strong>使用一种具备广泛使用性的prefix-free编码</strong>。</li></ul><p>用什么数据结构来设计prefix-free编码?</p><h3 id="用Trie构造编码"><a href="#用Trie构造编码" class="headerlink" title="用Trie构造编码"></a>用Trie构造编码</h3><p>一个二叉(<code>0, 1</code>)Trie: 叶节点是字符, 根节点到叶节点的路径就是编码.<img src="/images/huffman_trie.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"></p><p>压缩:<br>方法1：从叶开始; 按照路径到达根; 反向打印bits。<br>方法2：创建<code>键-值</code>对的符号表。</p><p>解压:</p><ol><li>从根节点开始, 根据位值是0还是1在Trie图上游走, 直到走到叶节点，则解压出一个字符</li><li>返回根节点, 继续第一步, 直到跑完所有编码.</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">Node</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">char</span> ch;   <span class="comment">// used only for leaf nodes</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> freq;  <span class="comment">// used only for compress</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node left, right;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(<span class="keyword">char</span> ch, <span class="keyword">int</span> freq, Node left, Node right)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.ch    = ch;</span><br><span class="line">      <span class="keyword">this</span>.freq  = freq;</span><br><span class="line">      <span class="keyword">this</span>.left  = left;</span><br><span class="line">      <span class="keyword">this</span>.right = right;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isLeaf</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;  <span class="keyword">return</span> left == <span class="keyword">null</span> &amp;&amp; right == <span class="keyword">null</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// compare Nodes by frequency</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Node that)</span></span></span><br><span class="line"><span class="function">    </span>&#123;  <span class="keyword">return</span> <span class="keyword">this</span>.freq - that.freq;  &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Runtime - Linear in input size N</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">expand</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">       Node root = readTrie(); <span class="comment">// read in encoding trie</span></span><br><span class="line">       <span class="keyword">int</span> N = BinaryStdIn.readInt(); <span class="comment">// read in number of chars</span></span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">       &#123;</span><br><span class="line">          Node x = root;</span><br><span class="line">          <span class="keyword">while</span> (!x.isLeaf())</span><br><span class="line">          &#123;</span><br><span class="line">             <span class="keyword">if</span> (!BinaryStdIn.readBoolean())</span><br><span class="line">                x = x.left;</span><br><span class="line">             <span class="keyword">else</span></span><br><span class="line">                x = x.right;</span><br><span class="line">          &#125;</span><br><span class="line">          BinaryStdOut.write(x.ch, <span class="number">8</span>);</span><br><span class="line">       &#125;</span><br><span class="line">       BinaryStdOut.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如何读取一个Trie：根据Trie的前序遍历序列重构.<img src="/images/preorder_traversal_trie.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Node <span class="title">readTrie</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">if</span> (BinaryStdIn.readBoolean())</span><br><span class="line">   &#123;</span><br><span class="line">      <span class="keyword">char</span> c = BinaryStdIn.readChar(<span class="number">8</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Node(c, <span class="number">0</span>, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">   &#125;</span><br><span class="line">   Node x = readTrie();</span><br><span class="line">   Node y = readTrie();</span><br><span class="line">   <span class="keyword">return</span> <span class="keyword">new</span> Node(<span class="string">'\0'</span>, <span class="number">0</span>, x, y);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>如何把Trie写为序列：以前序遍历的方式写Trie；额外用一个位标记是否叶节点。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeTrie</span><span class="params">(Node x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">if</span> (x.isLeaf())</span><br><span class="line">   &#123;</span><br><span class="line">      BinaryStdOut.write(<span class="keyword">true</span>);</span><br><span class="line">      BinaryStdOut.write(x.ch, <span class="number">8</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   BinaryStdOut.write(<span class="keyword">false</span>);</span><br><span class="line">   writeTrie(x.left);</span><br><span class="line">   writeTrie(x.right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="用哈夫曼算法构建最优编码"><a href="#用哈夫曼算法构建最优编码" class="headerlink" title="用哈夫曼算法构建最优编码"></a>用哈夫曼算法构建最优编码</h3><p>就是用Huffman算法. Huffman算法是把最短的编码赋给出现频率最高的字符, 把最长的编码留给出现频率较低的字符. 在Trie上的效果就变成频率最高的字符路径最短, 长路径都留给频率低的字符. 这样总的效果就是使用了更少的数据位来表达同样的信息.</p><ol><li>统计输入的各个字符的频率<code>freq[i]</code>。</li><li>为每个<code>char i</code>构建一个具有权重<code>freq[i]</code>的Trie(子节点为null), 从此节点开始</li><li>重复以下过程直到融合为一个trie(根节点)：<ul><li>选择当前权重最小的两Tries, <code>freq[i]</code>和<code>freq[j]</code>, 其中<code>i &lt;= j, freq[i] &lt;= freq[j]</code></li><li>给它们创建父节点, 权重为<code>freq[i] + freq[j]</code>, 两个子Trie和其父节点合并为一个Trie, 而且路径0(左边)总是指向较小的子Trie, 路径1(右边)指向较大的.</li></ul></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Node <span class="title">buildTrie</span><span class="params">(<span class="keyword">int</span>[] freq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    MinPQ&lt;Node&gt; pq = <span class="keyword">new</span> MinPQ&lt;Node&gt;();</span><br><span class="line">    <span class="comment">// initialize PQ with singleton tries</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">char</span> i = <span class="number">0</span>; i &lt; R; i++)</span><br><span class="line">       <span class="keyword">if</span> (freq[i] &gt; <span class="number">0</span>)</span><br><span class="line">          pq.insert(<span class="keyword">new</span> Node(i, freq[i], <span class="keyword">null</span>, <span class="keyword">null</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (pq.size() &gt; <span class="number">1</span>)</span><br><span class="line">    &#123;  <span class="comment">// merge two smallest tries</span></span><br><span class="line">       Node x = pq.delMin();</span><br><span class="line">       Node y = pq.delMin();</span><br><span class="line">       Node parent = <span class="keyword">new</span> Node(<span class="string">'\0'</span>, x.freq + y.freq, x, y);</span><br><span class="line">       pq.insert(parent);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pq.delMin();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过这个算法, 可以保证频率最高(权重最大)的字符的叶节点就是最左叶节点, 一般编码为<code>0</code>, 其他依次类推. 可以证明Huffman算法生成的最优prefix-free编码.</p><p><a href="https://algs4.cs.princeton.edu/55compression/Huffman.java.html" target="_blank" rel="noopener">完整代码见</a></p><p>Implementation.<br>・Pass 1:  tabulate char frequencies and build trie.<br>・Pass 2:  encode file by traversing trie or lookup table</p><p>Running time. Using a binary heap ⇒ <code>N + R log R</code>. N input size, R alphabet size.</p><p>对于具有n个叶子节点的哈夫曼树，一共需要<code>2*n-1</code>个节点: 二叉树有三种类型节点，即子节点数为2的节点，为1的节点和为0的叶节点。而哈夫曼树的非叶子节点是由两个节点生成的，因此不能出现只有单子节点的节点，如果叶子节点个数为n, 那么非叶子节点的个数为<code>n-1</code>.</p><p>哈夫曼编码广泛应用于jpeg, pdf, MP3, MP4等文件编码中.</p><p>在神经网络中, 哈夫曼树也被用于构建层级Softmax.</p><p>一个使用Huffman Encoding的实例：<br><a href="https://github.com/congchan/cs106b-programming-abstraction/tree/master/HW6_Huffman%20Encoding/Huffman/src" target="_blank" rel="noopener">https://github.com/congchan/cs106b-programming-abstraction/tree/master/HW6_Huffman%20Encoding/Huffman/src</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;避免歧义的编码&quot;&gt;&lt;a href=&quot;#避免歧义的编码&quot; class=&quot;headerlink&quot; title=&quot;避免歧义的编码&quot;&gt;&lt;/a&gt;避免歧义的编码&lt;/h2&gt;&lt;p&gt;在构建压缩编码的对应关系时，我们使用不同的数量的位来编码不同的字符.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Information Retrieval" scheme="http://shukebeta.me/tags/Information-Retrieval/"/>
    
      <category term="Data Compression" scheme="http://shukebeta.me/tags/Data-Compression/"/>
    
      <category term="Huffman Compression" scheme="http://shukebeta.me/tags/Huffman-Compression/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 07 - 数据压缩</title>
    <link href="http://shukebeta.me/NLP-07-data-compression-introduction/"/>
    <id>http://shukebeta.me/NLP-07-data-compression-introduction/</id>
    <published>2018-10-09T16:00:00.000Z</published>
    <updated>2018-12-17T13:06:31.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><p>压缩数据以节省储存空间，节省传输时间。同时很多文件都有很多冗余信息，这为压缩提供了很多可能性。<br><a id="more"></a><br>通用文件压缩<br>·文件：GZIP，BZIP，7z<br>·Archivers：PKZIP<br>·文件系统：NTFS，HFS +，ZFS</p><p>多媒体<br>·图像：GIF，JPEG<br>·声音：MP3<br>·视频：MPEG，DivX™，HDTV</p><p>通讯<br>·ITU-T T4 Group 3 Fax<br>·V.42bis调制解调器<br>·Skype</p><p>数据库</p><h3 id="压缩率"><a href="#压缩率" class="headerlink" title="压缩率"></a>压缩率</h3><p><code>Compression ratio = Bits in Compressed B / bits in B</code>.</p><blockquote><p>自然语言的压缩率为50-75％或更高.</p></blockquote><h3 id="读写二进制"><a href="#读写二进制" class="headerlink" title="读写二进制"></a>读写二进制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public class BinaryStdIn &#123;</span><br><span class="line">    boolean readBoolean() // read 1 bit of data and return as a boolean value</span><br><span class="line">    char readChar() // read 8 bits of data and return as a char value</span><br><span class="line">    char readChar(int r) // read r bits of data and return as a char value</span><br><span class="line">    // similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits)</span><br><span class="line">    boolean isEmpty() // is the bitstream empty?</span><br><span class="line">    void close() // close the bitstream</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class BinaryStdOut &#123;</span><br><span class="line">    void write(boolean b) // write the specified bit</span><br><span class="line">    void write(char c) // write the specified 8-bit char</span><br><span class="line">    void write(char c, int r) // write the r least significant bits of the specified char</span><br><span class="line">    // similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits)</span><br><span class="line">    void close() // close the bitstream</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>比如使用三种方法表达<code>12/31/1999</code><br>1, A character stream (StdOut),<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StdOut.print(month + <span class="string">"/"</span> + day + <span class="string">"/"</span> + year);</span><br></pre></td></tr></table></figure></p><p><code>00110001</code> 1<br><code>00110010</code> 2<br><code>00101111</code> /<br><code>00110111</code> 3<br><code>00110001</code> 1<br><code>00101111</code> /<br><code>00110001</code> 1<br><code>00111001</code> 9<br><code>00111001</code> 9<br><code>00111001</code> 1<br>共 80bits<br>2, Three ints (BinaryStdOut)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BinaryStdOut.write(month);</span><br><span class="line">BinaryStdOut.write(day);</span><br><span class="line">BinaryStdOut.write(year);</span><br></pre></td></tr></table></figure></p><p><code>00000000 00000000 00000000 00001100</code> 12<br><code>00000000 00000000 00000000 00011111</code> 31<br><code>00000000 00000000 00000111 11001111</code> 1999<br>共96bits<br>3，A 4-bit field, a 5-bit field, and a 12-bit field (BinaryStdOut)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BinaryStdOut.write(month, <span class="number">4</span>);</span><br><span class="line">BinaryStdOut.write(day, <span class="number">5</span>);</span><br><span class="line">BinaryStdOut.write(year, <span class="number">12</span>);</span><br></pre></td></tr></table></figure></p><p><code>1100</code> 12<br><code>11111</code> 13<br><code>0111110 01111</code> 1999<br>共21bits</p><h3 id="通用数据压缩算法？"><a href="#通用数据压缩算法？" class="headerlink" title="通用数据压缩算法？"></a>通用数据压缩算法？</h3><p>不存在的，因为假如真的存在一种可以压缩所有比特串的算法，那么该算法就可以继续压缩已经被它压缩过的数据，那意味着所有比特串可以被压缩为0比特.</p><h2 id="Run-length-encoding"><a href="#Run-length-encoding" class="headerlink" title="Run-length encoding"></a>Run-length encoding</h2><p>Simple type of redundancy in a bitstream. Long runs of repeated bits：<br><code>0000000000000001111111000000011111111111</code><br>Compression, 4-bit counts to represent alternating runs of 0s and 1s: 15 0s, then 7 1s, then 7 0s, then 11 1s.<br><code>1111 0111 0111 1011</code><br><!-- more --><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RunLength</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="comment">// maximum run-length count</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> R    = <span class="number">256</span>;</span><br><span class="line">   <span class="comment">// number of bits per count</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> LG_R = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Reads a sequence of bits from standard input; compresses</span></span><br><span class="line"><span class="comment">     * them using run-length coding with 8-bit run lengths; and writes the</span></span><br><span class="line"><span class="comment">     * results to standard output.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">()</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">        <span class="keyword">char</span> run = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> old = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (!BinaryStdIn.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">boolean</span> b = BinaryStdIn.readBoolean();</span><br><span class="line">            <span class="keyword">if</span> (b != old) &#123;</span><br><span class="line">                BinaryStdOut.write(run, LG_R);</span><br><span class="line">                run = <span class="number">1</span>;</span><br><span class="line">                old = !old;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123; <span class="comment">// 如果长度超过最大值, 写入0</span></span><br><span class="line">                <span class="keyword">if</span> (run == R-<span class="number">1</span>) &#123;</span><br><span class="line">                    BinaryStdOut.write(run, LG_R);</span><br><span class="line">                    run = <span class="number">0</span>;</span><br><span class="line">                    BinaryStdOut.write(run, LG_R);</span><br><span class="line">                &#125;</span><br><span class="line">                run++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        BinaryStdOut.write(run, LG_R);</span><br><span class="line">        BinaryStdOut.close();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Reads a sequence of bits from standard input (that are encoded</span></span><br><span class="line"><span class="comment">     * using run-length encoding with 8-bit run lengths); decodes them;</span></span><br><span class="line"><span class="comment">     * and writes the results to standard output.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">expand</span><span class="params">()</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">      <span class="keyword">boolean</span> bit = <span class="keyword">false</span>;</span><br><span class="line">      <span class="keyword">while</span> (!BinaryStdIn.isEmpty())</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="keyword">int</span> run = BinaryStdIn.readInt(lgR);</span><br><span class="line">         <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; run; i++)</span><br><span class="line">            BinaryStdOut.write(bit);</span><br><span class="line">         bit = !bit;</span><br><span class="line">      &#125;</span><br><span class="line">      BinaryStdOut.close();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;数据压缩&quot;&gt;&lt;a href=&quot;#数据压缩&quot; class=&quot;headerlink&quot; title=&quot;数据压缩&quot;&gt;&lt;/a&gt;数据压缩&lt;/h2&gt;&lt;p&gt;压缩数据以节省储存空间，节省传输时间。同时很多文件都有很多冗余信息，这为压缩提供了很多可能性。&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Information Retrieval" scheme="http://shukebeta.me/tags/Information-Retrieval/"/>
    
      <category term="Data Compression" scheme="http://shukebeta.me/tags/Data-Compression/"/>
    
  </entry>
  
  <entry>
    <title>众数问题 - Boyer–Moore majority vote algorithm</title>
    <link href="http://shukebeta.me/algorithms-majority-element/"/>
    <id>http://shukebeta.me/algorithms-majority-element/</id>
    <published>2018-10-02T16:00:00.000Z</published>
    <updated>2018-10-17T09:15:10.517Z</updated>
    
    <content type="html"><![CDATA[<p>数组中有一个数字出现的次数超过数组长度的一半，例如输入一个长度为9的数组<code>1,2,3,2,2,2,5,4,2</code>。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。因为这个数出现次数超过了数组长度一半以上, 那么它就是数组中出现次数最多的数, 故谓之<strong>众数</strong>.<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">MoreThanHalfNum_Solution</span><span class="params">(self, numbers)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        most = numbers[<span class="number">0</span>]</span><br><span class="line">        count = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> numbers:</span><br><span class="line">            <span class="keyword">if</span> item == most:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> count &lt; <span class="number">0</span>:</span><br><span class="line">                    most = item</span><br><span class="line">                    count = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> numbers.count(most) &lt;= len(numbers) / <span class="number">2</span> <span class="keyword">else</span> most</span><br></pre></td></tr></table></figure></p><h2 id="众数问题"><a href="#众数问题" class="headerlink" title="众数问题"></a>众数问题</h2><p>众数问题可以推广泛化：给定大小为<code>n</code>的整数数组，找到所有出现超过<code>n / m</code>次的元素。这种问题可以使用 Boyer-Moore 算法解决.</p><blockquote><p>The Boyer–Moore majority vote algorithm is an algorithm for finding the majority of a sequence of elements using linear time and constant space. It is named after Robert S. Boyer and J Strother Moore, who published it in 1981, and is a prototypical example of a streaming algorithm.</p></blockquote><p>如果存在众数元素，该算法会找到众数元素：对于出现次数一半以上的元素。但是，如果没有众数，算法将不会检测到该事实，并且仍将输出其中一个元素。</p><p>这个时候需要第二次遍历数据, 验证在第一次通过中找到的元素是否真正占众数。</p><p>比如找到所有出现超过<code>n / 3</code>次的元素, 最多只可能有2个, 可以用长度为2的数据结构(这里选择map)来记录众数.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">majorityElement</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        m = <span class="number">2</span></span><br><span class="line">        cand = [<span class="number">0</span>] * m</span><br><span class="line">        freq = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> len(freq) &lt; m:</span><br><span class="line">                freq[item] = <span class="number">1</span> + freq.get(item, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> item <span class="keyword">in</span> freq:</span><br><span class="line">                freq[item] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> list(freq):</span><br><span class="line">                    freq[k] -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> freq[k] &lt;= <span class="number">0</span>:</span><br><span class="line">                        freq.pop(k)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [k <span class="keyword">for</span> k <span class="keyword">in</span> freq <span class="keyword">if</span> nums.count(k) &gt; len(nums) // (m + <span class="number">1</span>)]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数组中有一个数字出现的次数超过数组长度的一半，例如输入一个长度为9的数组&lt;code&gt;1,2,3,2,2,2,5,4,2&lt;/code&gt;。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。因为这个数出现次数超过了数组长度一半以上, 那么它就是数组中出现次数最多的数, 故谓之&lt;strong&gt;众数&lt;/strong&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="CS" scheme="http://shukebeta.me/categories/CS/"/>
    
    
      <category term="Java" scheme="http://shukebeta.me/tags/Java/"/>
    
      <category term="Algorithms" scheme="http://shukebeta.me/tags/Algorithms/"/>
    
      <category term="Dynamic Programming" scheme="http://shukebeta.me/tags/Dynamic-Programming/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 06 - 不同树结构的字符串符号表</title>
    <link href="http://shukebeta.me/NLP-06-string-symbol-table-other-Trie/"/>
    <id>http://shukebeta.me/NLP-06-string-symbol-table-other-Trie/</id>
    <published>2018-09-30T16:00:00.000Z</published>
    <updated>2018-10-09T15:20:32.671Z</updated>
    
    <content type="html"><![CDATA[<h2 id="各种树的变种"><a href="#各种树的变种" class="headerlink" title="各种树的变种"></a>各种树的变种</h2><p>为了适应不同的应用场景, 人们使用不同的树结构来实现符号表.</p><h3 id="九宫格输入法"><a href="#九宫格输入法" class="headerlink" title="九宫格输入法"></a>九宫格输入法</h3><p>对于手机的九宫格输入法, 简单的实现方式是多次敲击: 通过反复按键输入一个字母，直到出现所需的字母。<br><a id="more"></a><br>但 <a href="http://www.t9.com/" target="_blank" rel="noopener">http://www.t9.com/</a> 的 T9 texting 支持更高效的输入方法:<br>・Find all words that correspond to given sequence of numbers.<br>・Press 0 to see all completion options.<br><img src="/images/t9.png" alt=""><br>Ex. hello<br>・多次敲击: 4 4 3 3 5 5 5 5 5 5 6 6 6<br>・T9: 4 3 5 5 6</p><p>可以使用 8-way trie 来实现.</p><h3 id="三元搜索Trie"><a href="#三元搜索Trie" class="headerlink" title="三元搜索Trie"></a>三元搜索Trie</h3><p><code>R</code>较大的R-way trie的空间效率不高，读取比较大的文件往往导致内存不足。但弊端是开辟出的数组内存利用率其实不高。现在很多系统都使用Unicode，分支可高达<code>65,536</code>. 所以需要更高效的方法。</p><p>Ternary search tries:<br>・Store characters and values in nodes (not keys).<br>・Each node has 3 children: smaller (left), equal (middle), larger (right).<br><img src="/images/tst.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br>Search in a TST: Follow links corresponding to each character in the key.<br>・If less, take left link; if greater, take right link.<br>・If equal, take the middle link and move to the next key character.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TST</span>&lt;<span class="title">Value</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Node root;</span><br><span class="line">    <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">    </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> Value val;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">char</span> c;</span><br><span class="line">        <span class="keyword">private</span> Node left, mid, right;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(String key, Value val)</span></span></span><br><span class="line"><span class="function">    </span>&#123; root = put(root, key, val, <span class="number">0</span>); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">put</span><span class="params">(Node x, String key, Value val, <span class="keyword">int</span> d)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">char</span> c = key.charAt(d);</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) &#123; x = <span class="keyword">new</span> Node(); x.c = c; &#125;</span><br><span class="line">        <span class="keyword">if</span> (c &lt; x.c) x.left = put(x.left, key, val, d);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (c &gt; x.c) x.right = put(x.right, key, val, d);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (d &lt; key.length() - <span class="number">1</span>) x.mid = put(x.mid, key, val, d+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">else</span> x.val = val;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(String key)</span></span></span><br><span class="line"><span class="function">    </span>&#123; <span class="keyword">return</span> get(key) != <span class="keyword">null</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Value <span class="title">get</span><span class="params">(String key)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        Node x = get(root, key, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">return</span> x.val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">get</span><span class="params">(Node x, String key, <span class="keyword">int</span> d)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">         <span class="keyword">char</span> c = key.charAt(d);</span><br><span class="line">         <span class="keyword">if</span> (c &lt; x.c) <span class="keyword">return</span> get(x.left, key, d);</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">if</span> (c &gt; x.c) <span class="keyword">return</span> get(x.right, key, d);</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">if</span> (d &lt; key.length() - <span class="number">1</span>) <span class="keyword">return</span> get(x.mid, key, d+<span class="number">1</span>);</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>TSTs比hashing更快（特别是对于搜索缺失键的情况）。</p><h3 id="基数树"><a href="#基数树" class="headerlink" title="基数树"></a>基数树</h3><p>Radix Tree, 也叫 Patricia trie (Practical Algorithm to Retrieve Information Coded in Alphanumeric), crit-bit tree, 压缩前缀树:<br>・Remove one-way branching.<br>・Each node represents a sequence of characters.<br>・Implementation: one step beyond this course.<br><img src="/images/radix_trie.png" alt="" title="put(&quot;shells&quot;, 1); put(&quot;shellfish&quot;, 2); image from: https://algs4.cs.princeton.edu/"><br>对于基数树的每个节点，如果该节点是唯一的子树的话，就和父节点合并。</p><p>Applications.<br>・Database search.<br>・P2P network search.<br>・IP routing tables: find longest prefix match.<br>・Compressed quad-tree for N-body simulation.<br>・Efficiently storing and querying XML documents.</p><h3 id="后缀树"><a href="#后缀树" class="headerlink" title="后缀树"></a>后缀树</h3><p>后缀树（Suffix tree）指字符串后缀的基数树: 一个<code>String S</code>的后缀树是一个边（edge）被标记为字符串的树。因此每一个<code>S</code>的后缀都唯一对应一条从根节点到叶节点的路径。这样就形成了一个<code>S</code>的后缀的基数树。<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Suffix_tree_BANANA.svg/250px-Suffix_tree_BANANA.svg.png" alt="" title="image from: https://en.wikipedia.org/"></p><p>Applications.<br>・Linear-time: longest repeated substring, longest common substring, longest palindromic substring, substring search, tandem repeats, ….<br>・Computational biology databases (BLAST, FASTA).</p><h2 id="字符符号表总结"><a href="#字符符号表总结" class="headerlink" title="字符符号表总结"></a>字符符号表总结</h2><p>Red-black BST.<br>・Performance guarantee: log N key compares.<br>・Supports ordered symbol table API.</p><p>Hash tables.<br>・Performance guarantee: constant number of probes.<br>・Requires good hash function for key type.</p><p>Tries. R-way, TST.<br>・Performance guarantee: log N characters accessed.<br>・Supports character-based operations.<br><img src="/images/string_symbol_table_cost_sum.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"></p><blockquote><p>You can get at anything by examining 50-100 bits</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;各种树的变种&quot;&gt;&lt;a href=&quot;#各种树的变种&quot; class=&quot;headerlink&quot; title=&quot;各种树的变种&quot;&gt;&lt;/a&gt;各种树的变种&lt;/h2&gt;&lt;p&gt;为了适应不同的应用场景, 人们使用不同的树结构来实现符号表.&lt;/p&gt;
&lt;h3 id=&quot;九宫格输入法&quot;&gt;&lt;a href=&quot;#九宫格输入法&quot; class=&quot;headerlink&quot; title=&quot;九宫格输入法&quot;&gt;&lt;/a&gt;九宫格输入法&lt;/h3&gt;&lt;p&gt;对于手机的九宫格输入法, 简单的实现方式是多次敲击: 通过反复按键输入一个字母，直到出现所需的字母。&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Trie" scheme="http://shukebeta.me/tags/Trie/"/>
    
      <category term="Information Retrieval" scheme="http://shukebeta.me/tags/Information-Retrieval/"/>
    
      <category term="Symbol table" scheme="http://shukebeta.me/tags/Symbol-table/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 05 - 字符串符号表和三元搜索Trie</title>
    <link href="http://shukebeta.me/NLP-05-string-symbol-table-Trie/"/>
    <id>http://shukebeta.me/NLP-05-string-symbol-table-Trie/</id>
    <published>2018-09-29T16:00:00.000Z</published>
    <updated>2018-09-30T09:02:12.262Z</updated>
    
    <content type="html"><![CDATA[<h2 id="符号表"><a href="#符号表" class="headerlink" title="符号表"></a>符号表</h2><blockquote><p>在计算机科学中，符号表是一种用于语言翻译器（例如编译器和解释器）中的数据结构。在符号表中，程序源代码中的每个标识符都和它的声明或使用信息绑定在一起，比如其数据类型、作用域以及内存地址。<br>常用哈希表来实现.</p></blockquote><p>符号表的应用非常广泛, 可用于实现Set, Dictionary, 文件索引, 稀疏向量/矩阵等数据结构和相关的运算操作, 还有其他如过滤查询(Exception filter), 一致性查询(concordance queries)等操作.</p><p>字符符号表就是专门针对字符操作的符号表, API:<br>Prefix match - Keys with prefix <code>sh</code>: <code>she</code>, <code>shells</code>, and <code>shore</code>.<br>Wildcard match - Keys that match <code>.he</code>: <code>she</code> and <code>the</code>.<br>Longest prefix - Key that is the longest prefix of <code>shellsort</code>: <code>shells</code>.<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">StringST</span>&lt;<span class="title">Value</span>&gt; </span>&#123;</span><br><span class="line">    StringST(); create a symbol table with string keys</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(String key, Value val)</span></span>; put key-value pair into the symbol table</span><br><span class="line">    <span class="function">Value <span class="title">get</span><span class="params">(String key)</span></span>; value paired with key</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">delete</span><span class="params">(String key)</span></span>; delete key and corresponding value</span><br><span class="line">    <span class="function">Iterable&lt;String&gt; <span class="title">keys</span><span class="params">()</span></span>; all keys</span><br><span class="line">    <span class="function">Iterable&lt;String&gt; <span class="title">keysWithPrefix</span><span class="params">(String s)</span></span>; keys having s as a prefix</span><br><span class="line">    <span class="function">Iterable&lt;String&gt; <span class="title">keysThatMatch</span><span class="params">(String s)</span></span>; <span class="function">keys that match <span class="title">s</span> <span class="params">(where . is a wildcard)</span></span></span><br><span class="line"><span class="function">    String <span class="title">longestPrefixOf</span><span class="params">(String s)</span></span>; longest key that is a prefix of s</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="以Trie为基础的字符符号表"><a href="#以Trie为基础的字符符号表" class="headerlink" title="以Trie为基础的字符符号表"></a>以Trie为基础的字符符号表</h3><p>algs4中提供了用 R-way trie 来实现符号表(symbol table)例子:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TrieST</span>&lt;<span class="title">Value</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> R = <span class="number">256</span>; <span class="comment">// extended ASCII</span></span><br><span class="line">    <span class="keyword">private</span> Node root = <span class="keyword">new</span> Node();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> Object value;</span><br><span class="line">        <span class="keyword">private</span> Node[] next = <span class="keyword">new</span> Node[R];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(String key, Value val)</span> </span>&#123;</span><br><span class="line">        root = put(root, key, val, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">put</span><span class="params">(Node x, String key, Value val, <span class="keyword">int</span> d)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) x = <span class="keyword">new</span> Node();</span><br><span class="line">        <span class="keyword">if</span> (d == key.length()) &#123; x.value = val; <span class="keyword">return</span> x; &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">char</span> c = key.charAt(d);</span><br><span class="line">        x.next[c] = put(x.next[c], key, val, d+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(String key)</span> </span>&#123; <span class="keyword">return</span> get(key) != <span class="keyword">null</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Value <span class="title">get</span><span class="params">(String key)</span> </span>&#123;</span><br><span class="line">        Node x = get(root, key, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">return</span> (Value) x.val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">get</span><span class="params">(Node x, String key, <span class="keyword">int</span> d)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (d == key.length()) <span class="keyword">return</span> x;</span><br><span class="line">        <span class="keyword">char</span> c = key.charAt(d);</span><br><span class="line">        <span class="keyword">return</span> get(x.next[c], key, d+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>按顺序迭代所有键：<br>·中序遍历trie，找到的键添加到队列中<br>·维护从根到当前节点路径的字符序列<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">keys</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Queue&lt;String&gt; queue = <span class="keyword">new</span> Queue&lt;String&gt;();</span><br><span class="line">    collect(root, <span class="string">""</span>, queue);</span><br><span class="line">    <span class="keyword">return</span> queue;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">collect</span><span class="params">(Node x, String prefix, Queue&lt;String&gt; q)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">if</span> (x.val != <span class="keyword">null</span>) q.enqueue(prefix);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">char</span> c = <span class="number">0</span>; c &lt; R; c++)</span><br><span class="line">        collect(x.next[c], prefix + c, q);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="前缀匹配"><a href="#前缀匹配" class="headerlink" title="前缀匹配"></a>前缀匹配</h3><p>Find all keys in a symbol table starting with a given prefix.<br>Ex. Autocomplete in a cell phone, search bar, text editor, or shell.<br>・User types characters one at a time.<br>・System reports all matching strings.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">keysWithPrefix</span><span class="params">(String prefix)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Queue&lt;String&gt; queue = <span class="keyword">new</span> Queue&lt;String&gt;();</span><br><span class="line">    Node x = get(root, prefix, <span class="number">0</span>);</span><br><span class="line">    collect(x, prefix, queue);</span><br><span class="line">    <span class="keyword">return</span> queue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="最长前缀"><a href="#最长前缀" class="headerlink" title="最长前缀"></a>最长前缀</h3><p>Find longest key in symbol table that is a prefix of query string.<br>Ex. To send packet toward destination IP address, router chooses IP address in routing table that is longest prefix match.</p><p>・Search for query string.<br>・Keep track of longest key encountered.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">longestPrefixOf</span><span class="params">(String query)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> length = search(root, query, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> query.substring(<span class="number">0</span>, length);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(Node x, String query, <span class="keyword">int</span> d, <span class="keyword">int</span> length)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span> length;</span><br><span class="line">    <span class="keyword">if</span> (x.val != <span class="keyword">null</span>) length = d;</span><br><span class="line">    <span class="keyword">if</span> (d == query.length()) <span class="keyword">return</span> length;</span><br><span class="line">    <span class="keyword">char</span> c = query.charAt(d);</span><br><span class="line">    <span class="keyword">return</span> search(x.next[c], query, d+<span class="number">1</span>, length);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;符号表&quot;&gt;&lt;a href=&quot;#符号表&quot; class=&quot;headerlink&quot; title=&quot;符号表&quot;&gt;&lt;/a&gt;符号表&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在计算机科学中，符号表是一种用于语言翻译器（例如编译器和解释器）中的数据结构。在符号表中，程序源代码中的每个标识符都和它的声明或使用信息绑定在一起，比如其数据类型、作用域以及内存地址。&lt;br&gt;常用哈希表来实现.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;符号表的应用非常广泛, 可用于实现Set, Dictionary, 文件索引, 稀疏向量/矩阵等数据结构和相关的运算操作, 还有其他如过滤查询(Exception filter), 一致性查询(concordance queries)等操作.&lt;/p&gt;
&lt;p&gt;字符符号表就是专门针对字符操作的符号表, API:&lt;br&gt;Prefix match - Keys with prefix &lt;code&gt;sh&lt;/code&gt;: &lt;code&gt;she&lt;/code&gt;, &lt;code&gt;shells&lt;/code&gt;, and &lt;code&gt;shore&lt;/code&gt;.&lt;br&gt;Wildcard match - Keys that match &lt;code&gt;.he&lt;/code&gt;: &lt;code&gt;she&lt;/code&gt; and &lt;code&gt;the&lt;/code&gt;.&lt;br&gt;Longest prefix - Key that is the longest prefix of &lt;code&gt;shellsort&lt;/code&gt;: &lt;code&gt;shells&lt;/code&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Trie" scheme="http://shukebeta.me/tags/Trie/"/>
    
      <category term="Information Retrieval" scheme="http://shukebeta.me/tags/Information-Retrieval/"/>
    
      <category term="Symbol table" scheme="http://shukebeta.me/tags/Symbol-table/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 04 - “和谐” - 多模式匹配算法 - AC自动机</title>
    <link href="http://shukebeta.me/NLP-04-string-match-algorithm-ac-automaton/"/>
    <id>http://shukebeta.me/NLP-04-string-match-algorithm-ac-automaton/</id>
    <published>2018-09-28T16:00:00.000Z</published>
    <updated>2019-07-30T09:36:47.023Z</updated>
    
    <content type="html"><![CDATA[<p>虽然KMP可以用于<a href="/NLP-01-string-searching-algorithm-01-kmp">单模式匹配问题</a>，但如果是多模式问题, KMP的性能就得不到保证。比如根据墙内法律要求, 墙内的搜索引擎需要过滤敏感词后才能合法运营。敏感词的数量不少, 如果要求包含敏感词的网页不能被搜索到, 那么搜索引擎在爬取网页信息时, 就要标记网页的文本中是否包含任意个敏感词.<br><a id="more"></a></p><p>这就是典型的多模匹配问题. 这种情况下如果使用Trie，那么需要遍历网页的每一个字符位置，对每一个位置进行Trie前缀匹配。如果词典的词语数量为N，每个词语长度为L，文章的长度为M，那么需要进行的计算次数是在<code>N*M*L</code>这个级别的. 即使把词语的长度L简化为常数级别的, 整个算法的复杂度也至少是$O(n^2)$.</p><h2 id="AC自动机"><a href="#AC自动机" class="headerlink" title="AC自动机"></a>AC自动机</h2><p>可以看到，KMP算法可以避免back up（在检查字符的过程中不需要回头），而Trie可以存储多个模式的信息。如果把二者结合在一起，也许能从性能上解决多模式（任意位置）匹配问题。这就是Aho–Corasick算法（AC自动机）。</p><blockquote><p>Aho–Corasick算法是由Alfred V. Aho和Margaret J.Corasick 发明的字符串搜索算法，用于在输入的一串字符串中匹配有限组字典中的子串。它与普通字符串匹配的不同点在于同时与所有字典串进行匹配。算法均摊情况下具有近似于线性的时间复杂度，约为字符串的长度加所有匹配的数量。</p></blockquote><p>所以算法的关键就是通过Trie把多个模式构建为一个DFA（Deterministic finite state automaton），然后让模式串末尾对应的状态作为一个DFA的终止节点。这样，对于一个要检查的长字符串（如一段网页内容），让这个字符串在DFA上跑一趟，每一个字符表示一种跳转方式，如果这段字符能够跳到任何一个终结节点, 那么就表明这段字符串匹配了至少一个模式, 如果整段字符跑完都没到达终结节点, 那么这个网页就是”和谐的”.</p><p>在单模式匹配中, 用KMP构建的DFA是比较简单的, 从左到右, 开头的状态就是开始状态, 结尾的状态就是结束状态:<br><img src="/images/build_dfa.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br>而多模式匹配中, 在Trie的结构基础上构建出来的DFA更像一个DFA的样子:<br><img src="/images/ushers_dfa.png" alt="" title="经典的ushers自动机，模式串是he/ she/ his /hers, 忽略了部分到根节点的转移边. image from: https://baike.baidu.com/pic"><br>Trie中的节点, 就类似于DFA中的状态. 如果让字符串<code>shis</code>在上面跑, 假如仅仅是靠Trie(也即是没有虚线标识的转移), 那么第一次从字符串的第一个字符<code>s</code>开始转移, 经过转移路径<code>0 - 85 - 90</code>之后就转不动了, 因为Trie记录的模式中没有<code>shi</code>, 这个时候得back up, 从第二个位置<code>h</code>开始再匹配一遍. 这个过程中就产生重复匹配, 而参考KMP的思路, 在匹配<code>shi</code>的过程中, 其实已经挖掘出了<code>hi</code>这个子串了, 而这个子串是跟模式<code>his</code>对应的, 如果有办法不回头继续匹配下去就能提高性能了.</p><p>而DFA中虚线的失败转移就是用来解决这个问题的: 当走到状态<code>90</code>时, 前面有了小部分子串<code>h</code>刚好对应状态<code>74</code>, 这个时候用虚线作为失败转移, 转移到<code>74</code>, 在状态<code>74</code>中寻找下一个转移<code>i</code>, 这样就实现了不回头继续匹配了.</p><p>因为AC自动机是在Trie的基础上添加边, 用于指示各个节点经过不同字符后跳转到哪个节点, 结果就变成了图, 所以也叫做<strong>Trie图</strong>.</p><p>要构建AC自动机:</p><ol><li>首先要把所有模式都吃进一个Trie中(最近看多进击的巨人了), 构建出一个由不同实线串联起来的状态机, 其中代表刚好吻合一个模式的状态标记为终结节点(如上图绿色节点)</li><li>然后补全其他字符的转移(失败转移), 用虚线表示. 补全了所有字符的转移方式, 才能让字符串永不回头地匹配下去, 避免了back up, 保证性能.</li></ol><p>问题的关键在如何补全所有的状态转移.</p><h3 id="补全状态转移"><a href="#补全状态转移" class="headerlink" title="补全状态转移"></a>补全状态转移</h3><p><img src="/images/ushers_dfa.png" alt="" title="经典的ushers自动机，模式串是he/ she/ his /hers, 忽略了部分到根节点的转移边. image from: https://baike.baidu.com/pic"><br>这里要在Trie结构中定义一个<a href="https://hihocoder.com/problemset/problem/1036" target="_blank" rel="noopener">后缀节点</a>的概念: Trie中对应路径(已有模式)去掉部分前缀字符后剩余的后缀字符在Trie中对应的结点. 比如上图中, <code>h</code>作为<code>sh</code>的一个后缀, <code>h</code>对应的Trie节点<code>74</code>就是<code>sh</code>对应节点<code>90</code>的后缀节点. 等于说, <strong>节点和其后缀节点对应的模式有一部分后缀是相同</strong>.</p><p>如果知道了每一个节点的后缀节点, 那么在匹配的过程中, 在任一位置匹配失败, 都可以通过失败转移的方式转移到后缀节点, 继续进行后续匹配, 而不会遗漏, 因为后缀节点对应这个目前为止已匹配字符的某一部分后缀. 等于说, 后缀节点告诉我们, 在字符串中出现与模式不同的字符串时(匹配失败), 如何转移到其他状态.</p><p>所以问题的关键又变成了如何求后缀节点.</p><h3 id="求后缀节点"><a href="#求后缀节点" class="headerlink" title="求后缀节点"></a>求后缀节点</h3><p>观察Trie结构可以发现两个要点</p><ol><li>字符串任何一个位置对应的状态节点，一定比它的后缀节点更深，比如前面例子中状态节点<code>90</code>在第二层, 而其后缀节点<code>74</code>在第一层. 这点也是理所当然的, 毕竟后缀比较短. 从动态规划的角度考虑, 字符串任一位置<code>i</code>对应的状态节点的后缀节点一定是<code>k&lt;i</code>的节点中的某一个.</li><li>因为每一个状态<code>i</code>都是由其父节点<code>j</code>通过某一个字符<code>c</code>转移而来, 那么<code>i</code>的后缀节点一定是<code>j</code>的后缀节点通过同样的字符<code>c</code>转移而来. 或者说, 如果<code>j</code>的后缀节点是<code>jj</code>, 那么<code>j</code>和<code>jj</code>有着相同的后缀, 它们通过同样的转移字符<code>c</code>转移后, 二者到达的节点也一定有着相同的后缀.</li></ol><p>比如上面Ushers自动机例子中, 如果用字符串<code>sshis</code>来跑, 那么<code>ssh</code>对应的状态<code>90</code>, 是由前缀<code>ss</code>通过字符<code>h</code>转移而来. 因为<code>ssh</code>的后缀节点, 同样是某一个有共同后缀的字符(<code>h</code>或者<code>sh</code>)对应的状态(在这里是<code>h</code>对应的<code>74</code>). 可以发现<code>74</code>是由根节点<code>0</code>通过同样的字符<code>h</code>转移而来的. 反过来说, 节点<code>0</code>就是节点<code>90</code>的父节点<code>85</code>的后缀节点.</p><p>在多个模式中, 如果有某模式的前缀刚好是另一模式的子串(后缀). 比如上面Ushers自动机例子中, 模式<code>her</code>(或者<code>he</code>)的前缀<code>he</code>就是模式<code>she</code>的子串, 则会二者存在失败转移的关联. 如果没有, 那么就跳回初始状态节点.</p><p>所以补全所有状态转移的具体实现方法就是运用动态规划的原理:</p><ul><li>从Trie根节点开始, 逐层往下补全每一层的状态转移, 也就是宽度优先遍历(BFS), 这样下层的状态转移就可以利用上层的结果. 动态规划的转移方程可以描述为: <strong>每一个通过字符<code>c</code>转移而来的状态节点<code>i</code>的后缀节点 = <code>i</code>的父节点的后缀节点通过<code>c</code>转移到的状态节点</strong></li><li>初始状态包含两部分:<ul><li>一个是根节点(初始状态<code>0</code>), 它的后缀节点就是它自己,</li><li>另一个是第一层的状态节点, 如<code>85, 74</code>, 因为它们对应的是长度为<code>1</code>的字符, 没有后缀, 所以它们的后缀节点也是根节点<code>0</code>.</li></ul></li></ul><p>在实现中还要注意, <strong>后缀结点为标记结点的结点也需要被标记</strong>. 因为在状态转移过程中, 如果某个虚线转移刚好转移到终结节点, 但在字符串遍历的过程中, 并没有选择走这一条线, 就会忽略了这个终结节点, 导致匹配失败, 或者多走了更多的路. 比如在上面的例子中, 如果把模式<code>she</code>改为<code>shee</code>, <code>91</code>不再是终结节点, 而是延伸到<code>92</code>为终结节点, <code>91</code>的后缀节点是<code>76</code>. 如果用字符串<code>sshe</code>来跑这个DFA, 就会出现走到最后字符<code>e</code>时, 在节点<code>91</code>结束, 匹配失败. 所以需要把<code>91</code>也标记为终结节点.</p><h3 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 把字典通过insert把所有单词插入Trie树，</span></span><br><span class="line"><span class="comment"> * 然后通过setSuffix()构建出对应的Trie图，</span></span><br><span class="line"><span class="comment"> * 然后从Trie图的根节点开始，沿着文章str的每一个字符，走出对应的边，</span></span><br><span class="line"><span class="comment"> * 直到遇到一个标记结点或者整个str都遍历完成</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Trie</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> TrieNode trie;</span><br><span class="line">    Queue&lt;TrieNode&gt; queue;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Trie</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        trie = <span class="keyword">new</span> TrieNode(<span class="keyword">null</span>, <span class="string">' '</span>);</span><br><span class="line">        queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        TrieNode curNode = trie;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">char</span> x : word.toCharArray()) &#123;</span><br><span class="line">            curNode = insert(curNode, x);</span><br><span class="line">        &#125;</span><br><span class="line">        curNode.setLast(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** insert char x, means create a new node in the x edge.</span></span><br><span class="line"><span class="comment">     * return created node  */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> TrieNode <span class="title">insert</span><span class="params">(TrieNode node, <span class="keyword">char</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node.get(x) == <span class="keyword">null</span>) &#123;</span><br><span class="line">            node.set(x);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> node.get(x);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** BFS on the trie */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSuffix</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        queue.add(trie);</span><br><span class="line">        <span class="keyword">while</span> (!queue.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">/** poll() removes the present head.</span></span><br><span class="line"><span class="comment">             http://www.tutorialspoint.com/java/util/linkedlist_poll.htm */</span></span><br><span class="line">            TrieNode node = queue.poll();</span><br><span class="line">            setSuffix(node);</span><br><span class="line">            complementDFA(node);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Set node's suffix, complement lacking edge</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> TrieNode <span class="title">setSuffix</span><span class="params">(TrieNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node.root == <span class="keyword">null</span>) &#123; <span class="comment">// Trie root</span></span><br><span class="line">            node.suffix = node;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (node.root.root == <span class="keyword">null</span>) &#123;</span><br><span class="line">            node.suffix = node.root.suffix;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            node.suffix = node.root.suffix.get(node.fromIndex);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (node.suffix.isLast) &#123;</span><br><span class="line">            node.isLast = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> node.suffix;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Complement DFA according to suffix */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">complementDFA</span><span class="params">(TrieNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node.isLast) &#123; <span class="keyword">return</span>; &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; node.edges.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (node.edges[i] == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (node.root == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    node.edges[i] = node;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    node.edges[i] = node.suffix.edges[i];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                queue.add(node.edges[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">search</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> contains = <span class="keyword">false</span>;</span><br><span class="line">        TrieNode curNode = trie;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class="line">            <span class="keyword">char</span> x = s.charAt(i);</span><br><span class="line">            curNode = curNode.get(x);</span><br><span class="line">            <span class="keyword">if</span> (curNode.isLast) &#123;</span><br><span class="line">                contains = <span class="keyword">true</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> contains;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TrieNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> R = <span class="number">26</span>;</span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> ATO0 = <span class="number">97</span>;</span><br><span class="line">        <span class="keyword">boolean</span> isLast;</span><br><span class="line">        TrieNode[] edges;</span><br><span class="line">        TrieNode root;</span><br><span class="line">        <span class="keyword">char</span> fromIndex;</span><br><span class="line">        TrieNode suffix;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">TrieNode</span><span class="params">(TrieNode root, <span class="keyword">char</span> from)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.root = root;</span><br><span class="line">            fromIndex = from;</span><br><span class="line">            edges = <span class="keyword">new</span> TrieNode[R];</span><br><span class="line">            isLast = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> TrieNode <span class="title">get</span><span class="params">(<span class="keyword">char</span> ch)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> edges[ch - ATO0];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** instantiate the ch child in edges */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">char</span> ch)</span> </span>&#123;</span><br><span class="line">            edges[ch - ATO0] = <span class="keyword">new</span> TrieNode(<span class="keyword">this</span>, ch);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setLast</span><span class="params">(<span class="keyword">boolean</span> isLast)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isLast = isLast;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner in = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        Trie t = <span class="keyword">new</span> Trie();</span><br><span class="line">        String[] X = &#123;<span class="string">"sb"</span>, <span class="string">"dsb"</span>, <span class="string">"cjdsb"</span>, <span class="string">"qnmlgb"</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span> (String x : X) &#123;</span><br><span class="line">            t.insert(x);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        t.setSuffix();</span><br><span class="line">        String s = <span class="string">"aadbaaadaaac"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (t.search(s)) &#123;</span><br><span class="line">            System.out.println(<span class="string">"YES"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"NO"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然KMP可以用于&lt;a href=&quot;/NLP-01-string-searching-algorithm-01-kmp&quot;&gt;单模式匹配问题&lt;/a&gt;，但如果是多模式问题, KMP的性能就得不到保证。比如根据墙内法律要求, 墙内的搜索引擎需要过滤敏感词后才能合法运营。敏感词的数量不少, 如果要求包含敏感词的网页不能被搜索到, 那么搜索引擎在爬取网页信息时, 就要标记网页的文本中是否包含任意个敏感词.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Trie图" scheme="http://shukebeta.me/tags/Trie%E5%9B%BE/"/>
    
      <category term="AC自动机" scheme="http://shukebeta.me/tags/AC%E8%87%AA%E5%8A%A8%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 03 - 单模式匹配与拼写检查 - Trie</title>
    <link href="http://shukebeta.me/NLP-03-string-match-algorithm-Trie/"/>
    <id>http://shukebeta.me/NLP-03-string-match-algorithm-Trie/</id>
    <published>2018-09-27T16:00:00.000Z</published>
    <updated>2018-09-30T09:02:24.563Z</updated>
    
    <content type="html"><![CDATA[<p>Trie 也称字典树，名称来源于Re<font color="red">trie</font>val，支持$O(n)$插入和查询操作，以空间换取时间的数据结构. 用于词频统计和输入统计领域, 可以高效地存储大规模的字典数据, 也可以用于模糊匹配, 搜索最长前缀词等.</p><blockquote><p>A <strong>trie</strong>, also called <strong>digital tre</strong>e, <strong>radix tree</strong> or <strong>prefix tree</strong> is a kind of search tree - an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node.</p></blockquote><a id="more"></a><p><img src="/images/Trie_example.png" alt="" title="A trie for keys &quot;A&quot;,&quot;to&quot;, &quot;tea&quot;, &quot;ted&quot;, &quot;ten&quot;, &quot;i&quot;, &quot;in&quot;, and &quot;inn&quot;. Image from https://en.wikipedia.org/wiki/Trie"></p><h2 id="Trie"><a href="#Trie" class="headerlink" title="Trie"></a>Trie</h2><p>Trie没有规定每一个节点的分支数量, 用<strong>R-way Trie</strong>来表示分支数量为<code>R</code>的Trie. 对于不同的应用, 可以设置不同的<code>R</code>.</p><h3 id="字符（模糊）匹配与拼写检查"><a href="#字符（模糊）匹配与拼写检查" class="headerlink" title="字符（模糊）匹配与拼写检查"></a>字符（模糊）匹配与拼写检查</h3><p>应用例子是在一本字典中查找特定前缀的所有单词. 简化的例子是在英文字典中, 根据查询前缀, 返回相同前缀的所有单词数. 同样的结构可以用来检查拼写错误.</p><p>那么只需要在每一个节点存储该节点以下所有单词数就行了. 每一个节点包含一个长度26的数组，以方便快速定位对应的26个字母, 类似B-tree:<img src="/images/b_tree.png" alt="" title="image from https://www.coursera.org/learn/algorithms-part1"><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 字母本身可以用来作为索引：a - 97, z - 122</span></span><br><span class="line"><span class="comment"> * Color 是备用属性， 用于标记该节点是否为单词结尾，这里暂时用不到。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Trie</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> ALPHABET = <span class="number">26</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> ATO0 = <span class="number">97</span>;</span><br><span class="line">    <span class="keyword">int</span> color;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    Trie[] node;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Trie</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.n = n;</span><br><span class="line">        node = <span class="keyword">new</span> Trie[ALPHABET];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(String words)</span> </span>&#123;</span><br><span class="line">        Trie[] curNode = node;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">char</span> x : words.toCharArray()) &#123;</span><br><span class="line">            <span class="keyword">int</span> index = x - ATO0;</span><br><span class="line">            insert(curNode, index);</span><br><span class="line">            curNode = curNode[index].node;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(Trie[] curNode, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (curNode[index] == <span class="keyword">null</span>) &#123;</span><br><span class="line">            curNode[index] = <span class="keyword">new</span> Trie(<span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            curNode[index].n++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(String prefix)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        Trie[] curNode = node;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">char</span> x : prefix.toCharArray()) &#123;</span><br><span class="line">            <span class="keyword">int</span> index = x - ATO0;</span><br><span class="line">            <span class="keyword">if</span> (curNode[index] == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            count = curNode[index].n;</span><br><span class="line">            curNode = curNode[index].node;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>如果要问题扩展为返回所有相同前缀的单词，那么就要在插入字典时，在对应单词结尾的节点标记颜色。</p><h3 id="提高扩展性"><a href="#提高扩展性" class="headerlink" title="提高扩展性"></a>提高扩展性</h3><p>用固定长度为26的数组来处理英文，好处是数组内存占用小，索引时也不需要搜索，直接用字符码作为索引。也可以根据ASCII码进一步扩大数组长度以支持更多字符。</p><p>为了提高可扩展性，可以考虑用其他更灵活的数据结构来替代数组，比如HashMap，同时把HashMap放进一个TrieNode类。这样以后要修改核心的存储结构，只需要改动TrieNode即可，其余的接口不用改。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Trie</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> TrieNode node;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Trie</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.node = <span class="keyword">new</span> TrieNode();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        TrieNode curNode = node;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">char</span> x : word.toCharArray()) &#123;</span><br><span class="line">            curNode = curNode.set(x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(String prefix)</span> </span>&#123;</span><br><span class="line">        TrieNode curNode = node;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">char</span> x : prefix.toCharArray()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (curNode.get(x) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            curNode = curNode.get(x);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> curNode.count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TrieNode</span> </span>&#123;</span><br><span class="line">        HashMap&lt;Character, TrieNode&gt; map;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> count;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">char</span> value;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">TrieNode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            count = <span class="number">0</span>;</span><br><span class="line">            map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">TrieNode</span><span class="params">(Character val)</span> </span>&#123;</span><br><span class="line">            count = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">this</span>.value = val;</span><br><span class="line">            map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> TrieNode <span class="title">get</span><span class="params">(<span class="keyword">char</span> ch)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> map.get(ch);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> TrieNode <span class="title">set</span><span class="params">(<span class="keyword">char</span> ch)</span> </span>&#123;</span><br><span class="line">            TrieNode t = map.get(ch);</span><br><span class="line">            <span class="keyword">if</span> (t == <span class="keyword">null</span>) &#123;</span><br><span class="line">                t = <span class="keyword">new</span> TrieNode(ch);</span><br><span class="line">                <span class="keyword">this</span>.map.put(ch, t);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                t.count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> t;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">char</span> <span class="title">getValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.value;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>HashMap的寻址虽然会靠字符码作为地址的数组慢一点点，但也是非常快的:$O(\log N)$。但HashMap本身是比较耗内存的数据结构, 所以如果知道要处理的数据是在特定范围内的, 比如节点就是在256个字符中, 那么还是不要不用HashMap.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Trie 也称字典树，名称来源于Re&lt;font color=&quot;red&quot;&gt;trie&lt;/font&gt;val，支持$O(n)$插入和查询操作，以空间换取时间的数据结构. 用于词频统计和输入统计领域, 可以高效地存储大规模的字典数据, 也可以用于模糊匹配, 搜索最长前缀词等.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A &lt;strong&gt;trie&lt;/strong&gt;, also called &lt;strong&gt;digital tre&lt;/strong&gt;e, &lt;strong&gt;radix tree&lt;/strong&gt; or &lt;strong&gt;prefix tree&lt;/strong&gt; is a kind of search tree - an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Trie" scheme="http://shukebeta.me/tags/Trie/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 02 - 字符搜索匹配算法 02 - Boyer-Moore(BM) 和 Rabin-Karp(RK)</title>
    <link href="http://shukebeta.me/NLP-02-string-searching-algorithm-02-bm-rk/"/>
    <id>http://shukebeta.me/NLP-02-string-searching-algorithm-02-bm-rk/</id>
    <published>2018-09-26T16:00:00.000Z</published>
    <updated>2018-10-07T07:44:44.008Z</updated>
    
    <content type="html"><![CDATA[<p>字符串搜索/匹配算法中, Boyer-Moore(BM)比前面的<a href="/NLP-01-string-searching-algorithm-kmp">Knuth–Morris–Pratt(KMP)</a>更高效. BM算法从右到左扫描模式中的字符。当匹配的字符在模式中不存在时，可以跳过最多达M个字符.<img src="/images/bm.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br><a id="more"></a></p><h2 id="BM"><a href="#BM" class="headerlink" title="BM"></a>BM</h2><p>在决定要跳过多少字符时, 分几种情况考虑:<br>1, mismatch character <code>T</code> not in pattern: increment <code>i</code> one character beyond <code>T</code><br>2.1, Mismatch character in pattern: mismatch character <code>N</code> in pattern, align text <code>N</code> with rightmost pattern <code>N</code><br>2.2, Mismatch character in pattern (but heuristic no help): mismatch character <code>E</code> in pattern, align text <code>E</code> with rightmost pattern <code>E</code>? 还是 increment i by 1? 无法确定, 需要辅助信息.</p><p>需要预先计算模式各个字符在模式最右边出现的索引(若无则<code>-1</code>):<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">right = <span class="keyword">new</span> <span class="keyword">int</span>[R];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; R; c++)</span><br><span class="line">right[c] = -<span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">right[pat.charAt(j)] = j;</span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(String txt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> N = txt.length();</span><br><span class="line"><span class="keyword">int</span> M = pat.length();</span><br><span class="line"><span class="keyword">int</span> skip;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= N-M; i += skip)</span><br><span class="line">&#123;</span><br><span class="line">skip = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = M-<span class="number">1</span>; j &gt;= <span class="number">0</span>; j--)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (pat.charAt(j) != txt.charAt(i+j))</span><br><span class="line">&#123;</span><br><span class="line">skip = Math.max(<span class="number">1</span>, j - right[txt.charAt(i+j)]);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (skip == <span class="number">0</span>) <span class="keyword">return</span> i; <span class="comment">// match</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> N;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Substring search with the Boyer-Moore mismatched character heuristic takes about <code>~ N / M</code> character compares to search for a pattern of length <code>M</code> in a text of length <code>N</code>.<br>Worst-case. Can be as bad as <code>~ M N</code>.</p></blockquote><p>Boyer-Moore变种: 通过添加类似KMP的规则来防止重复模式，可以将最坏情况改善为<code>~3 N</code>字符比较。</p><h2 id="RK"><a href="#RK" class="headerlink" title="RK"></a>RK</h2><p>Rabin-Karp 基于 modular hashing：<br>・Compute a hash of pattern characters <code>0</code> to <code>M - 1</code>.<br>・For each <code>i</code>, compute a hash of text characters <code>i</code> to <code>M + i - 1</code>.<br>・If pattern hash = text substring hash, check for a match.</p><p>所以算法的关键在于如何高效地计算哈希值：Horner’s method - 用于评估M阶多项式的线性时间方法<br><img src="/images/horner_method.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Compute hash for M-digit key</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">hash</span><span class="params">(String key, <span class="keyword">int</span> M)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">long</span> h = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">h = (R * h + key.charAt(j)) % Q;</span><br><span class="line"><span class="keyword">return</span> h;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>给定$x_i$，如何计算$x_{i+1}$：<br>$$x_i = t_i R^{M-1} + T_{i+1}R^{M-2} + … + t_{i+M-1}R^0$$<br>$$x_{i+1} = t_{i+1}R^{M-1} + T_{i+2}R^{M-2} + … + t_{i+M}R^0$$<br>$$x_{i+1} = (x_i - t_i R^{M-1}) R + t_{i+M}$$<br>M-digit, base-R integer, modulo Q，$R^{M-1}$是可以预先计算的.<br><img src="/images/rk.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RabinKarp</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> patHash; <span class="comment">// pattern hash value</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> M; <span class="comment">// pattern length</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> Q; <span class="comment">// modulus</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> R; <span class="comment">// radix</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> RM; <span class="comment">// R^(M-1) % Q</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">RabinKarp</span><span class="params">(String pat)</span> </span>&#123;</span><br><span class="line">M = pat.length();</span><br><span class="line">R = <span class="number">256</span>;</span><br><span class="line">Q = longRandomPrime(); <span class="comment">// a large prime (but avoid overflow)</span></span><br><span class="line"></span><br><span class="line">RM = <span class="number">1</span>; <span class="comment">// precompute R^&#123;M – 1&#125; (mod Q)</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= M-<span class="number">1</span>; i++)</span><br><span class="line">RM = (R * RM) % Q;</span><br><span class="line">patHash = hash(pat, M);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">hash</span><span class="params">(String key, <span class="keyword">int</span> M)</span></span></span><br><span class="line"><span class="function"></span>&#123; <span class="comment">/* as before */</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** check for hash collision using rolling hash function */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(String txt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> N = txt.length();</span><br><span class="line"><span class="keyword">int</span> txtHash = hash(txt, M);</span><br><span class="line"><span class="keyword">if</span> (patHash == txtHash) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = M; i &lt; N; i++)</span><br><span class="line">&#123;</span><br><span class="line">txtHash = (txtHash + Q - RM*txt.charAt(i-M) % Q) % Q;</span><br><span class="line">txtHash = (txtHash*R + txt.charAt(i)) % Q;</span><br><span class="line"><span class="keyword">if</span> (patHash == txtHash) <span class="keyword">return</span> i - M + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> N;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>有两种方式判断是否匹配：</p><ul><li>Monte Carlo version. Return match if hash match.<ul><li>Always runs in linear time. Extremely likely to return correct answer (but not always!).</li></ul></li><li>Las Vegas version. Check for substring match if hash match; continue search if false collision.<ul><li>Always returns correct answer. Extremely likely to run in linear time (but worst case is M N).</li></ul></li></ul><blockquote><p>In theory, if <code>Q</code> is a sufficiently large random prime (about $M N^2$), then the probability of a false collision is about <code>1 / N</code>.<br>In practice, choose <code>Q</code> to be a large prime (but not so large to cause overflow). Under reasonable assumptions, probability of a collision is about <code>1 / Q</code>.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/images/substring_search_cost_summary.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br>算法可以拓展到二维模式匹配, 多模式匹配等问题.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;字符串搜索/匹配算法中, Boyer-Moore(BM)比前面的&lt;a href=&quot;/NLP-01-string-searching-algorithm-kmp&quot;&gt;Knuth–Morris–Pratt(KMP)&lt;/a&gt;更高效. BM算法从右到左扫描模式中的字符。当匹配的字符在模式中不存在时，可以跳过最多达M个字符.&lt;img src=&quot;/images/bm.png&quot; alt=&quot;&quot; title=&quot;image from: https://www.coursera.org/learn/algorithms-part2/&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Boyer-Moore" scheme="http://shukebeta.me/tags/Boyer-Moore/"/>
    
  </entry>
  
  <entry>
    <title>NLP与信息处理 01 - 字符搜索匹配算法 01 - KMP</title>
    <link href="http://shukebeta.me/NLP-01-string-searching-algorithm-01-kmp/"/>
    <id>http://shukebeta.me/NLP-01-string-searching-algorithm-01-kmp/</id>
    <published>2018-09-25T16:00:00.000Z</published>
    <updated>2018-10-07T07:57:01.029Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.</p></blockquote><p>字符串搜索/匹配算法在大规模文本应用中有非常重要的作用，比如文章敏感词搜索，多关键词过滤搜索等。如果使用暴力搜索，则时间复杂度很高（若 m 为关键字的长度， n 为要待搜索的字符串长度， k为关键字数量，则复杂度为$O(n \times m \times k)$。而好的算法可以让这些问题的时间复杂度大大降低。</p><p>常用的算法有Knuth–Morris–Pratt(KMP), Boyer-Moore(BM), Rabin-Karp(RK), Trie, Trie图, AC自动机等.<br><a id="more"></a></p><h2 id="KMP"><a href="#KMP" class="headerlink" title="KMP"></a>KMP</h2><p>Knuth–Morris–Pratt 算法在每次搜索匹配前，预处理模式字符串，记录模式字符串自身重复的情况，即包含左临界的子字符串和包含右临界的子字符串的重复长度，以此来作为后续匹配的参考。当实际去匹配带搜索字符串时，想象是我们拿着模式字符串从左到右匹配过去。任何时候，如果发现匹配不上的时，不是简单地右移一位继续重新匹配，而是在已匹配部分的范围内，直接跳过k个字符，右移到与模式开头部分重复的位置，并接着重复部分后面继续开始匹配（重复的部分证明已经匹配过）。</p><p>一个很好解释<a href="https://blog.csdn.net/v_july_v/article/details/7041827" target="_blank" rel="noopener">参考</a><br><img src="/images/kmp1.png" alt=""><img src="/images/kmp2.png" alt="" title="https://blog.csdn.net/v_july_v/article/details/7041827"><br>KMP算法的理论基础基于确定性有限状态自动机 DFA（Deterministic finite state automaton）。DFA可以理解为抽象的字符搜索机：</p><ul><li>有限数量的<strong>状态</strong>（包括开始和停止）。</li><li>字母表每个字符对应一个状态转换。</li><li>只接受能通往停止状态的转换序列。</li></ul><p><img src="/images/kmp_dfa.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br>如<code>ABCAABABABAB</code>对应<code>0→1→2→0→1→1→2→3→4→5→4→5→4</code></p><p>可以看出KMP算法的几个要点:</p><ul><li>状态State: 表示模式中已匹配的字符数。<code>pattern[]</code>最长前缀(同时是<code>txt[0..i]</code>的后缀)的长度.</li><li>需要预先从<code>pattern</code>中计算<code>dfa[][]</code></li><li>字符串遍历指针只会前进不会后退</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(String txt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i, j, N = txt.length();</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>, j = <span class="number">0</span>; i &lt; N &amp;&amp; j &lt; M; i++)</span><br><span class="line">j = dfa[txt.charAt(i)][j];</span><br><span class="line"><span class="keyword">if</span> (j == M) <span class="keyword">return</span> i - M;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">return</span> N;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Running time: at most N character accesses to Simulate DFA on text.</p><h3 id="使用DFA"><a href="#使用DFA" class="headerlink" title="使用DFA"></a>使用DFA</h3><p>用状态<code>state j</code>表示模式<code>pat</code>的前<code>j</code>个字符已经匹配. 用<code>dfa[c][j]</code>表示在状态<code>state j</code>时, 遇到下一个字符<code>c</code>, 应该转移到什么状态.</p><p>在状态<code>state j</code>：<br>· 如果匹配成功，<code>char c == pat.charAt(j)</code>，则转移到<code>state j+1</code>: <code>dfa[pat.charAt(j)][j] = j+1</code>.<br>· 如果失败，则从<code>pat[1..j-1]</code>对应的DFA状态<code>X</code>，向着<code>c</code>方向转移：<code>copy dfa[][X] to dfa[][j]</code>.<br>・Update <code>X</code>.<br><img src="/images/build_dfa.png" alt="" title="image from: https://www.coursera.org/learn/algorithms-part2/"><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KMP</span><span class="params">(String pat)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">this</span>.pat = pat;</span><br><span class="line">M = pat.length();</span><br><span class="line">dfa = <span class="keyword">new</span> <span class="keyword">int</span>[R][M];</span><br><span class="line">dfa[pat.charAt(<span class="number">0</span>)][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> X = <span class="number">0</span>, j = <span class="number">1</span>; j &lt; M; j++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; R; c++)</span><br><span class="line">dfa[c][j] = dfa[c][X]; <span class="comment">// copy mismatch cases</span></span><br><span class="line">dfa[pat.charAt(j)][j] = j+<span class="number">1</span>;</span><br><span class="line">X = dfa[pat.charAt(j)][X];</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Running time. M character accesses (but space/time proportional to R M).</p><h3 id="NEXT数组"><a href="#NEXT数组" class="headerlink" title="NEXT数组"></a>NEXT数组</h3><p>假设现在文本串<code>S</code>匹配到<code>i</code>位置，模式串<code>P</code>匹配到<code>j</code>位置:<br><img src="https://img-blog.csdn.net/20140812223633281" alt="" title="image from: https://blog.csdn.net/v_july_v/article/details/7041827"><br>首先第一位NEXT值<code>next[0]</code>一定是<code>-1</code>, 后面的第<code>j+1</code>个值<code>next[j]</code>代表<code>[0, j-1]</code>部分有多少重复前后缀. 可以根据前一位的<code>next[j-1]</code>值<code>k</code>推断:</p><ul><li>如果<code>P[j] == P[k]</code>, 则意味着重复前后缀长度<code>+1</code>, 那么<code>next[j] = next[j-1] + 1 = k + 1</code>.</li><li>如果<code>P[j] != P[k]</code>, 则意味着前面重复前后缀无法继续下去. 此时不能简单地判定<code>next[j] = 0</code>, 需要继续往前看是否有更短小的重复前后缀. 又因为重复前后缀意味着前后端相同, 因此可以直接匹配<code>P[j] ?= P[k&#39;], k&#39; = next[k]</code>, 以此循环下去直到<code>k&#39; = -1</code>为止.</li></ul><p>以上算法还可以进一步优化: 当<code>P[j] != S[i]</code>时，下次必然匹配<code>P[next [j]] ?= S[i]</code>，此时如果<code>P[j] == P[next[j]]</code>，则意味着匹配必然失败. 这样就出现了冗余匹配。所以如果出现<code>P[j] == P[next[j]]</code>，则令<code>next[j] = next[next[j]] = next[next[j-1] + 1] = next[k + 1]</code>.<img src="http://hi.csdn.net/attachment/201106/14/8394323_13080758591kyV.jpg" alt="" title="image from: https://blog.csdn.net/v_july_v/article/details/7041827"><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">GetNext</span><span class="params">(<span class="keyword">char</span>[] p, <span class="keyword">int</span>[] next)</span> </span>&#123;</span><br><span class="line">next[<span class="number">0</span>] = -<span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> k = -<span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> j = -<span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (j &lt; p.length - <span class="number">1</span>) &#123;</span><br><span class="line"><span class="comment">//p[k]表示前缀，p[j]表示后缀</span></span><br><span class="line"><span class="keyword">if</span> (k == -<span class="number">1</span> || p[j] == p[k]) &#123;</span><br><span class="line">            <span class="keyword">if</span> (p[++j] != p[++k])</span><br><span class="line">                next[j] = k;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">next[j] = next[k];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">k = next[k];</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>对于优化后的next数组可以发现一点：如果模式串的后缀跟前缀相同，那么它们的next值也是相同的，例如模式串abcabc，它的前缀后缀都是abc，其优化后的next数组为：-1 0 0 -1 0 0，前缀后缀abc的next值都为-1 0 0.</p></blockquote><h3 id="基于NEXT数组的KMP算法"><a href="#基于NEXT数组的KMP算法" class="headerlink" title="基于NEXT数组的KMP算法"></a>基于NEXT数组的KMP算法</h3><p>基于前面求出的NEXT数组, 可以根据以下步骤实现KMP算法:</p><ul><li>假设现在文本串<code>S</code>匹配到<code>i</code>位置，模式串<code>P</code>匹配到<code>j</code>位置<ul><li>if <code>j = -1</code>, or <code>S[i] == P[j]</code>(当前字符匹配成功), then <code>i++, j++</code></li><li>else, <code>j = next[j]</code>。当失配时，模式串<code>P</code>相对于文本串<code>S</code>向右移动<code>j - next[j]</code>位(大于等于1)。</li></ul></li></ul><p><img src="https://img-blog.csdn.net/20150812214857858" alt="" title="image from: https://img-blog.csdn.net/20150812214857858"><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 返回模式串第一次在文本串中出现的位置 */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">KmpSearch</span><span class="params">(<span class="keyword">char</span>[] s, <span class="keyword">char</span>[] p)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; s.length &amp;&amp; j &lt; p.length) &#123;</span><br><span class="line"><span class="keyword">if</span> (j == -<span class="number">1</span> || s[i] == p[j]) &#123;</span><br><span class="line">i++;</span><br><span class="line">j++;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">j = next[j];</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (j == p.length) &#123;</span><br><span class="line">        <span class="keyword">return</span> i - j;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="KMP应用"><a href="#KMP应用" class="headerlink" title="KMP应用"></a>KMP应用</h3><p>判断一段文字（原串）里面是否存在敏感词（模式串）。原题地址 <a href="http://hihocoder.com/problemset/problem/1015?sid=1368409：" target="_blank" rel="noopener">http://hihocoder.com/problemset/problem/1015?sid=1368409：</a></p><p>输入：第一行一个整数N，表示测试数据组数。接下来的N*2行，每两行表示一个测试数据。在每一个测试数据中，第一行为模式串，由不超过10^4个大写字母组成，第二行为原串，由不超过10^6个大写字母组成。其中N&lt;=20.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">5</span><br><span class="line">HA</span><br><span class="line">HAHAHA</span><br><span class="line">WQN</span><br><span class="line">WQN</span><br><span class="line">ADA</span><br><span class="line">ADADADA</span><br><span class="line">BABABB</span><br><span class="line">BABABABABABABABABB</span><br><span class="line">DAD</span><br><span class="line">ADDAADAADDAAADAAD</span><br></pre></td></tr></table></figure></p><p>输出: 对于每一个测试数据，按照它们在输入中出现的顺序输出一行Ans，表示模式串在原串中出现的次数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">1</span><br><span class="line">3</span><br><span class="line">1</span><br><span class="line">0</span><br></pre></td></tr></table></figure></p><p>这里是需要找出出现的次数，因此不仅仅是找到第一个，还要继续遍历原串统计剩余出现的次数，同时要考虑重叠的部分。关键的功能代码可以复制上面的，但是需要作出一些改动。</p><p>首先，因为部分重叠情况也是要统计的，比如<code>ADADADA</code>这个原串, 包含3个<code>ADA</code>. 所以要考虑即使匹配成功后, 模式串的索引<code>j</code>能够跳回前面的重复前缀位置. 考虑使用未优化过的NEXT计算方法.</p><p>除此之外, 通过额外计算多一位NEXT值, 记录完整的模式串的重复前后缀长度, 以协助索引.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">GetNext</span><span class="params">(<span class="keyword">char</span>[] p, <span class="keyword">int</span>[] next)</span> </span>&#123;</span><br><span class="line">    next[<span class="number">0</span>] = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> k = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (j &lt; p.length) &#123;</span><br><span class="line">        <span class="keyword">if</span> (k == -<span class="number">1</span> || p[j] == p[k]) &#123;</span><br><span class="line">            next[++j] = ++k;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            k = next[k];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>对于<code>ADA</code>, 原来未优化过的NEXT是<code>-1,0,0</code>, 现在是<code>-1,0,0,1</code>, 最后一位<code>1</code>表示<code>ADA</code>这个模式串的重复前后缀是长度为<code>1</code>的<code>A</code>.</p><p>借助额外长度的NEXT, 只需要小小地改动KMP计算逻辑, 最重要的是当<code>j</code>到达<code>NEXT</code>的额外位置时, 通过<code>j = next[j];</code>把<code>j</code>后拨到重复前缀的位置, 以方便进行后面的搜索.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 返回模式串在原串中出现的次数 */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">KmpSearch</span><span class="params">(<span class="keyword">char</span>[] s, <span class="keyword">char</span>[] p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span>[] next = <span class="keyword">new</span> <span class="keyword">int</span>[p.length + <span class="number">1</span>]; <span class="comment">// 额外多计算一位</span></span><br><span class="line">    GetNext(p, next);</span><br><span class="line">    <span class="keyword">int</span> c = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> len = s.length;</span><br><span class="line">    <span class="keyword">while</span> (i &lt; len) &#123;</span><br><span class="line">        <span class="keyword">if</span> (j == -<span class="number">1</span> || s[i] == p[j]) &#123;</span><br><span class="line">            i++; j++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            j = next[j];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (j == p.length) &#123;</span><br><span class="line">            c++;</span><br><span class="line">            j = next[j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;字符串搜索/匹配算法在大规模文本应用中有非常重要的作用，比如文章敏感词搜索，多关键词过滤搜索等。如果使用暴力搜索，则时间复杂度很高（若 m 为关键字的长度， n 为要待搜索的字符串长度， k为关键字数量，则复杂度为$O(n \times m \times k)$。而好的算法可以让这些问题的时间复杂度大大降低。&lt;/p&gt;
&lt;p&gt;常用的算法有Knuth–Morris–Pratt(KMP), Boyer-Moore(BM), Rabin-Karp(RK), Trie, Trie图, AC自动机等.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="http://shukebeta.me/categories/AI/"/>
    
      <category term="NLP" scheme="http://shukebeta.me/categories/AI/NLP/"/>
    
    
      <category term="NLP" scheme="http://shukebeta.me/tags/NLP/"/>
    
      <category term="Algorithm" scheme="http://shukebeta.me/tags/Algorithm/"/>
    
      <category term="Trie" scheme="http://shukebeta.me/tags/Trie/"/>
    
      <category term="KMP" scheme="http://shukebeta.me/tags/KMP/"/>
    
      <category term="Aho–Corasick algorithm" scheme="http://shukebeta.me/tags/Aho%E2%80%93Corasick-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>位操作 - 快速幂</title>
    <link href="http://shukebeta.me/bits-operations-07/"/>
    <id>http://shukebeta.me/bits-operations-07/</id>
    <published>2018-09-25T16:00:00.000Z</published>
    <updated>2018-10-22T09:18:06.806Z</updated>
    
    <content type="html"><![CDATA[<p>如何实现快速的幂运算？<br><a id="more"></a></p><p>要求$c = a^b$, 按照朴素算法把<code>a</code>连乘<code>b</code>次的时间复杂度是$O(n)$. 而快速幂能做到$O(\log n)$。把<code>b</code>转换为二进制, 二进制数第<code>i</code>位的权为$2^{i-1}$，就可以把二进制拆分为若干个以<code>2</code>为底的真数, 然后利用幂数的性质，例如用朴素算法求$a^{11}$要求乘<code>11</code>次. 考虑到<code>11</code>的二进制为<code>1011</code>, 如果把$a^{11}$拆分为:<br>$$a^{11} = a^{a_0 2^0 + a_1 2^1 + a_2 0 + a_3 2^3} = a^1 a^2 a^8$$<br>可以看到每一个因子都是上一个因子的平方，利用$a^2 a^2$求出$a^4$, 同样利用$a^4$的平方求出$a^8$, 每次计算只需要用到上一次计算出来的结果, 所以总的运算次数是<code>4</code>次. 任何一个数<code>b</code>最多能写成长度为$O(\log b)$的二进制, 因此这个算法就是$O(\log n)$.</p><p>在程序设计中是根据<code>b</code>的二进制中是否为<code>1</code>来控制是否乘以上一次翻倍的积</p><ul><li>不断右移<code>b</code>, 直到<code>b</code>不再有<code>1</code>：<ul><li>根据当前位的权重（当前<code>b</code>最后一位）是否为<code>1</code>来决定<code>c</code>是否乘以最新的<code>a</code></li><li>把<code>a</code>平方，用于下一位计算</li></ul></li></ul><p>在Java中要考虑极端值INT_MIN<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 递归</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">myPow</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span> temp = myPow(x, n/<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">if</span> (n % <span class="number">2</span> ==<span class="number">0</span>) <span class="keyword">return</span> temp * temp;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(n &gt; <span class="number">0</span>) <span class="keyword">return</span> x*temp*temp;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> (temp*temp) / x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 循环</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">myPow</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> ans = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(n &lt; <span class="number">0</span>)&#123;</span><br><span class="line">        n = -(n+<span class="number">1</span>); <span class="comment">// 处理极端值</span></span><br><span class="line">        x = <span class="number">1</span>/x;</span><br><span class="line">        ans *= x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    System.out.println(n);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((n &amp; <span class="number">1</span>) == <span class="number">1</span>) ans *= x;</span><br><span class="line">        x *= x;</span><br><span class="line">        n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="快速幂取余"><a href="#快速幂取余" class="headerlink" title="快速幂取余"></a>快速幂取余</h3><p>求<code>a^b mod c</code>.<br>如果<code>b</code>是偶数, <code>a^b mod c</code> = $(a^2)^{b/2} \% c$<br>如果<code>b</code>是奇数, <code>a^b mod c</code> = $((a^2)^{b/2} \times a) \% c$</p><p>又因为取余有性质:<code>a^b mod c = (a mod c)^b</code></p><p>引理：<code>(a * b) mod c = [( a mod c ) * (b mod c) ] mod c</code></p><p>证明：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">设 a mod c =d，b mod c= e;</span><br><span class="line">       则：a=t*c + d ;  b=k*c + e ;</span><br><span class="line">       (a*b)mod c = (t*c+d)(t*c+e)</span><br><span class="line">                 = (tk c^2 + ( te+dk ) *c + d*e) mod c</span><br><span class="line">                 = de mod c</span><br></pre></td></tr></table></figure></p><p>即积的取余等于取余的积的取余.</p><p>利用快速幂的思想, 令<code>k = (a * a) mod c</code>，所要求的最终结果即为 <code>k^(b/2) mod c</code>, 这个过程可以迭代下去, 如果b是奇数, 或多出一项<code>a mod c</code>. 当<code>b = 0</code>时, 所有因子已经相乘, 迭代结束, 复杂度为<code>O(log b)</code><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span>  <span class="title">PowerMod</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span>  ans = <span class="number">1</span>;</span><br><span class="line">    a = a % c;</span><br><span class="line">    <span class="keyword">while</span>(b&gt;<span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(b % <span class="number">2</span> = = <span class="number">1</span>)</span><br><span class="line">            ans = (ans * a) % c;</span><br><span class="line">        b = b/<span class="number">2</span>;       <span class="comment">//   b&gt;&gt;=1;</span></span><br><span class="line">        a = (a * a) % c;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如何实现快速的幂运算？&lt;br&gt;
    
    </summary>
    
      <category term="CS" scheme="http://shukebeta.me/categories/CS/"/>
    
    
      <category term="Software Engineer" scheme="http://shukebeta.me/tags/Software-Engineer/"/>
    
      <category term="Algorithms" scheme="http://shukebeta.me/tags/Algorithms/"/>
    
      <category term="Computer Science" scheme="http://shukebeta.me/tags/Computer-Science/"/>
    
  </entry>
  
</feed>
