<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="0p5a0VOKCc8etsdDimlaZoAC96x8VeV9Ab5HWs5NcVw" />








  <meta name="baidu-site-verification" content="EbMAKHjVzF" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Python,NLP,UoE," />










<meta name="description" content="爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh References:Accelerated natural language processingANLP revision guideLecture Slides from the Stanford Course">
<meta name="keywords" content="Python,NLP,UoE">
<meta property="og:type" content="article">
<meta property="og:title" content="Accelerated Natural Language Processing">
<meta property="og:url" content="http://shukebeta.me/UoE-anlp/index.html">
<meta property="og:site_name" content="Computer Science &amp; AI">
<meta property="og:description" content="爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh References:Accelerated natural language processingANLP revision guideLecture Slides from the Stanford Course">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://shukebeta.me/images/kneser-ney.png">
<meta property="og:image" content="http://shukebeta.me/images/Treebank.png">
<meta property="og:image" content="http://shukebeta.me/images/viterbi.png">
<meta property="og:image" content="http://shukebeta.me/images/Recursive_descent.png">
<meta property="og:image" content="http://shukebeta.me/images/CKY_proba1.png">
<meta property="og:image" content="http://shukebeta.me/images/CKY_proba2.png">
<meta property="og:image" content="http://shukebeta.me/images/softmax.png">
<meta property="og:image" content="http://shukebeta.me/images/discriminative.png">
<meta property="og:image" content="http://shukebeta.me/images/Dependency_Relations.png">
<meta property="og:image" content="http://shukebeta.me/images/cos.png">
<meta property="og:image" content="http://shukebeta.me/images/PMI.png">
<meta property="og:image" content="http://shukebeta.me/images/PMI_counts.png">
<meta property="og:image" content="http://shukebeta.me/images/t_test.png">
<meta property="og:image" content="http://shukebeta.me/images/perplexity.png">
<meta property="og:image" content="http://shukebeta.me/images/Contingency.png">
<meta property="og:image" content="http://shukebeta.me/images/Pearson.png">
<meta property="og:updated_time" content="2018-12-11T03:04:19.599Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Accelerated Natural Language Processing">
<meta name="twitter:description" content="爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh References:Accelerated natural language processingANLP revision guideLecture Slides from the Stanford Course">
<meta name="twitter:image" content="http://shukebeta.me/images/kneser-ney.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://shukebeta.me/UoE-anlp/"/>





  <title>Accelerated Natural Language Processing | Computer Science & AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Computer Science & AI</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://shukebeta.me/UoE-anlp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Computer Science & AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Accelerated Natural Language Processing</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-10-01T00:00:00+08:00">
                2017-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index">
                    <span itemprop="name">AI</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/UoE-anlp/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="UoE-anlp/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/UoE-anlp/" class="leancloud_visitors" data-flag-title="Accelerated Natural Language Processing">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words&#58;</span>
                
                <span title="Words">
                  6,821
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Estimated &asymp;</span>
                
                <span title="Estimated">
                  42 min
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh</p>
<p>References:<br><a href="http://www.inf.ed.ac.uk/teaching/courses/anlp/" target="_blank" rel="noopener">Accelerated natural language processing</a><br><a href="https://www.inf.ed.ac.uk/teaching/courses/anlp/review/review_ay17.html" target="_blank" rel="noopener">ANLP revision guide</a><br><a href="https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html" target="_blank" rel="noopener">Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning</a><br><a id="more"></a></p>
<h2 id="概率模型-Probability-model"><a href="#概率模型-Probability-model" class="headerlink" title="概率模型 Probability model"></a>概率模型 Probability model</h2><p>概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率</p>
<h3 id="估算概率-Probability-estimation"><a href="#估算概率-Probability-estimation" class="headerlink" title="估算概率 Probability estimation"></a>估算概率 Probability estimation</h3><p>相关频率/最大似然估计<br>Relative frequency / maximum likelihood estimation p(X) = Count(x)/N</p>
<h3 id="平滑-Smoothing"><a href="#平滑-Smoothing" class="headerlink" title="平滑 Smoothing"></a>平滑 Smoothing</h3><p>一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。</p>
<h3 id="Language-modeling"><a href="#Language-modeling" class="headerlink" title="Language modeling"></a>Language modeling</h3><ul>
<li>What: To compute the probability of sentence /sequence of words P(w1, w2, w3…), or to predict upcomming words P(w|w1, w2, w3…)… a language model is also a probability model.</li>
<li>Why: the motivation is that probability is essential in identifying information in noisy, ambiguous inputs: speech recognition, machine translation, spelling correction…</li>
<li>How: rely on chain rule of probability, the products of a sequence of <strong>conditional</strong> probability.</li>
<li>Simplified by Markov Assumption: approximate the conditional probability by only accounting several prefixes,<br><code>P(the| water is so transparent that) ≈ P(the| that)</code></li>
<li><a href="#evaluation-concepts-and-methods">Evaluation</a>: how good is the model</li>
</ul>
<h2 id="GENERATIVE-PROBABILISTIC-MODELS"><a href="#GENERATIVE-PROBABILISTIC-MODELS" class="headerlink" title="GENERATIVE PROBABILISTIC MODELS"></a>GENERATIVE PROBABILISTIC MODELS</h2><p>Generative(joint) models palce probabilities P(c,d) over both observed data d and the hidden variables c (generate the obersved data from hidden stuff).</p>
<h3 id="N-Gram-Language-Model"><a href="#N-Gram-Language-Model" class="headerlink" title="N-Gram Language Model"></a>N-Gram Language Model</h3><ul>
<li>Unigram P(w1,w2,w3..) ≈ P(w1)*P(w2)*P(w3)</li>
<li>Bigram P(wn| w1,w2,w3..) ≈ P(wn| wn-1)</li>
<li>Estimate probability by counting:<br><code>P(wi| prefixes) = count(prefixes, wi)/count(prefixes)</code></li>
<li>In practice, use log space to avoid underflow, and adding is faster than multiplying.</li>
<li>Insufficient:<ul>
<li>long-distance dependencies</li>
<li>N-grams only work well for word prediction if the test corpus looks like the training corpus.</li>
</ul>
</li>
<li>To deal with 0 probability, commonly use <a href="#kneser-ney-smoothing">Kneser-Ney smoothing</a>, for very large N-grams like web, use stupid backoff.</li>
</ul>
<h4 id="Add-alpha-smoothing"><a href="#Add-alpha-smoothing" class="headerlink" title="Add alpha smoothing"></a>Add alpha smoothing</h4><ul>
<li>Assign equal probability to all unseen events.</li>
<li>Applied in text classification, or domains where zeros probability is not common.</li>
</ul>
<h4 id="Backoff-smoothing"><a href="#Backoff-smoothing" class="headerlink" title="Backoff smoothing"></a>Backoff smoothing</h4><ul>
<li>Use information from lower order N-grams (shorter histories)</li>
<li>Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram.</li>
<li>Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams.</li>
</ul>
<h4 id="Interpolation-smoothing"><a href="#Interpolation-smoothing" class="headerlink" title="Interpolation smoothing"></a>Interpolation smoothing</h4><ul>
<li>Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts</li>
<li>Simple interpolation: <code>P(w3|w1,w2)=1P(w3|w1,w2)+λ2P(w3|w2)+λ3P(w3)</code>, Σλ=1.</li>
<li>λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data</li>
</ul>
<h4 id="Kneser-Ney-smoothing"><a href="#Kneser-Ney-smoothing" class="headerlink" title="Kneser-Ney smoothing"></a>Kneser-Ney smoothing</h4><ul>
<li>Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams.</li>
<li>Use a better estimate for probabilities of lower-order unigrams, the continuation probability, P_continuatin(w) is how likely is w to appear as a novel continutaion.<ul>
<li>For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w.</li>
<li>Every bigram type was a novel continuation the first time it was seen.</li>
<li>normalized by the total number of word bigram types.</li>
</ul>
</li>
<li>To lower the probability of some fix bigram like “San Franscio”</li>
<li>For bigram, <code>Pkn(wi|wi-1)=max(count(wi-1,wi)-d, 0)/c(wi-1) +λ(wi-1)P_continuatin(wi), λ(wi-1) = d{w:count(wi-1,w)&gt;0}/c(wi-1)</code>, where {w:count(wi-1,w)&gt;0} is the number of word types that can follow wi-1, also is the # of word types we discounted, also is the # of times we applied normalized discount.</li>
<li>For general N-gram, <img src="/images/kneser-ney.png" alt=""></li>
</ul>
<h3 id="Naive-Bayes-classifier"><a href="#Naive-Bayes-classifier" class="headerlink" title="Naive Bayes classifier"></a>Naive Bayes classifier</h3><ul>
<li>Application: <a href="#text-classification">Text classification</a>, to classify a text, we calculate each class probability given the test sequence, and choose the biggest one.</li>
<li>Evaluation: <a href="#precision-recall-f-measure">precision, recall, F-measure</a></li>
<li>Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了</li>
</ul>
<h3 id="Text-classification"><a href="#Text-classification" class="headerlink" title="Text classification"></a>Text classification</h3><p>Or text categorization, method is not limited to NB, see <a href="http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab7.html" target="_blank" rel="noopener">lab7</a>.<br>Spam email, gender/authorship/language identification, sentiments analysis,(opinion extraction, subjectivity analysis)…</p>
<h4 id="Sentiments-analysis"><a href="#Sentiments-analysis" class="headerlink" title="Sentiments analysis"></a>Sentiments analysis</h4><ul>
<li>For sentiment(or other text classification), word occurrence may matter more than word frequency. Thus it often improves performance to clip the word counts in each document at 1.<ul>
<li>This variant binary NB is called binary multinominal naive Bayes or binary NB.</li>
<li>Remove duplicates in each data sample - bag of words representation, boolean features. Binarized seems to work better than full word counts.</li>
</ul>
</li>
<li>Deal with negation: <code>like, not like</code>, A very simple baseline that is commonly used in sentiment to deal with negation is during text normalization to prepend the prefix NOT_ to every word after a token of logical negation</li>
<li>Sentiment lexicons: lists of words that are preannotated with positive or negative sentiment. To deal with insufficient labeled training data. A common way to use lexicons in the classifier is to use as one feature the total<br>count of occurrences of any words in the positive lexicon, and as a second feature the total count of occurrences of words in the negative lexicon. Using just two features results in classifiers that are much less sparse to small amounts of training data, and may generalize better. See <a href="http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab8.html" target="_blank" rel="noopener">lab8</a>.</li>
</ul>
<h4 id="Naive-Bayes-Assumptions"><a href="#Naive-Bayes-Assumptions" class="headerlink" title="Naive Bayes Assumptions"></a>Naive Bayes Assumptions</h4><ul>
<li>Bags of words: a set of unordered words/features with its frequency in the documents, their order was ignored.</li>
<li>Conditional independence: the probabilities P(w|C) are independence given the class, thus a sequence of words(w1,w2,w3…) probability coculd be estimate via prducts of each P(wi|C) by walking through every pisition of the sequence, noted that the orders in the sequence does not matter.</li>
</ul>
<h4 id="NB-Training"><a href="#NB-Training" class="headerlink" title="NB Training"></a>NB Training</h4><ul>
<li>Each classes’ prior probability P(C) is the percentage of the classes in the training set.</li>
<li>For the test set, its probability as a class j, is the products of its sequence probability P(w1, w2, w3…|Cj) and P(Cj), normalized by the sequence probability P(w1, w2, w3…), which could be calculated by summing all P(w1, w2, w3…|Cj)*P(Cj).</li>
<li>The joint features probability P(w1, w2, w3…|C) of each class is calculated by naively multiplying each word’s MLE given that class.</li>
<li>In practice, to deal with 0 probability, we dun use MLE, instead we use add alpha smoothing.<ul>
<li>Why 0 probability matters? Because it makes the whole sequence probability P(w1, w2, w3…|C) 0, then all the other features as evidence for the class are eliminated too.</li>
<li>How: first extract all the vocabulary V in the training set.</li>
<li>Then, for each feature/word k, its add alpha smoothing probability estimation within a class j is (Njk + alpha)/(Nj+V*alpha).</li>
<li>This is not the actual probability, but just the numerator.</li>
</ul>
</li>
</ul>
<h4 id="Naive-bayes-relationship-to-language-modelling"><a href="#Naive-bayes-relationship-to-language-modelling" class="headerlink" title="Naive bayes relationship to language modelling"></a>Naive bayes relationship to language modelling</h4><ul>
<li>When using all of the words as features for naive bayes, then each class in naive bayes is a unigram languange model.</li>
<li>For each word, assign probability P(word|C),</li>
<li>For each sentence, assign probability P(S|C) = P(w1,w2,w3…|C)</li>
<li>Running multiple languange models(classes) to assign probabilities, and pick out the highest language model.</li>
</ul>
<h3 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a>Hidden Markov Model</h3><p>The HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence.</p>
<ul>
<li>parameter λ:<ul>
<li>Initial probability π</li>
<li>Transition probability matrix A, $P(Tag_{i+1} | Tag_{i})$</li>
<li>Emission probability B, $P(Word | Tag)$</li>
</ul>
</li>
<li>Application: part-of-speech tagging, name entity recognition(NEr), parse tree, speech recognition</li>
<li>Hidden: these tags, trees or words is not observed(hidden)</li>
<li>The three fundamental problems of HMM:<ul>
<li>decoding: discover the best hidden state sequence via <a href="#viterbi-algorithm">Viterbi algorithm</a>, Viterbi的实现参考<a href="https://github.com/congchan/Chinese-nlp/blob/master/hmm_pos_tag.ipynb" target="_blank" rel="noopener">HMM POS Tagging</a></li>
<li>Probability of the observation: Given an HMM with know parameters λ and an observation sequence O, determine the likelihood P(O| λ) (a language model regardless of tags) via <a href="#forward-algorithm">Forward algorithm</a></li>
<li>Learning: Given only the observed sequence, learn the best(MLE) HMM parameters λ via <a href="#forward-backward-algorithm">forward-backward algorithm</a>, thus <a href="#hmm-training">training a HMM</a> is an unsupervised learning task.</li>
</ul>
</li>
</ul>
<p>总结:</p>
<ul>
<li>前向算法和后向算法解决如何计算似然$P(O| λ)$的问题</li>
<li>Viterbi算法解决HMM 解码问题.</li>
<li>这些算法都是动态规划算法</li>
</ul>
<h4 id="Part-of-speech-tagging"><a href="#Part-of-speech-tagging" class="headerlink" title="Part-of-speech tagging"></a>Part-of-speech tagging</h4><ul>
<li>Part-of-speech(POS), word classes, or syntactic categories, a description of eight parts-of-speech: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner.<ol>
<li>noun 名詞 (代號 n. )</li>
<li>pronoun 代名詞 (代號 pron. )</li>
<li>verb 動詞 (代號 v. )</li>
<li>adjective 形容詞 (代號 adj. )</li>
<li>adverb 副詞 (代號 adv. )</li>
<li>preposition 介系詞 (代號 prep. )</li>
<li>conjunction 連接詞 (代號 conj. )</li>
<li>interjection 感歎詞 (代號 int. )</li>
</ol>
</li>
<li>Motivation: Use model to find the best tag sequence T for an untagged sentence S: argmax P(T|S) -&gt; argmax P(S|T)*P(T), where P(T) is the  transition (prior) probabilities,  P(S|T) is the emission (likelihood) probabilities.</li>
<li>Parts-of-speech can be divided into two broad supercategories: <a href="#open-class-closed-class">closed class types and open class types</a></li>
<li>Search for the best tag sequence: <a href="#Viterbi-Algorithm">Viterbi algorithm</a></li>
<li>evaluation: tag accuracy</li>
</ul>
<h5 id="Transition-Probability-Matrix"><a href="#Transition-Probability-Matrix" class="headerlink" title="Transition Probability Matrix"></a>Transition Probability Matrix</h5><ul>
<li>Tags or states</li>
<li>Each (i,j) represent the probability of moving from state i to j</li>
<li>When estimated from sequences, should include beginning <code>&lt;s&gt;</code> and end <code>&lt;/s&gt;</code> markers.</li>
<li>Tag transition probability matrix: the probability of tag i followed by j</li>
</ul>
<h5 id="Emission-Probability"><a href="#Emission-Probability" class="headerlink" title="Emission Probability"></a>Emission Probability</h5><ul>
<li>Also called observation likelihoods, each expressing the probability of an observation j being generated from a states i.</li>
<li>Word/symbol</li>
</ul>
<h5 id="Penn-Treebank"><a href="#Penn-Treebank" class="headerlink" title="Penn Treebank"></a>Penn Treebank</h5><p><img src="/images/Treebank.png" alt=""></p>
<h4 id="Viterbi-Algorithm"><a href="#Viterbi-Algorithm" class="headerlink" title="Viterbi Algorithm"></a>Viterbi Algorithm</h4><ul>
<li>Decoding task: the task of determining which sequence of variables is the underlying source of some sequence of observations.</li>
<li>Intuition: The probability of words w1 followed by w2 with tag/state i and j (i,j is index of all Tags), is the chain rule of the probability of i followed by j and the probability of i output wi P(w1|i) and P(w2 |j), then choose the maximum from all the possible i j. Then using chain rule to multiply the whole sequence of words.</li>
<li>The value of each cell Vt(j) is computed by recursively taking the most probable path that could lead us to this cell from left columns to right. See exampls in <a href="http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab_solutions.html#Tutorial_2" target="_blank" rel="noopener">tutorial 2</a><br><img src="/images/viterbi.png" alt=""></li>
<li>Since HMM based on Markov Assumptions, so the present column Vt is only related with the nearby left column Vt-1.</li>
</ul>
<h4 id="Forward-algorithm"><a href="#Forward-algorithm" class="headerlink" title="Forward algorithm"></a>Forward algorithm</h4><ul>
<li>Compute the likelihood of a particular observation sequence.</li>
<li>Implementation is almost the same as Viterbi.</li>
<li>Yet Viterbi takes the max over the previous path probabilities whereas the forward algorithm takes the sum.</li>
</ul>
<h4 id="HMM-Training"><a href="#HMM-Training" class="headerlink" title="HMM Training"></a>HMM Training</h4><p>learning the parameters of an HMM</p>
<h5 id="Forward-backward-algorithm"><a href="#Forward-backward-algorithm" class="headerlink" title="Forward-backward algorithm"></a>Forward-backward algorithm</h5><ul>
<li>inputs: just the observed sequence</li>
<li>output: the converged λ(A,B).</li>
<li>For each interation k until λ converged:<ul>
<li>Compute expected counts using λ(k-1)</li>
<li>Set λ(k) using MLE on the expected counts.</li>
</ul>
</li>
</ul>
<h3 id="Context-free-grammar"><a href="#Context-free-grammar" class="headerlink" title="Context-free grammar"></a>Context-free grammar</h3><p>CFG(phrase-structure grammar) consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered toLexicon gether, and a lexicon of words and symbols.</p>
<h4 id="Probabilistic-Context-Free-Grammar"><a href="#Probabilistic-Context-Free-Grammar" class="headerlink" title="Probabilistic Context-Free Grammar"></a>Probabilistic Context-Free Grammar</h4><p>PCFG(Stochastic Context-Free Grammar SCFG (SCFG)), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability.</p>
<ul>
<li>G = (T,N,S,R,P)<ul>
<li>T, N: Terminal and Non-terminal</li>
<li>S: starts symbol</li>
<li>R: Derive rule/grammar, N -&gt; N/C</li>
<li>P: a probability function, for a given N, ΣP(N-&gt;Ni/Ci)=1. Normally P(S-&gt;NP VP)=1, because this is the only rule for S.</li>
</ul>
</li>
<li>PCFG could generates a sentence/tree,<ul>
<li>thus it is a language model, assigns a probability to the string of words constituting a sentence</li>
<li>The probability of a tree t is the product of the probabilities of the rules used to generate it.</li>
<li>The probability of the string s is the sum of the probabilities of the trees/parses which have that string as their yield.</li>
<li>The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence.</li>
</ul>
</li>
<li>Application: Probabilistic parsing</li>
<li>Shortage: lack the lexicalization of a trigram model, i.e only a small fraction of the rules contains information about words. To solve this problem, use <a href="#lexicalization-of-pcfgs">lexicalized PCFGs</a></li>
</ul>
<h4 id="Lexicalization-of-PCFGs"><a href="#Lexicalization-of-PCFGs" class="headerlink" title="Lexicalization of PCFGs"></a>Lexicalization of PCFGs</h4><ul>
<li>The head word of phrase gives a good representation of the phrase’s structure and meaning</li>
<li>Puts the properties of words back into a PCFG</li>
<li>Word to word affinities are useful for certain ambiguities, because we know the probability of rule with words and words now, e.g. PP attachment ambiguity</li>
</ul>
<h4 id="Recursive-Descent-Parsing"><a href="#Recursive-Descent-Parsing" class="headerlink" title="Recursive Descent Parsing"></a>Recursive Descent Parsing</h4><ul>
<li>It is a top-down, depth-first parser:<ol>
<li>Blindly expand nonterminals until reaching a terminal (word).</li>
<li>If multiple options available, choose one but store current state<br>as a backtrack point (in a stack to ensure depth-first.)</li>
<li>If terminal matches next input word, continue; else, backtrack<br><img src="/images/Recursive_descent.png" alt=""></li>
</ol>
</li>
<li>can be massively inefficient (exponential in sentence length) if faced with local ambiguity</li>
<li>infinite loop</li>
</ul>
<h4 id="CKY-parsing"><a href="#CKY-parsing" class="headerlink" title="CKY parsing"></a>CKY parsing</h4><h5 id="Dynamic-programming"><a href="#Dynamic-programming" class="headerlink" title="Dynamic programming"></a>Dynamic programming</h5><h5 id="Well-formed-substring-table"><a href="#Well-formed-substring-table" class="headerlink" title="Well-formed substring table"></a>Well-formed substring table</h5><p>For parsing, subproblems are analyses of substrings, memoized in well-formed substring table(WFST, chart).</p>
<ul>
<li>Chart entries are indexed by start and end positions in the sentence, and correspond to:<ul>
<li>either a complete constituent (sub-tree) spanning those positions (if working bottom-up),</li>
<li>or a prediction about what complete constituent might be found (if working top-down).</li>
</ul>
</li>
<li>The chart is a matrix where cell [i, j] holds information about the word span from position i to position j:<ul>
<li>The root node of any constituent(s) spanning those words</li>
<li>Pointers to its sub-constituents</li>
<li>(Depending on parsing method,) predictions about what<br>constituents might follow the substring.</li>
</ul>
</li>
</ul>
<h5 id="Probability-CKY-parsing"><a href="#Probability-CKY-parsing" class="headerlink" title="Probability CKY parsing"></a>Probability CKY parsing</h5><p><img src="/images/CKY_proba1.png" alt=""><br><img src="/images/CKY_proba2.png" alt=""></p>
<h3 id="Noisy-channel-model"><a href="#Noisy-channel-model" class="headerlink" title="Noisy channel model:"></a>Noisy channel model:</h3><ul>
<li>The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel.</li>
<li>a probability model using Bayesian inference, input -&gt; noisy/errorful encoding -&gt; output, see an observation x (a misspelled word) and our job is to find the word w that generated this misspelled word.</li>
<li><code>P(w|x) = P(x|w)\*P(w)/P(x)</code></li>
</ul>
<h4 id="Noisy-channel-model-of-spelling-using-naive-bayes"><a href="#Noisy-channel-model-of-spelling-using-naive-bayes" class="headerlink" title="Noisy channel model of spelling using naive bayes"></a>Noisy channel model of spelling using <a href="#nb-training">naive bayes</a></h4><ul>
<li>The noisy channel model is to maximize the product of likelihood(probability estimation) P(s|w) and the prior probability of correct words P(w). Intuitively it is modleing the noisy channel that turn a correct word ‘w’ to the misspelling.</li>
<li>The likelihood(probability estimation) P(s|w) is called the the channel/error model, telling if it was the word ‘w’, how likely it was to generate this exact error.</li>
<li>The P(w) is called the language model</li>
</ul>
<h2 id="DISCRIMINATIVE-PROBABILISTIC-MODELS"><a href="#DISCRIMINATIVE-PROBABILISTIC-MODELS" class="headerlink" title="DISCRIMINATIVE PROBABILISTIC MODELS"></a>DISCRIMINATIVE PROBABILISTIC MODELS</h2><p>Discriminative(conditional) models take the data as given, and put a probability over hidden structure given the data, P(c|d).</p>
<h3 id="Exponential-Log-linear-MaxEnt-Logistic-models"><a href="#Exponential-Log-linear-MaxEnt-Logistic-models" class="headerlink" title="Exponential (Log-linear, MaxEnt, Logistic) models"></a>Exponential (Log-linear, MaxEnt, Logistic) models</h3><p>Make probability model from the linear combination of weights λ and features f as votes, normalized by the total votes<img src="/images/softmax.png" alt="">.</p>
<ul>
<li>It is a probabilistic distribution: it estimates a probability for each class/label, aka Softmax.</li>
<li>It is a classifier, choose the highest probability label.</li>
<li>Application: dependency parsing actions prediction, text classification, <a href="#word-sense-disambiguation">Word sense disambiguation</a><br><img src="/images/discriminative.png" alt=""></li>
</ul>
<h4 id="Topics-categorization"><a href="#Topics-categorization" class="headerlink" title="Topics categorization"></a>Topics categorization</h4><h4 id="Training-discriminative-model"><a href="#Training-discriminative-model" class="headerlink" title="Training discriminative model"></a>Training discriminative model</h4><ul>
<li>Features in NLP are more general, they specify indicator function(a yes/no[0,1] boolean matching function) of properties of the input and each class.</li>
<li>Weights: low possibility features will associate with low/negative weight, vise versa.</li>
<li>Define features: Pick sets of data points d which are distinctive enough to deserve model parameters: related words, words contians #, words end with ing, etc.</li>
</ul>
<h4 id="Regularization-in-discriminative-model"><a href="#Regularization-in-discriminative-model" class="headerlink" title="Regularization in discriminative model"></a>Regularization in discriminative model</h4><p>The issue of scale:</p>
<ul>
<li>Lots of features</li>
<li>sparsity:<ul>
<li>easily overfitting: need smoothing</li>
<li>Many features seen in training never occur again in test</li>
</ul>
</li>
<li>Optimization problem: feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. See <a href="http://www.inf.ed.ac.uk/teaching/courses/anlp/tutorials/anlp_t04-sol.pdf" target="_blank" rel="noopener">tutorial 4</a>.</li>
<li>Solution:<ul>
<li>Early stopping</li>
<li>Smooth the parameter via L2 regularization.</li>
<li>Smooth the data, like the add alpha smoothing, but hard to know what artificial data to create</li>
</ul>
</li>
</ul>
<h3 id="Generative-vs-Discriminative-Models"><a href="#Generative-vs-Discriminative-Models" class="headerlink" title="Generative vs. Discriminative Models"></a>Generative vs. Discriminative Models</h3><ul>
<li>Navie bayes models multi-count correlated evidence: each feature is multipled in, even when you have multiple features telling the same informaiton.</li>
<li>Maxent: solve this issue by weighting features so that model expectations match the observed(empirical) expectations.</li>
</ul>
<h2 id="Basic-Text-Processing"><a href="#Basic-Text-Processing" class="headerlink" title="Basic Text Processing"></a>Basic Text Processing</h2><h3 id="Regular-Expressions"><a href="#Regular-Expressions" class="headerlink" title="Regular Expressions"></a>Regular Expressions</h3><p>a language for specifying text search strings.</p>
<h3 id="Word-tokenization"><a href="#Word-tokenization" class="headerlink" title="Word tokenization"></a>Word tokenization</h3><p>NLP task needs to do text normalizaGon:</p>
<ol>
<li>Segmenting/tokenizing words in running text</li>
<li>Normalizing word formats</li>
<li>Segmenting sentences in running text</li>
</ol>
<p><code>they lay back on the San Francisco grass and looked at the stars and their</code></p>
<ul>
<li>Type: an element of the vocabulary.</li>
<li>Token: an instance of that type in the actual text.</li>
</ul>
<h2 id="LINGUISTIC-AND-REPRESENTATIONAL-CONCEPTS"><a href="#LINGUISTIC-AND-REPRESENTATIONAL-CONCEPTS" class="headerlink" title="LINGUISTIC AND REPRESENTATIONAL CONCEPTS"></a>LINGUISTIC AND REPRESENTATIONAL CONCEPTS</h2><h3 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h3><ul>
<li>Parsing is a combination of recognizing an input string and assigning a <strong>correct</strong> linguistic structure/tree to it based on a grammar.</li>
<li>The Syntactic, Statistical parsing are constituent-based representations(context-free grammars).</li>
<li>The Dependency Parsing are based on dependency structure(dependency grammars).</li>
</ul>
<h4 id="Syntactic-Parsing"><a href="#Syntactic-Parsing" class="headerlink" title="Syntactic Parsing"></a>Syntactic Parsing</h4><p>Syntactic parsing, is the task of recognizing a sentence and assigning a correct syntactic structure to it.</p>
<ul>
<li>Syntactic parsing can be viewed as a search<ul>
<li>search space: all possible trees generated by the grammar</li>
<li>search guided by the structure of the space and the input.</li>
<li>search direction<ul>
<li>top-down: start with root category (S), choose expansions, build down to words.</li>
<li>bottom-up: build subtrees over words, build up to S.</li>
</ul>
</li>
<li>Search algorithm/strategy: DFS, BFS, Recursive descent parsing, CKY Parsing</li>
</ul>
</li>
<li>Challenge: Structual <a href="#ambiguity">Ambiguity</a></li>
</ul>
<h4 id="Statistical-Parsing"><a href="#Statistical-Parsing" class="headerlink" title="Statistical Parsing"></a>Statistical Parsing</h4><p>Or probabilistic parsing, Build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers.</p>
<ul>
<li>motivation: to solve the problem of disambiguation</li>
<li>algorithm: <a href="#probability-cky-parsing">probability CKY parsing</a></li>
<li>evaluation: Compare the output <strong>constituency</strong> parser with golden standard tree, a constituent(part of the output parser) marked as correct if it spans the same sentence positions with the corresponding constituent in golder standard tree. Then we get the <a href="#precision-recall-f-measure">precision, recall and F1 measure</a>.<ul>
<li>constituency: S-(0:10), NP-(0:2), VP-(0:9)…</li>
<li>Precission = (# correct constituents)/(# in parser output), recall = (# correct constituents)/(# in gold standard)</li>
<li>Not a good evaluation, because it higher order constituent is marked wrong simply it contains a lower level wrong constituent.</li>
</ul>
</li>
</ul>
<h4 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a><a href="#dependency-parsing">Dependency Parsing</a></h4><h3 id="Constituency"><a href="#Constituency" class="headerlink" title="Constituency"></a>Constituency</h3><p>Phrase structure, organizes words into nested constituents. Groups of words behaving as a single units, or constituents.</p>
<ul>
<li>Noun phrase(NP), a sequence of words surrounding at least one noun. While the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase</li>
<li>Preposed or Postposed constructions. While the entire phrase can be placed differently, the individual words making up the phrase cannot be.</li>
<li>Fallback: In languages with free word order, phrase structure<br>(constituency) grammars don’t make as much sense.</li>
<li>Headed phrase structure: many phrase has head, VP-&gt;VB, NP-&gt;NN, the other symbols excepct the head is modifyer.</li>
</ul>
<h3 id="Dependency-syntax"><a href="#Dependency-syntax" class="headerlink" title="Dependency syntax"></a>Dependency syntax</h3><p>Dependency structure shows which words depend on (modify or are arguments of) which other words.</p>
<ul>
<li>A fully lexicalized formalism without phrasal constituents and phrase-structure rules: binary, asymmetric grammatical relations between words.</li>
<li>More specific, head-dependent relations, with edges point from heads to their dependents.</li>
<li>Motivation: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. E.g. we may need both S → NP VP and S → VP NP, but could not tell too much information simply looking at the rule.</li>
<li>Dependencies: Identifies syntactic relations directly. The syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words.</li>
<li>Relation between phrase structure and dependency structure<ul>
<li>Convert phrase structure annotations to dependencies via head rules. (Convenient if we already have a phrase structure treebank.): For a given lexicalized constituency parse(CFG tree), remove the phrasal categories, remove the (duplicated) terminals,  and collapse chains of duplicates.</li>
<li>The closure of dependencies give constituency from a dependency tree</li>
</ul>
</li>
</ul>
<p><img src="/images/Dependency_Relations.png" alt=""></p>
<h4 id="Dependency-parsing"><a href="#Dependency-parsing" class="headerlink" title="Dependency parsing"></a>Dependency parsing</h4><ul>
<li>Motivation: context-free parsing algorithms base their decisions on adjacency; in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective); we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present).</li>
<li>Approach: Transition-based dependency parsing</li>
</ul>
<h4 id="Transition-based-dependency-parsing"><a href="#Transition-based-dependency-parsing" class="headerlink" title="Transition-based dependency parsing"></a>Transition-based dependency parsing</h4><p>transition-based systems use supervised machine learning methods to train classifiers that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators(actions).</p>
<ul>
<li>Bottom up</li>
<li>Like shift-reduce parsing, but the ‘reduce’ actions are specialized to create dependencies with head on left or right.</li>
<li>configuration：consists of a stack, an input buffer of words or tokens, and a set of relations/arcs, a set of actions.</li>
<li>How to choose the next action: each action is predicted by a <a href="#discriminative-probability-models">discriminative classifier</a>(often SVM, could be maxent) over each legal move.<ul>
<li>features: a sequence of the correct (configuration, action) pairs f(c ; x).</li>
</ul>
</li>
<li>Evaluation: accuracy (# correct dependencies with or ignore label)).</li>
</ul>
<h4 id="Dependency-tree"><a href="#Dependency-tree" class="headerlink" title="Dependency tree"></a>Dependency tree</h4><ul>
<li>Dependencies from a CFG tree using heads, must be projective: There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words.</li>
<li>But dependency theory normally does allow non-projective structures to account for displaced constituents.</li>
</ul>
<h4 id="Bounded-and-unbounded-dependencies"><a href="#Bounded-and-unbounded-dependencies" class="headerlink" title="Bounded and unbounded dependencies"></a>Bounded and unbounded dependencies</h4><p>Unbounded dependency could be considered as long distance dependency</p>
<ul>
<li>Long-distance dependencies: contained in wh-non-subject-question, “What flights do you have from Burbank to Tacoma Washington?”, the Wh-NP <code>what flights</code> is far away from the predicate that it is semantically related to, the main verb <code>have</code> in the VP.</li>
</ul>
<h3 id="Ambiguity"><a href="#Ambiguity" class="headerlink" title="Ambiguity"></a>Ambiguity</h3><h4 id="Structural-ambiguity"><a href="#Structural-ambiguity" class="headerlink" title="Structural ambiguity"></a>Structural ambiguity</h4><p>Occurs when the grammar can assign more than one parse to a sentence.</p>
<h5 id="Attachment-ambiguity"><a href="#Attachment-ambiguity" class="headerlink" title="Attachment ambiguity"></a>Attachment ambiguity</h5><p>A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place.</p>
<h5 id="Coordination-ambiguity"><a href="#Coordination-ambiguity" class="headerlink" title="Coordination ambiguity"></a>Coordination ambiguity</h5><p>different sets of phrases can be conjoined by a conjunction like and. E.g <code>green egg and bread</code>.</p>
<ul>
<li>Coordination: The major phrase types discussed here can be conjoined with conjunctions like <code>and, or, and but</code> to form larger constructions of the same type.</li>
</ul>
<h4 id="Global-and-local-ambiguity"><a href="#Global-and-local-ambiguity" class="headerlink" title="Global and local ambiguity"></a>Global and local ambiguity</h4><ul>
<li>global ambiguity: multiple analyses for a full sentence, like <code>I saw the man with the telescope</code></li>
<li>local ambiguity: multiple analyses for parts of sentence.<ul>
<li><code>the dog bit the child</code>: first three words could be NP (but aren’t).</li>
<li>Building useless partial structures wastes time.</li>
</ul>
</li>
</ul>
<h3 id="Morphology"><a href="#Morphology" class="headerlink" title="Morphology"></a>Morphology</h3><blockquote>
<p>构词学（英语言学分科学名：morphology，“组织与形态”；morphology (/mɔːrˈfɒlədʒi/[1]) is the study of words, how they are formed, and their relationship to other words in the same language.<ref>），又称形态学，是语言学的一个分支，研究单词（word）的内部结构和其形成方式。如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher。构词学正是研究这种单字间组成的关系，并试着整理出其组成的规则。</ref></p>
</blockquote>
<h4 id="Challenge-of-rich-Morphology"><a href="#Challenge-of-rich-Morphology" class="headerlink" title="Challenge of rich Morphology"></a>Challenge of rich Morphology</h4><p>For a morphologically rich language, many issues would arise because of the morphological complexity.</p>
<ul>
<li>These productive word-formation processes result in a large vocabulary for these languages</li>
<li>Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages</li>
<li>For POS, augmentations become necessary when dealing with highly inflected or agglutinative languages with rich morphology like Czech, Hungarian and Turkish., part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a<br>single primitive tag.</li>
<li>Dependency grammar is better than constituency in dealing with languages that are morphologically rich。</li>
</ul>
<h4 id="morphemes"><a href="#morphemes" class="headerlink" title="morphemes"></a>morphemes</h4><p>The way words are built up from smaller meaning-bearing units.</p>
<h4 id="Lemma"><a href="#Lemma" class="headerlink" title="Lemma"></a>Lemma</h4><ul>
<li>Lexeme, refers to the set of all the forms that have the same meaning,</li>
<li>lemma: refers to the particular form that is chosen by convention to represent the lexeme.</li>
<li>E.g: <code>run, runs, ran, running</code> are forms of the same lexeme, with run as the lemma.</li>
</ul>
<h4 id="Affixes"><a href="#Affixes" class="headerlink" title="Affixes"></a>Affixes</h4><p>Adding additional meanings of various kinds. “+ed, un+”</p>
<ul>
<li>suffix : follow the stem<ul>
<li>Plural of nouns ‘cat+s’</li>
</ul>
<ol start="2">
<li>Comparative and superlative of adjectives ‘small+er’ </li>
<li>Formation of adverbs ‘great+ly’</li>
<li>Verb tenses ‘walk+ed’ </li>
<li>All inflectional morphology in English uses suffixes</li>
</ol>
</li>
<li>Prefix: precede the stem<ul>
<li>In English: these typically change the meaning </li>
</ul>
<ol start="2">
<li>Adjectives ‘un+friendly’, ‘dis+interested’</li>
<li>Verbs ‘re+consider’</li>
<li>Some language use prefixing much more widely</li>
</ol>
</li>
<li>Infix: inserted inside the stem</li>
<li>Circumfix: do both(follow, precede)</li>
</ul>
<h4 id="Morphological-parsing"><a href="#Morphological-parsing" class="headerlink" title="Morphological parsing"></a>Morphological parsing</h4><p>Method: Finite-state transducers</p>
<h5 id="Finite-state-transducers"><a href="#Finite-state-transducers" class="headerlink" title="Finite-state transducers"></a>Finite-state transducers</h5><p>FST, a transducer maps between one representation and another; It is a kind of FSA which maps between two sets of symbols.</p>
<h4 id="Root"><a href="#Root" class="headerlink" title="Root"></a>Root</h4><ul>
<li>Root, stem and base are all terms used in the literature to designate that part of a word that remains when all affixes have been removed.</li>
<li>The root word is the primary lexical unit of a word, and of a word family (this root is then called the base word), which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents.</li>
<li>E.g: In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’.</li>
</ul>
<h4 id="Stem"><a href="#Stem" class="headerlink" title="Stem"></a>Stem</h4><ul>
<li>Stem is of concern only when dealing with inflectional morphology</li>
<li>Stemming: reduce terms to their stems in info retrieval,</li>
<li>E.g: In the form ‘untouchables’ the stem is ‘untouchable’, ‘touched’ -&gt; ‘touch’; ‘wheelchairs’ -&gt; ‘wheelchair’.</li>
</ul>
<h4 id="Inflectional-vs-Derivational-Morphology"><a href="#Inflectional-vs-Derivational-Morphology" class="headerlink" title="Inflectional vs. Derivational Morphology"></a>Inflectional vs. Derivational Morphology</h4><p>Inflectional<br>· nouns for count (plural: +s) and for possessive case (+’s)<br>· verbs for tense (+ed, +ing) and a special 3rd person singular present form (+s)<br>· adjectives in comparative (+er) and superlative (+est) forms.</p>
<p>Derivational<br>· Changing the part of speech, e.g. noun to verb: ‘word → wordify’<br>· Changing the verb back to a noun<br>· Nominalization: formation of new nouns, often verbs or adjectives</p>
<table>
<thead>
<tr>
<th>Inflectional</th>
<th>Derivational</th>
</tr>
</thead>
<tbody>
<tr>
<td>does not change basic meaning or part of speech</td>
<td>may change the part of speech or meaning of a word</td>
</tr>
<tr>
<td>expresses grammatical features or relations between words</td>
<td>not driven by syntactic relations outside the word</td>
</tr>
<tr>
<td>applies to all words of the same part of speech, inflection occurs at word edges: govern+ment+s, centr+al+ize+d</td>
<td>applies closer to the stem</td>
</tr>
</tbody>
</table>
<h3 id="Open-class-Closed-class"><a href="#Open-class-Closed-class" class="headerlink" title="Open-class Closed-class"></a>Open-class Closed-class</h3><p>Closed classes are those with relatively fixed membership</p>
<ul>
<li>prepositions: on, under, over, near, by, at, from, to, with</li>
<li>determiners: a, an, the</li>
<li>pronouns: she, who, I, others</li>
<li>conjunctions: and, but, or, as, if, when</li>
<li>auxiliary verbs: can, may, should, are</li>
<li>particles: up, down, on, off, in, out, at, by</li>
<li>numerals: one, two, three, first, second, third</li>
</ul>
<p>Open-class</p>
<ul>
<li>Nouns, verbs, adjectives, adverbs</li>
</ul>
<h3 id="Word-sense"><a href="#Word-sense" class="headerlink" title="Word sense"></a>Word sense</h3><p>A discrete representation of an aspect of a word’s meaning.<br>How: <a href="#distributional-semantic-models">Distributional semantic models</a></p>
<h4 id="Word-sense-disambiguation"><a href="#Word-sense-disambiguation" class="headerlink" title="Word sense disambiguation"></a>Word sense disambiguation</h4><p>WSD, The task of selecting the correct sense for a word, formulated as a classification task.</p>
<ul>
<li>Chose features: Directly neighboring words, content words, syntactically related words, topic of the text, part-of-speech tag, surrounding part-of-speech tags, etc …</li>
</ul>
<h4 id="Collocation"><a href="#Collocation" class="headerlink" title="Collocation"></a>Collocation</h4><p>A sequence of words or terms that co-occur more often than would be expected by chance.</p>
<h4 id="Lexical-semantic-relationships"><a href="#Lexical-semantic-relationships" class="headerlink" title="Lexical semantic relationships"></a>Lexical semantic relationships</h4><p>Relations between word senses</p>
<h5 id="synonym"><a href="#synonym" class="headerlink" title="synonym"></a>synonym</h5><p>代名词, When two senses of two different words (lemmas) are identical, or nearly identical, the two senses are synonyms. E.g. couch/sofa vomit/throw up filbert/hazelnut car/automobile</p>
<h5 id="hyponym"><a href="#hyponym" class="headerlink" title="hyponym"></a>hyponym</h5><p>下义词, One sense is a hyponym of another sense if the first sense is more specific, denoting a subclass of the other. E.g. car is a hyponym of vehicle; dog is a hyponym of animal, and mango is a hyponym of fruit.</p>
<h5 id="hypernym"><a href="#hypernym" class="headerlink" title="hypernym"></a>hypernym</h5><p>Superordinate, 上位词, vehicle is a hypernym of car, and animal is a hypernym of dog.</p>
<h5 id="similarity"><a href="#similarity" class="headerlink" title="similarity"></a>similarity</h5><p>Or distance, a looser metric than synonymy.<br>Two ways to measure similarity:</p>
<ul>
<li>Thesaurus词库-based: are words nearby in hypernym hierarchy? Do words have similar definitions?</li>
<li>Distributional: do words have similar distributional contexts</li>
</ul>
<h3 id="Distributional-semantic-models"><a href="#Distributional-semantic-models" class="headerlink" title="Distributional semantic models"></a>Distributional semantic models</h3><p>Vector semantics(embeddings): The meaning of a word is represented as a vector.</p>
<ul>
<li>Two words are similar if they have similar word contexts vector.</li>
<li>Term-context matrix(Co-occurrence    Matrices): a word/term is defined by a vector over counts of context words. The row represent words, columns contexts.<ul>
<li>Problem: simple frequency isn’t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. “the” and “of” are very frequent, but maybe not the most discriminative.</li>
<li>Sulution: use <a href="#pointwise-mutual-information">Pointwise mutual information</a>. Then the Co-occurrence    Matrices is filled with PPMI, instead of raw counts.</li>
</ul>
</li>
<li>Measuring vectors similarity based on PPMI:<ul>
<li>Dot product(inner product): More frequent words will have higher dot products, which cause similarity sensitive to word frequency.</li>
<li>Cosine: normalized dot product <img src="/images/cos.png" alt="Cosine">, Raw frequency or PPMI is non-negative, so cosine range [0,1].</li>
</ul>
</li>
<li>Evaluation of similarity<ul>
<li>Intrinsic: <a href="#correlation">correlation</a> between algorithm and human word similarity ratings.</li>
<li>Check if there is <a href="#correlation">correlation</a> between similarity measures and word frequency.</li>
</ul>
</li>
<li>Application: sentiment analysis, see <a href="http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab8.html" target="_blank" rel="noopener">lab8</a></li>
</ul>
<h4 id="Pointwise-mutual-information"><a href="#Pointwise-mutual-information" class="headerlink" title="Pointwise mutual information"></a>Pointwise mutual information</h4><p>PMI: do events x and y co-occur more than if they were independent?</p>
<ul>
<li>PMI between two words: <img src="/images/PMI.png" alt="PMI"></li>
<li>Compute PMI on a term-context matrix(using counts):<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PMI(x, y) = log2( N·C(x, y)/C(x)C(y) )</span><br><span class="line">p(w=information,c=data)	= 6/19</span><br><span class="line">p(w=information) = 11/19</span><br><span class="line">p(c=data) = 7/19</span><br><span class="line">PMI(information,data) = log2(6\*19/(11\*7))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/images/PMI_counts.png" alt="PMI"></p>
<ul>
<li>PMI is biased towards infrequent events, solution:<ul>
<li>Add-one smoothing<h5 id="PPMI"><a href="#PPMI" class="headerlink" title="PPMI"></a>PPMI</h5>Positive PMI, could better handle low frequencies<br><code>PPMI = max(PMI,0)</code></li>
</ul>
</li>
</ul>
<h4 id="t-test"><a href="#t-test" class="headerlink" title="t-test"></a>t-test</h4><p>The t-test statistic, like PMI, can be used to measure how much<br>more frequent the association is than chance.</p>
<ul>
<li>The t-test statistic computes the difference between observed and expected means, normalized by the variance.</li>
<li>The higher the value of t, the greater the likelihood that we can reject the null hypothesis.</li>
<li>Null hypothesis: the two words are independent, and hence P(a,b) = P(a)P(b) correctly models the relationship between the two words.<br><img src="/images/t_test.png" alt="t-test"></li>
</ul>
<h4 id="Minimum-Edit-Distance"><a href="#Minimum-Edit-Distance" class="headerlink" title="Minimum Edit Distance"></a>Minimum Edit Distance</h4><p>the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.<br>Algorithm: searching the shortest path, use Dynamic programming to avoid repeating, (use BFS to search the shortest path?)</p>
<h4 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a><a href="http://wordnetweb.princeton.edu/perl/webwn" target="_blank" rel="noopener">WordNet</a></h4><p>A hierarchically organizesd lexical database, resource for English sense relations</p>
<ul>
<li>Synset: The set of near-synonyms for a WordNet sense (for synonym set)</li>
</ul>
<h3 id="Topic-modelling"><a href="#Topic-modelling" class="headerlink" title="Topic modelling"></a>Topic modelling</h3><p>Instead of using supervised topic classification – rather not fix topics in advance nor do manual annotation, Use clustering to teases out the topics. Only the number of topics is specified in advance.</p>
<ul>
<li>Latent Dirichlet allocation(LDA): each document may be viewed as a mixture of various topics where each document is generated by LDA.</li>
<li>A topic is a distribution over words</li>
<li>generate document:<ol>
<li>Randomly choose a distribution over topics</li>
<li>For each word in the document<ol>
<li>randomly choose a topic from the distribution over topics</li>
<li>randomly choose a word from the corresponding topic (distribution over the vocabulary)</li>
</ol>
</li>
</ol>
</li>
<li>training: repeat until converge<ol>
<li>assign each word in each document to one of T topics.</li>
<li>For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t)</li>
<li>Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d)</li>
</ol>
</li>
</ul>
<h3 id="Meaning-representation-language"><a href="#Meaning-representation-language" class="headerlink" title="Meaning representation language"></a>Meaning representation language</h3><p>The symbols in our meaning representations correspond to objects, properties, and relations in the world.</p>
<ul>
<li>Qualifications of MRL:<ul>
<li>Canonical form: sentences with the same (literal) meaning should have the same MR.</li>
<li>Compositional: The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined.</li>
<li>Verifiable: Can use the MR of a sentence to determine whether the sentence is true with respect to some given model of the world.</li>
<li>Unambiguous: an MR should have exactly one interpretation.</li>
<li>Inference: we should be able to verify sentences not only directly, but also by drawing conclusions based on the input MR and facts in the knowledge base.</li>
<li>Expressivity: the MRL should allow us to handle a wide range of meanings and express appropriate relationships between the words in a sentence.</li>
</ul>
</li>
<li>Good MRL: First-order Logic</li>
</ul>
<h4 id="First-order-Logic"><a href="#First-order-Logic" class="headerlink" title="First-order Logic"></a>First-order Logic</h4><p>FOL, Predicate logic, meets all of the MRL qualifications <strong>except compositionality</strong>.</p>
<ul>
<li>Expressions are constructed from terms:<ul>
<li>constant and variable symbols that represent entities</li>
<li>function symbols that allow us to indirectly specify entities</li>
<li>predicate symbols that represent properties of entities and relations between entities</li>
</ul>
</li>
<li>Terms can be combined into predicate-argument structures<ul>
<li>Logical connectives: ∨ - or, ∧ - and, ¬, ⇒</li>
<li>Quantifiers: ∀ (universal quantifier, i.e., “for all”), ∃ (existential<br>quantifier, i.e. “exists”)</li>
</ul>
</li>
<li>Predicates in FOL<ul>
<li>Predicates with multiple arguments represent relations between entities: member-of(UK, EU)</li>
<li>“/N” to indicate that a predicate takes N arguments: member-of/2</li>
</ul>
</li>
<li>Variables in FOL<ul>
<li>An expression consisting only of a predicate with a variable among its arguments is interpreted as a set: likes(x, Gim) is the set of entities that like Gim.</li>
<li>A predicate with a variable among its arguments only has a truth value if it is bound by a quantifier: ∀x.likes(x, Gim) has an interpretation as either true or false.</li>
<li>Universal Quantifier (∀): Cats are mammals has MR ∀x.cat(x) ⇒ mammal(x)</li>
<li>Existential Quantifier (∃): Used to express that a property/relation is true of some entity, without specifying which one: Marie owns a cat has MR ∃x.cat(x) ∧ owns(Marie,x)</li>
</ul>
</li>
</ul>
<h4 id="Lambda-λ-Expression"><a href="#Lambda-λ-Expression" class="headerlink" title="Lambda λ Expression"></a>Lambda λ Expression</h4><p>Extend FOL, to work with ‘partially constructed’ formula, <strong>Compositionality</strong>.</p>
<ul>
<li>E.g.： λx.sleep(x) is the function that takes an entity x to the FOL expression sleep(x). λx.sleep(x)(Marie) -&gt; sleep(Marie)</li>
<li>Verbal (event) MRs： <code>λz. λy. λx. Giving1(x,y,z) (book)(Mary)(John) -&gt; Giving1(John, Mary, book) -&gt; John gave Mary a book</code></li>
<li>Problem:<ul>
<li>fixed arguments</li>
<li>Requires separate <code>Giving</code> predicate for each syntactic subcategorisation frame(number/type/position of arguments).</li>
<li>Separate predicates have no logical relation: if <code>Giving3(a, b, c, d, e)</code> is true, what about <code>Giving2(a, b, c, d)</code> and <code>Giving1(a, b, c)</code>.</li>
</ul>
</li>
<li>Solution: Reification of events 事件具象化</li>
</ul>
<h4 id="Reification-of-events"><a href="#Reification-of-events" class="headerlink" title="Reification of events"></a>Reification of events</h4><p><code>John gave Mary a book -&gt; ∃e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary)
∧ Given(e,z) ∧ Book(z)</code></p>
<ul>
<li>Reify: to “make real” or concrete, i.e., give events the same status as<br>entities.</li>
<li>In practice, introduce variables for events, which we can quantify over</li>
<li>Entailment relations: automatically gives us logical entailment relations between events<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[John gave Mary a book on Tuesday] -&gt; [John gave Mary a book]</span><br><span class="line">∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) ∧ Time(e, Tuesday)</span><br><span class="line">-&gt;</span><br><span class="line">∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Semantic-Parsing"><a href="#Semantic-Parsing" class="headerlink" title="Semantic Parsing"></a>Semantic Parsing</h3><p>Aka semantic analysis. Systems for mapping from a text string to any logical form.</p>
<ul>
<li>Motivation: deriving a meaning representation from a sentence.</li>
<li>Application: question answering</li>
<li>Method: Syntax driven semantic analysis with semantic attachments</li>
</ul>
<h4 id="Syntax-Driven-Semantic-Analysis"><a href="#Syntax-Driven-Semantic-Analysis" class="headerlink" title="Syntax Driven Semantic Analysis"></a>Syntax Driven Semantic Analysis</h4><ul>
<li>Principle of compositionality: the construction of constituent meaning is derived from/composed of the meaning of the constituents/words within that constituent, guided by word order and syntactic relations.</li>
<li>Build up the MR by augmenting CFG rules with semantic composition rules. Add semantic attachments to CFG rules.</li>
<li>Problem: encounter invalide FOL for some (base-form) MR, need type-raise.</li>
<li>Training</li>
</ul>
<h4 id="Semantic-attachments"><a href="#Semantic-attachments" class="headerlink" title="Semantic attachments"></a>Semantic attachments</h4><p>E.g<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">VP → Verb NP : &#123;Verb.sem(NP.sem)&#125;</span><br><span class="line">Verb.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y)</span><br><span class="line">NP.sem = Meat</span><br><span class="line">-&gt;</span><br><span class="line">VP.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) (Meat)</span><br><span class="line">= λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat)</span><br></pre></td></tr></table></figure></p>
<p>The MR for VP, is computed by applying the MR function to VP’s children.</p>
<p>Complete the rule:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">S → NP VP : &#123;VP.sem(NP.sem)&#125;</span><br><span class="line">VP.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat)</span><br><span class="line">NP.sem = AyCaramba</span><br><span class="line">-&gt;</span><br><span class="line">S.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) (AyCa.)</span><br><span class="line">= ∃e. Serving(e) ∧ Server(e, AyCaramba) ∧ Served(e, Meat)</span><br></pre></td></tr></table></figure></p>
<h4 id="Lexical-semantics"><a href="#Lexical-semantics" class="headerlink" title="Lexical semantics"></a>Lexical semantics</h4><p>the meaning of individual words.</p>
<h2 id="EVALUATION-CONCEPTS-AND-METHODS"><a href="#EVALUATION-CONCEPTS-AND-METHODS" class="headerlink" title="EVALUATION CONCEPTS AND METHODS"></a>EVALUATION CONCEPTS AND METHODS</h2><h3 id="Instrinsic-vs-extrinsic-evaluation"><a href="#Instrinsic-vs-extrinsic-evaluation" class="headerlink" title="Instrinsic vs. extrinsic evaluation"></a>Instrinsic vs. extrinsic evaluation</h3><h4 id="Extrinsic"><a href="#Extrinsic" class="headerlink" title="Extrinsic"></a>Extrinsic</h4><p>Use something external to measure the model. End-to-end evaluation, the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves.</p>
<ol>
<li>Put each model in a task: spelling corrector, speech recognizer, MT system</li>
<li>Run the task, get an accuracy for A and for B<ul>
<li>How many misspelled words corrected properly</li>
<li>How many words translated correctly</li>
</ul>
</li>
<li>Compare accuracy for A and B</li>
</ol>
<p>Unfortunately, running big NLP systems end-to-end is often very expensive.</p>
<h4 id="Intrinsic"><a href="#Intrinsic" class="headerlink" title="Intrinsic"></a>Intrinsic</h4><p>Measures independenly to any application. Train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. Which means whichever model assigns a higher probability to the test set</p>
<h3 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h3><ul>
<li>It is intrinsic.</li>
<li>Intuition based on Shannon game:The best language model is one that best predicts an unseen test set(e.g. next word), gives the highest P(sentence) to the word that actually occurs.</li>
<li>Definition： Perplexity is the inverse probability of the test set, normalized by the number of words(lie between 0-1).</li>
<li><img src="/images/perplexity.png" alt="Use log probability"></li>
<li>So minimizing perplexity is the same as maximizing probability</li>
<li>Cannot divide 0, so use <a href="#smoothing">smoothing</a>.</li>
<li>Bad approximation: unless the test data looks just like the training data, so generally only useful in pilot experiments.</li>
</ul>
<h3 id="Human-evaluation"><a href="#Human-evaluation" class="headerlink" title="Human evaluation"></a>Human evaluation</h3><p>E.g to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to<br>gold labels match. We will refer to these human labels as the <strong>gold labels</strong>.</p>
<h3 id="Precision-Recall-F-measure"><a href="#Precision-Recall-F-measure" class="headerlink" title="Precision, Recall, F-measure"></a>Precision, Recall, F-measure</h3><ul>
<li>To deal with unbalanced lables</li>
<li>Application: <a href="#text-classification">text classification</a>, parsing.</li>
<li>Evaluation in text classification: the 2 by 2 contingency table<img src="/images/Contingency.png" alt="contingency table">, golden lable is true or false, the classifier output is positive or negative.</li>
</ul>
<h4 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h4><p>% of positive items that are golden correct, from the view of classifier</p>
<h4 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h4><p>% of golden correct items that are positive, from the view of test set.</p>
<h4 id="F-measure"><a href="#F-measure" class="headerlink" title="F-measure"></a>F-measure</h4><ul>
<li>Motivation: there is tradeoff between precision and recall, so we need a combined meeasure that assesses the P/R tradeoff.</li>
<li>The b parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of b &gt; 1 favor recall, while values of b &lt; 1 favor precision.</li>
<li>Balanced F1 measure with beta =1, F = 2PR/(P+R)</li>
</ul>
<h4 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h4><p>Recalled that confusion matrix’s row represent golden label, column represent the classifier’s output, to anwser the quesion：for any pair of classes(c1,c2), how many test sample from c1 were incorrectly assigned to c2&gt;</p>
<ul>
<li>Recall: Fraction of samples in c1 classified correctly, CM(c1,c1)/sum(CM(c1,:))</li>
<li>Precision: fraction of samples assigned c1 that are actually c1, CM(c1,c1)/sum(CM(:,c1))</li>
<li>Accuracy: sum of diagnal / all</li>
</ul>
<h3 id="Correlation"><a href="#Correlation" class="headerlink" title="Correlation"></a>Correlation</h3><p>When two sets of data are strongly linked together we say they have a High Correlation.<br>Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases.</p>
<ul>
<li>Pearson correlation: covariance of the two variables divided by the product of their standard deviations.<img src="/images/Pearson.png" alt="Pearson"></li>
<li>Spearman correlation: the Pearson correlation between the rank values of the two variables</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
            <a href="/tags/UoE/" rel="tag"># UoE</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/UoE-nlu/" rel="next" title="Natural Language Understanding">
                <i class="fa fa-chevron-left"></i> Natural Language Understanding
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/machine-learning-with-sklearn/" rel="prev" title="Machine Learning with Scikit-learn (Sklearn) 机器学习实践">
                Machine Learning with Scikit-learn (Sklearn) 机器学习实践 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- Go to www.addthis.com/dashboard to customize your tools -->
<div class="addthis_inline_share_toolbox">
  <script type = "text/javascript" src = "//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b35f789bd238372" async = "async" ></script>
</div>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Cong" />
            
              <p class="site-author-name" itemprop="name">Cong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">86</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/congchan/" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:shooterbeta@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#概率模型-Probability-model"><span class="nav-number">1.</span> <span class="nav-text">概率模型 Probability model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#估算概率-Probability-estimation"><span class="nav-number">1.1.</span> <span class="nav-text">估算概率 Probability estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#平滑-Smoothing"><span class="nav-number">1.2.</span> <span class="nav-text">平滑 Smoothing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Language-modeling"><span class="nav-number">1.3.</span> <span class="nav-text">Language modeling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GENERATIVE-PROBABILISTIC-MODELS"><span class="nav-number">2.</span> <span class="nav-text">GENERATIVE PROBABILISTIC MODELS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#N-Gram-Language-Model"><span class="nav-number">2.1.</span> <span class="nav-text">N-Gram Language Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Add-alpha-smoothing"><span class="nav-number">2.1.1.</span> <span class="nav-text">Add alpha smoothing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Backoff-smoothing"><span class="nav-number">2.1.2.</span> <span class="nav-text">Backoff smoothing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interpolation-smoothing"><span class="nav-number">2.1.3.</span> <span class="nav-text">Interpolation smoothing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kneser-Ney-smoothing"><span class="nav-number">2.1.4.</span> <span class="nav-text">Kneser-Ney smoothing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Naive-Bayes-classifier"><span class="nav-number">2.2.</span> <span class="nav-text">Naive Bayes classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-classification"><span class="nav-number">2.3.</span> <span class="nav-text">Text classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sentiments-analysis"><span class="nav-number">2.3.1.</span> <span class="nav-text">Sentiments analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Naive-Bayes-Assumptions"><span class="nav-number">2.3.2.</span> <span class="nav-text">Naive Bayes Assumptions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NB-Training"><span class="nav-number">2.3.3.</span> <span class="nav-text">NB Training</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Naive-bayes-relationship-to-language-modelling"><span class="nav-number">2.3.4.</span> <span class="nav-text">Naive bayes relationship to language modelling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hidden-Markov-Model"><span class="nav-number">2.4.</span> <span class="nav-text">Hidden Markov Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Part-of-speech-tagging"><span class="nav-number">2.4.1.</span> <span class="nav-text">Part-of-speech tagging</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Transition-Probability-Matrix"><span class="nav-number">2.4.1.1.</span> <span class="nav-text">Transition Probability Matrix</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Emission-Probability"><span class="nav-number">2.4.1.2.</span> <span class="nav-text">Emission Probability</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Penn-Treebank"><span class="nav-number">2.4.1.3.</span> <span class="nav-text">Penn Treebank</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Viterbi-Algorithm"><span class="nav-number">2.4.2.</span> <span class="nav-text">Viterbi Algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward-algorithm"><span class="nav-number">2.4.3.</span> <span class="nav-text">Forward algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HMM-Training"><span class="nav-number">2.4.4.</span> <span class="nav-text">HMM Training</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Forward-backward-algorithm"><span class="nav-number">2.4.4.1.</span> <span class="nav-text">Forward-backward algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Context-free-grammar"><span class="nav-number">2.5.</span> <span class="nav-text">Context-free grammar</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Probabilistic-Context-Free-Grammar"><span class="nav-number">2.5.1.</span> <span class="nav-text">Probabilistic Context-Free Grammar</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lexicalization-of-PCFGs"><span class="nav-number">2.5.2.</span> <span class="nav-text">Lexicalization of PCFGs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Recursive-Descent-Parsing"><span class="nav-number">2.5.3.</span> <span class="nav-text">Recursive Descent Parsing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CKY-parsing"><span class="nav-number">2.5.4.</span> <span class="nav-text">CKY parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Dynamic-programming"><span class="nav-number">2.5.4.1.</span> <span class="nav-text">Dynamic programming</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Well-formed-substring-table"><span class="nav-number">2.5.4.2.</span> <span class="nav-text">Well-formed substring table</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Probability-CKY-parsing"><span class="nav-number">2.5.4.3.</span> <span class="nav-text">Probability CKY parsing</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Noisy-channel-model"><span class="nav-number">2.6.</span> <span class="nav-text">Noisy channel model:</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Noisy-channel-model-of-spelling-using-naive-bayes"><span class="nav-number">2.6.1.</span> <span class="nav-text">Noisy channel model of spelling using naive bayes</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DISCRIMINATIVE-PROBABILISTIC-MODELS"><span class="nav-number">3.</span> <span class="nav-text">DISCRIMINATIVE PROBABILISTIC MODELS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exponential-Log-linear-MaxEnt-Logistic-models"><span class="nav-number">3.1.</span> <span class="nav-text">Exponential (Log-linear, MaxEnt, Logistic) models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Topics-categorization"><span class="nav-number">3.1.1.</span> <span class="nav-text">Topics categorization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-discriminative-model"><span class="nav-number">3.1.2.</span> <span class="nav-text">Training discriminative model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regularization-in-discriminative-model"><span class="nav-number">3.1.3.</span> <span class="nav-text">Regularization in discriminative model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Generative-vs-Discriminative-Models"><span class="nav-number">3.2.</span> <span class="nav-text">Generative vs. Discriminative Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Text-Processing"><span class="nav-number">4.</span> <span class="nav-text">Basic Text Processing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Regular-Expressions"><span class="nav-number">4.1.</span> <span class="nav-text">Regular Expressions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-tokenization"><span class="nav-number">4.2.</span> <span class="nav-text">Word tokenization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LINGUISTIC-AND-REPRESENTATIONAL-CONCEPTS"><span class="nav-number">5.</span> <span class="nav-text">LINGUISTIC AND REPRESENTATIONAL CONCEPTS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Parsing"><span class="nav-number">5.1.</span> <span class="nav-text">Parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Syntactic-Parsing"><span class="nav-number">5.1.1.</span> <span class="nav-text">Syntactic Parsing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Statistical-Parsing"><span class="nav-number">5.1.2.</span> <span class="nav-text">Statistical Parsing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dependency-Parsing"><span class="nav-number">5.1.3.</span> <span class="nav-text">Dependency Parsing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Constituency"><span class="nav-number">5.2.</span> <span class="nav-text">Constituency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dependency-syntax"><span class="nav-number">5.3.</span> <span class="nav-text">Dependency syntax</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Dependency-parsing"><span class="nav-number">5.3.1.</span> <span class="nav-text">Dependency parsing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transition-based-dependency-parsing"><span class="nav-number">5.3.2.</span> <span class="nav-text">Transition-based dependency parsing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dependency-tree"><span class="nav-number">5.3.3.</span> <span class="nav-text">Dependency tree</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bounded-and-unbounded-dependencies"><span class="nav-number">5.3.4.</span> <span class="nav-text">Bounded and unbounded dependencies</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ambiguity"><span class="nav-number">5.4.</span> <span class="nav-text">Ambiguity</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Structural-ambiguity"><span class="nav-number">5.4.1.</span> <span class="nav-text">Structural ambiguity</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Attachment-ambiguity"><span class="nav-number">5.4.1.1.</span> <span class="nav-text">Attachment ambiguity</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Coordination-ambiguity"><span class="nav-number">5.4.1.2.</span> <span class="nav-text">Coordination ambiguity</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Global-and-local-ambiguity"><span class="nav-number">5.4.2.</span> <span class="nav-text">Global and local ambiguity</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Morphology"><span class="nav-number">5.5.</span> <span class="nav-text">Morphology</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Challenge-of-rich-Morphology"><span class="nav-number">5.5.1.</span> <span class="nav-text">Challenge of rich Morphology</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#morphemes"><span class="nav-number">5.5.2.</span> <span class="nav-text">morphemes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lemma"><span class="nav-number">5.5.3.</span> <span class="nav-text">Lemma</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Affixes"><span class="nav-number">5.5.4.</span> <span class="nav-text">Affixes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Morphological-parsing"><span class="nav-number">5.5.5.</span> <span class="nav-text">Morphological parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Finite-state-transducers"><span class="nav-number">5.5.5.1.</span> <span class="nav-text">Finite-state transducers</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Root"><span class="nav-number">5.5.6.</span> <span class="nav-text">Root</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Stem"><span class="nav-number">5.5.7.</span> <span class="nav-text">Stem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inflectional-vs-Derivational-Morphology"><span class="nav-number">5.5.8.</span> <span class="nav-text">Inflectional vs. Derivational Morphology</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Open-class-Closed-class"><span class="nav-number">5.6.</span> <span class="nav-text">Open-class Closed-class</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-sense"><span class="nav-number">5.7.</span> <span class="nav-text">Word sense</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Word-sense-disambiguation"><span class="nav-number">5.7.1.</span> <span class="nav-text">Word sense disambiguation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Collocation"><span class="nav-number">5.7.2.</span> <span class="nav-text">Collocation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lexical-semantic-relationships"><span class="nav-number">5.7.3.</span> <span class="nav-text">Lexical semantic relationships</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#synonym"><span class="nav-number">5.7.3.1.</span> <span class="nav-text">synonym</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hyponym"><span class="nav-number">5.7.3.2.</span> <span class="nav-text">hyponym</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hypernym"><span class="nav-number">5.7.3.3.</span> <span class="nav-text">hypernym</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#similarity"><span class="nav-number">5.7.3.4.</span> <span class="nav-text">similarity</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distributional-semantic-models"><span class="nav-number">5.8.</span> <span class="nav-text">Distributional semantic models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Pointwise-mutual-information"><span class="nav-number">5.8.1.</span> <span class="nav-text">Pointwise mutual information</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#PPMI"><span class="nav-number">5.8.1.1.</span> <span class="nav-text">PPMI</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#t-test"><span class="nav-number">5.8.2.</span> <span class="nav-text">t-test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Minimum-Edit-Distance"><span class="nav-number">5.8.3.</span> <span class="nav-text">Minimum Edit Distance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#WordNet"><span class="nav-number">5.8.4.</span> <span class="nav-text">WordNet</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Topic-modelling"><span class="nav-number">5.9.</span> <span class="nav-text">Topic modelling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Meaning-representation-language"><span class="nav-number">5.10.</span> <span class="nav-text">Meaning representation language</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#First-order-Logic"><span class="nav-number">5.10.1.</span> <span class="nav-text">First-order Logic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lambda-λ-Expression"><span class="nav-number">5.10.2.</span> <span class="nav-text">Lambda λ Expression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reification-of-events"><span class="nav-number">5.10.3.</span> <span class="nav-text">Reification of events</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semantic-Parsing"><span class="nav-number">5.11.</span> <span class="nav-text">Semantic Parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Syntax-Driven-Semantic-Analysis"><span class="nav-number">5.11.1.</span> <span class="nav-text">Syntax Driven Semantic Analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Semantic-attachments"><span class="nav-number">5.11.2.</span> <span class="nav-text">Semantic attachments</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lexical-semantics"><span class="nav-number">5.11.3.</span> <span class="nav-text">Lexical semantics</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EVALUATION-CONCEPTS-AND-METHODS"><span class="nav-number">6.</span> <span class="nav-text">EVALUATION CONCEPTS AND METHODS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Instrinsic-vs-extrinsic-evaluation"><span class="nav-number">6.1.</span> <span class="nav-text">Instrinsic vs. extrinsic evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Extrinsic"><span class="nav-number">6.1.1.</span> <span class="nav-text">Extrinsic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Intrinsic"><span class="nav-number">6.1.2.</span> <span class="nav-text">Intrinsic</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Perplexity"><span class="nav-number">6.2.</span> <span class="nav-text">Perplexity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Human-evaluation"><span class="nav-number">6.3.</span> <span class="nav-text">Human evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Precision-Recall-F-measure"><span class="nav-number">6.4.</span> <span class="nav-text">Precision, Recall, F-measure</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Precision"><span class="nav-number">6.4.1.</span> <span class="nav-text">Precision</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Recall"><span class="nav-number">6.4.2.</span> <span class="nav-text">Recall</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#F-measure"><span class="nav-number">6.4.3.</span> <span class="nav-text">F-measure</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Confusion-matrix"><span class="nav-number">6.4.4.</span> <span class="nav-text">Confusion matrix</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Correlation"><span class="nav-number">6.5.</span> <span class="nav-text">Correlation</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://shootingspace.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://shukebeta.me/UoE-anlp/';
          this.page.identifier = 'UoE-anlp/';
          this.page.title = 'Accelerated Natural Language Processing';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://shootingspace.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("KJ3aRNAv0BvPIe1SoKj9frht-gzGzoHsz", "gm1RJIiLJ5g6f6lmDxkpWzVG");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
